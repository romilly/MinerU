[
    {
        "type": "text",
        "text": "Adrian Kosowski∗† ",
        "text_level": 1,
        "bbox": [
            166,
            212,
            300,
            227
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Przemysław Uznanski´ † Jan Chorowski† Michał Bartoszkiewicz† ",
        "bbox": [
            334,
            212,
            640,
            243
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Pathway, Palo Alto, USA research@pathway.com ",
        "bbox": [
            413,
            250,
            584,
            277
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "ABSTRACT ",
        "text_level": 1,
        "bbox": [
            450,
            303,
            545,
            318
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. ",
        "bbox": [
            174,
            325,
            825,
            381
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We introduce ‘Dragon Hatchling’ (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of $n$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. ",
        "bbox": [
            174,
            382,
            825,
            438
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. ",
        "bbox": [
            174,
            440,
            825,
            508
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "BDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time. Our results, formalized as a chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show a macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: “the equations of reasoning”. ",
        "bbox": [
            173,
            511,
            825,
            608
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "BDH can be represented as a brain model. It contains $n$ neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. ",
        "bbox": [
            173,
            609,
            825,
            734
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. ",
        "bbox": [
            174,
            736,
            825,
            805
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We believe BDH opens the door to a new theory of “Thermodynamic Limit” behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time. ",
        "bbox": [
            174,
            808,
            823,
            849
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Contents ",
        "text_level": 1,
        "bbox": [
            114,
            90,
            192,
            106
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 Introduction 3 ",
        "text_level": 1,
        "bbox": [
            116,
            125,
            892,
            140
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1.1 Motivation . 3   \n1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning 5   \n1.3 Contribution of this work 7   \n1.4 Notation 9 ",
        "bbox": [
            138,
            146,
            885,
            224
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 BDH: a language model architecture given by local distributed graph dynamics 10 ",
        "text_level": 1,
        "bbox": [
            119,
            243,
            885,
            258
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.1 Formalism for local graph-based language models . . 10   \n2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) 12   \n2.3 Interpretation of attention as a micro-inductive bias of reasoning 14   \n2.4 Interpretation of BDH as an oscillator network toy-model . 14   \n2.5 Expressing BDH using brain models 16 ",
        "bbox": [
            137,
            263,
            885,
            364
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "BDH-GPU: a tensor-friendly version of the BDH architecture 17 ",
        "text_level": 1,
        "bbox": [
            112,
            382,
            887,
            397
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3.1 Notation for BDH-GPU . 17   \n3.2 Definition of BDH-GPU as a state-space system 18   \n3.3 Interpretation of BDH-GPU as a local interacting particle system 20   \n3.4 Expressing BDH-GPU using BDH: preserving parameter and state size 21 ",
        "bbox": [
            122,
            402,
            890,
            482
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "4 Implementation and scaling laws 23 ",
        "text_level": 1,
        "bbox": [
            114,
            500,
            885,
            516
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "4.1 Implementation characteristics of BDH-GPU 23   \n4.2 Comparison of BDH-GPU to GPT2-like Transformers 24   \n4.3 Comparison of BDH-GPU to other sequence processing architectures 24 ",
        "bbox": [
            137,
            521,
            885,
            579
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "5 Analysis: emergence of modularity and scale-free structure 26 ",
        "text_level": 1,
        "bbox": [
            112,
            597,
            885,
            613
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "5.1 Background: modularity and scale-free property of systems . . 26   \n5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block 27   \n5.3 ReLU-lowrank as a signal propagation dynamics 28   \n5.4 Modularity in BDH-GPU signal propagation . 29   \n5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products 31 ",
        "bbox": [
            135,
            618,
            885,
            719
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "6 Analysis: linear attention, sparse positive activation, and monosemanticity 34 ",
        "text_level": 1,
        "bbox": [
            120,
            736,
            883,
            752
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "6.1 Macro-expressiveness of attention in BDH-GPU . 34   \n6.2 Micro-interpretation of attention in BDH-GPU 37   \n6.3 Empirical findings: monosemantic synapses 37   \n6.4 Empirical findings: sparse neuron activations 38 ",
        "bbox": [
            137,
            757,
            885,
            835
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "7 Playing with the Hatchling 41 ",
        "text_level": 1,
        "bbox": [
            116,
            854,
            882,
            871
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "7.1 Model merging: concatenating two models . . 41   \n7.2 Training without backpropagation through time 42 ",
        "bbox": [
            140,
            876,
            885,
            912
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "8 Conclusions 43 ",
        "bbox": [
            114,
            89,
            885,
            107
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "8.1 Takeaways for model engineering 43   \n8.2 Implications for brain science . 43   \n8.3 Societal impact 44   \nA Connection between generalization of reasoning and computational expressiveness 53   \nB Further description of experiments 53   \nC Omitted formal claims and proofs 55   \nD Desirable properties of a local graph dynamics for language models 58   \nE BDH-GPU PyTorch code listing 61 ",
        "bbox": [
            135,
            111,
            885,
            170
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            185,
            887,
            338
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "bbox": [
            116,
            354,
            253,
            372
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Long reasoning and long context inference pose a severe challenge of generalization across scales of time. From vibe coding to market research, users of Language Models and agentic systems are increasingly relying on defining tasks through informal prompts, which the language model is expected to follow over long sequences of actions or decisions, like a reasonable human actor would. Implicitly, most users expect machines to follow the generalization patterns of human reasoning, i.e., to generalize reasoning in the same way as humans do. The complexity of tasks attempted in this way has gone from the equivalent of hours of human work for a single prompt, to weeks (Emberson et al., 2025). However, experimental evidence suggests that the Transformer and other state-of-the-art architectures do not systematically generalize chain-of-thought (CoT) reasoning to scenarios longer than the ones seen during training (Shojaee et al., 2025). ",
        "bbox": [
            114,
            387,
            882,
            512
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Chain-of-Thought reasoning models can be considered through the lens of computational complexity theory. For a Language Model to generalize human reasoning on a given class of tasks, we expect this model to be able to emulate the corresponding reasoning function of the human brain efficiently.3 While the Transformer with Chain-of-Thought is Turing-complete and can efficiently emulate certain restricted classes of formal languages (Merrill and Sabharwal, 2024), this does not in itself provide a satisfactory answer as to how it emulates human reasoning. The human brain is an extremely complex graph-based distributed computing system with $n \\approx 8 \\cdot 1 0 ^ { 1 0 }$ neurons, and $m > 1 0 ^ { 1 4 }$ neuron connections (synapses), of which a certain percentage is actively used. The direct simulation of such a distributed system by a Language Model through generic Turing-machine reductions would require billions of CoT tokens of the Language Model to represent a single step of reasoning in the brain. So, do Transformer-like models actually relate to brain function? ",
        "bbox": [
            114,
            518,
            882,
            656
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Such a relationship should follow more closely from a tighter, more direct simulation. Finding such a connection between Language Models and human brain function has, so far, proved elusive. Indeed, when comparing a tensorbased Language Model based on feed-forward network blocks and attention, to a uniform, scale-free graph-based distributed system, such as the brain, the two may, at first glance, appear very dissimilar. ",
        "bbox": [
            116,
            662,
            882,
            719
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "This apparent dissimilarity of structure between Language Models and brain structure has been one of the main causes of concern in attempts to reconcile Computation and the Brain (Olshausen, 2018), as well as a cause of concern regarding the difficulty to foresee the behavior of autonomous AI systems. ",
        "bbox": [
            116,
            724,
            882,
            767
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In this paper, we show the link between the Transformer and Brain models. ",
        "bbox": [
            117,
            773,
            606,
            787
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1.1 Motivation ",
        "text_level": 1,
        "bbox": [
            117,
            805,
            230,
            819
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The development of Artificial Intelligence and the understanding of Neural Science have gone hand in hand since the 1940’s, both being efforts to understand the “mystery of intelligence”. The relationship between computing systems and the brain served as motivation for the pioneering theoreticians such as John von Neumann (1958), Alan Turing (1950), Goeff Hinton (2005), Warren McCulloch and Walter Pitts (1943), and Horace Barlow (1972). ",
        "bbox": [
            116,
            830,
            883,
            886
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Since then, milestones in Machine Learning around Artificial Neural Networks — using backpropagation with SGD (Rumelhart et al., 1986), followed by Deep Learning (LeCun et al., 2015), and the Attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) — have split the “mystery of how intelligence works” into two. First, we still have no clear explanation for the micro-to-macro correspondence of the reasoning function of the brain. Second, we do not understand the correspondence between the artificial and natural systems — notably, how effects observed in the brain (emergent network; sparse activations; oscillatory phenomena; unknown relationship to backpropagation mechanisms) map into those which appear in systems based on dense tensors, trained using gradient back-propagation over time. ",
        "bbox": [
            114,
            90,
            882,
            202
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Reconciling Reasoning Function of the Brain with Language Models. There is a seemingly deep divide between state-of-the-art language models, like the Transformer, and natural distributed systems with local graph dynamics, like those of the brain. Specifically, for the brain, we do not understand how the reasoning function emerges from neuronal dynamics at the microscale. For the Transformer, the interpretation of function is given at the level of vectors, but not at the level of particle dynamics or a uniform distributed computing system. ",
        "bbox": [
            116,
            217,
            882,
            286
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Language and reasoning are the key areas of higher-order brain function for which we do not yet have a complete understanding. Many other areas of brain function have been explained through analogies to Machine Learning architectures. For example, the visual cortex is becoming well-understood, especially in its peripheral layers, and the observed inference dynamics are shown to have a correspondence to known Deep Learning architectures (Mohsenzadeh et al., 2020). The use of sparse coding by the brain was considered in the context of processing visual cues (Olshausen and Field, 1997), as well as for the olfactory systems (Lin et al., 2014). By contrast, higher-order cognitive functions of the association cortex of the human brain, such as language and reasoning, are among the least understood. A number of models provide partial explanations and have been verified at small scales. Some of the first attempts include explaining context-dependent computation in the prefrontal cortex using population dynamics of an RNN (Mante et al., 2013). Later approaches include the Tolman-Eichenbaum Machine (Whittington et al., 2020, 2022), as well as a number of more recent works (Papadimitriou et al., 2020; Dabagia et al., 2024; Mitropolsky and Papadimitriou, 2025). One of the main stumbling blocks concerns going from spiking activation patterns at neurons, and localized attention effects at synapses, to a higher-order function, serving a reasoning purpose, efficiently organized at a scale of millions to billions of neurons. ",
        "bbox": [
            114,
            292,
            882,
            486
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Conversely, for Language Models architectures such as the Transformer, we miss a compact micro-interpretation as a distributed system. The expressiveness of the Transformer has been approximated using approaches from centralized computing and Complexity Theory, rather than from distributed systems. In the centralized perspective, a language model can be seen as a transformation function from inputs into outputs. The computational expressiveness of the Transformer architecture may then be approximated through frameworks based on RASP, such as RASP-L (Zhou et al., 2024) or C-RASP (Yang and Chiang, 2024; Huang et al., 2025). RASP-L provides a very convenient heuristic for estimating Transformer expressiveness at the rather coarse level of vector operations, while C-RASP provides a more specialized lower-bound on expressiveness, capturing a class of formulas of temporal counting logic. Both frameworks have been used to suggest theoretical models of task length generalization. This type of expressiveness techniques, however, do not lead to a uniform asymptotic model for the behavior of the Transformer, whether in GPT2 architecture or simplified. The scaling of the Transformer in its different dimensions, and the need to manipulate context length, complicate this goal. ",
        "bbox": [
            114,
            492,
            882,
            659
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The lack of such a uniform model also makes it hard to compare the capabilities of the Transformer to the capabilities of the brain at the level of correspondence of structure. Generally, the temporal behavior of a state-space system is reflected in its structure4. ",
        "bbox": [
            114,
            665,
            882,
            707
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Understanding whether it is possible to show alignment of the temporal behavior of two systems, which do not display any structural correspondence, and without a clear idea of how the weight tensors and state representation of one system ‘embed’ into the graph structure and state representation of the other system, is an awkward task. ",
        "bbox": [
            116,
            713,
            882,
            755
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "This brings us naturally to our motivational objective: Can we create Machine Learning models which are closer to the desirable properties of natural (human) reasoning systems, and which exhibit the same types of limit and scaling behavior as such natural systems? ",
        "bbox": [
            117,
            761,
            882,
            804
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Towards scale-free foreseeable AI. Ensuring correct scaling behavior of inference over time is of paramount importance for the deployment of AI whose reasoning or actions are not subject to strict human supervision. Most reasoning models and AI agentic systems admit limit objects (i.e., extensions to infinite time and infinite size) which are Turing-complete (cf. e.g. (Merrill and Sabharwal, 2024; Pérez et al., 2021; Jojic et al., 2023)). This means that they should be treated like computer programs — and should be approached by the users with the same standards of care, as a computer program of unknown origin and unknown purpose. ",
        "bbox": [
            114,
            818,
            882,
            875
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            117,
            90,
            883,
            119
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "An AI model can malfunction when allowed to run for a long time autonomously, i.e., without human validation of actions and reasoning outcomes. The most painful of all consequences, perhaps, is the concept of a failed generalization of reasoning (a malfunction with respect to the original task objective) over time, leading to a grotesque effect known as the “Paperclip Factory” (Bostrom, 2014). ",
        "bbox": [
            114,
            126,
            882,
            181
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Can the risk of such unsuccessful generalization be bounded? ",
        "bbox": [
            116,
            188,
            519,
            202
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "There are at least two scenarios in which a black-box model $M$ cannot be considered to have undergone previous empirical validation, and consequently cannot be used in higher-risk autonomous AI use cases. ",
        "bbox": [
            111,
            208,
            880,
            237
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "1. Length-generalization scenario: Model $M$ is expected to act autonomously on a task which is longer than tasks forming part of its validation set.   \n2. Model scaling scenario: Model $M$ is not exactly the same closed system as the one which was tested during validation. For example, suppose that models $M _ { 1 }$ and $M _ { 2 }$ were tested individually on smaller tasks, and let $M$ be an agentic system composed of instances of $M _ { 1 }$ and $M _ { 2 }$ which communicate with exchange messages with each other during inference. ",
        "bbox": [
            151,
            248,
            883,
            342
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "A natural way of avoiding both difficulties consists in studying systems which are scale-free with respect to size and time, and admit a form of uniform “thermodynamic limit” behavior. The limit behavior of computational systems at criticality naturally connects the size of the system with the probable duration of its operation, with the connection usually taking polynomial form (cf. e.g. (Björner et al., 1991; Cairns, 2018; Rolla, 2020) for examples of graph-based interacting particle systems for which rigorous results have been obtained in this direction). Consider a model $M _ { n }$ with architecture $\\mathcal { A }$ , parameterized by its size $n$ (with the interpretation of the number of uniform particles), and sampled from some space of $n$ -neuron models in architecture $\\mathcal { A }$ in some space equipped with a probability measure, $\\mathbf { \\boldsymbol { M _ { n } } } \\mathbf { \\bar { \\Omega } } \\sim \\mathbf { \\mathcal { P _ { A } } } ( n )$ . Informally, if the limit object $\\begin{array} { r } { \\mathcal { P } _ { \\mathcal { A } } : = \\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { P } _ { \\mathcal { A } } ( n ) } \\end{array}$ exists (under an appropriate, well-defined sense of uniformity of limit) then models $M _ { n }$ , for $n$ sufficiently large, will admit in the limit asymptotic properties, which can be used to characterize their behavior over time. ",
        "bbox": [
            114,
            354,
            882,
            492
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The existence of such a limit theory means that we can characterize, with bounded probability of error, the behavior of a family of large models, having $O ( n )$ parameters, while relying on a theory which is independent of the specific structure and size of the specific model. In this way, the limit behavior of a system of a very large number of interacting uniform particles over time becomes (stochastically) foreseeable in the sense of its adherence to expected behavior, which can be extrapolated from observations at shorter time scales. Thus, small tests may be conceived in order to provide validation for a scale-free system at long time scales. ",
        "bbox": [
            114,
            500,
            882,
            583
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Introducing Axiomatic AI. Axiomatic systems are those in which micro-foundations and the macro-description which arises from them are consistent and well-understood. The need for axiomatic understanding was highlighted by David Hilbert (1902), and has become the foundation in Statistical Physics (e.g. thermodynamics, fluid dynamics, spin glass theory), cellular mechanisms, Social Networks Science, and reconciliation of Microeconomics and Macroeconomics through a Network Economics perspective. ",
        "bbox": [
            116,
            601,
            882,
            670
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "This paper brings a micro-foundational understanding to Language Model inference, to the mechanisms of in-context learning, and Chain-of-Thought reasoning dynamics. ",
        "bbox": [
            117,
            676,
            877,
            705
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The considerations in this work naturally support a shift of perspective from Interpretable AI, which gives an approximate understanding of what the model is doing now (without necessarily telling us what its current actions are going to lead to over longer time scales), to Axiomatic AI, where we also understand the micro-foundations of how the model can be expected to behave subsequently over time. ",
        "bbox": [
            116,
            710,
            882,
            767
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ",
        "text_level": 1,
        "bbox": [
            114,
            786,
            704,
            803
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "In this section we provide the reader with some of the main intuitions behind this work which, we hope, will help to navigate the remaining, more formal parts of this paper with ease. ",
        "bbox": [
            114,
            814,
            883,
            842
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "While there are many formal deductive systems in logic, they predominantly rely on the modus ponens inference rule. Applied to a rule-based reasoning system, it takes the following form: ",
        "bbox": [
            116,
            848,
            879,
            877
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "If we know that the $i$ -th fact is true, and our ruleset $\\sigma$ indicates that the $i$ -th fact implies the $j$ -th fact, then we know that the $j$ -th fact is true as well. In an approximate reasoning system, the strength of the rule $\\sigma ( i , j )$ indicates how the ",
        "bbox": [
            112,
            882,
            882,
            911
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "belief $X ( i )$ of the system affects its belief $A ( j )$ . We could write: ",
        "bbox": [
            114,
            90,
            539,
            106
        ],
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/d9aeaf8d438572271237aa9794b015358392b975fbb9ec88ad2122e2d24d69b6.jpg",
        "text": "$$\nX ( i ) , \\sigma ( i , j )  A ( j ) ,\n$$",
        "text_format": "latex",
        "bbox": [
            423,
            113,
            573,
            131
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "to indicate that if $X ( i )$ is a weighted belief, it contributes $X ( i ) \\sigma ( i , j )$ to the system’s belief $A ( j )$ ",
        "bbox": [
            112,
            137,
            750,
            154
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Practical logical inference systems differ in strategies employed for rule selection, with the most advanced ones allowing direct manipulation of the ruleset, effectively resulting in a form of program evolution during inference5. For an approximate reasoning system, such a heuristic could manipulate the strength of rules, modulating the impact of belief $X ( i )$ on the system’s belief $A ( j )$ . ",
        "bbox": [
            114,
            159,
            882,
            215
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Hebbian learning (Hebb, 1949), often presented as the mnemonic “Neurons that fire together wire together”, can be seen as a heuristic for ruleset manipulation. It postulates that synaptic connections are strengthened when the activity of one neuron, $Y ( i )$ , led to the firing of another neuron, $X ( j )$ . In the context of an adaptive, approximate inference system, the Hebbian heuristic means that if during the course of operation a fact $i$ contributed some evidence for $j$ , the system increases the significance of the implication $\\sigma ( i , j )$ . We could write this rule as: ",
        "bbox": [
            116,
            219,
            882,
            290
        ],
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/a89fe371bd756f6477eefae3e503f61f1c51f013d25eb786f476262d615e0d92.jpg",
        "text": "$$\nY ( i ) , X ( j )  \\sigma ( i , j ) ,\n$$",
        "text_format": "latex",
        "bbox": [
            423,
            297,
            573,
            315
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "with the interpretation that co-presence (or a spike) of $Y ( i )$ followed by $X ( j )$ increases $\\sigma ( i , j )$ by $Y ( i ) X ( j )$ ",
        "bbox": [
            116,
            321,
            828,
            338
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "The relations (1) and (2), over a set of $n$ facts, may form the basis of a simple approximate reasoning system that adapts its operation to the problem at hand. Starting with some initial connections between facts, the system applies the rules to discover new facts, at the same time reweighting the ruleset in a way that strengthens the connections between the initial and derived facts. Effectively, should the system be rerun with the new ruleset, it would arrive at similar conclusions faster. ",
        "bbox": [
            114,
            342,
            883,
            412
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Suppose now that the reasoning system is equipped with two sets of rules: a fixed set $G$ and an evolving set $\\sigma$ . From a machine learning perspective, the fixed ruleset $G$ can be seen as model weights in Deep Learning terminology, learned using e.g. error backpropagation on a training set. On the other hand, the evolving ruleset can be seen as the temporal state of the reasoning system, sometimes called “fast weights” (Hinton and Plaut, 1987; Schmidhuber, 1993; Ba et al., 2016a). Fast-weights systems have a favorable ratio of state size to parameter count. A system with $n$ facts has $m = O ( n ^ { 2 } )$ trainable parameters (expressed using one or more $n \\times n$ matrices). A classical recurrent neural net, such as the LSTM (Hochreiter and Schmidhuber, 1997), treats individual fact (neuron) activations as its state, thus maintaining only $O ( n )$ state variables. On the other hand, the evolving set of fast-weights $\\sigma$ has $m = O ( n ^ { 2 } )$ state entries. We believe this 1-1 ratio of trainable parameter to state size is important in designing practical reasoning systems and may justify the success of the Transformer (Vaswani et al., 2017) and state-space (Gu and Dao, 2024) sequence processing models. ",
        "bbox": [
            114,
            417,
            882,
            570
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Now, bearing in mind that the trainable parameters and state have comparable size $m$ , we can adjust the ratio between this value $m$ and the size $n$ of the fact base. This will happen through a choice of sparsity for the $n \\times n$ matrices carrying parameters and state, resulting in a specific relationship of the two values, $n \\ll m \\ll n ^ { 2 }$ . In this way, our system gets a natural interpretation in terms of graphs on $n$ nodes and $m$ edges, with the graph edges tasked with their first two roles: carrying state, and, carrying trainable parameters. Finally, we will give our system an interpretation of a dynamical system with distributed (localized) dynamics, and we will task our edges with their third crucial role: mediating in communication between nodes of the system. In this way, through assimilation of edges to natural function in the brain, we will refer to the $m$ edges as synapses connecting a set of $n$ neurons into a distributed graph-based system. ",
        "bbox": [
            114,
            577,
            883,
            689
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "In the following Section 2, we will introduce BDH, a reasoning system that formalizes and combines relations (1) and (2) with dynamics involving fixed rules. The BDH system: ",
        "bbox": [
            112,
            694,
            880,
            723
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "1. is a reasoning system, efficiently using the modus ponens reasoning rule with heuristic rule reweighting, based on (1) and (2),   \n2. can be implemented with local graph dynamics, making it suitable for brain-like execution model, and amenable to a principled, axiomatic description,   \n3. contains a set of fixed connections (parameters), and a set of dynamically adjusted connections $( \\sigma )$ , which can be seen as its dynamic state updated with a Hebbian learning rule,   \n4. admits as its special case BDH-GPU, a GPU-efficient reasoning model architecture, introduced in Section 3 and experimentally validated at scale in Section 7 in direct comparison to state-of-the-art GPT2-like Transformers. ",
        "bbox": [
            151,
            733,
            883,
            876
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "1.3 Contribution of this work ",
        "text_level": 1,
        "bbox": [
            116,
            90,
            333,
            106
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The focus of this paper is in explaining the dynamics of the primary function of language and reasoning models: inference. We provide a description of a language model architecture which is directly comparable to the Transformer, and admits a clear and interpretable local interpretation of its inference dynamics as a programmable interacting particle system. ",
        "bbox": [
            114,
            116,
            882,
            172
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Language Models as Local Graph Dynamics. In Section 2, we introduce a graph-based model architecture called BDH, where all model parameters are represented as topology and weights of the communication graph, and model state during inference is represented as edge-reweighting applied to this graph topology. ",
        "bbox": [
            116,
            186,
            883,
            229
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Claim 1 (informal overview of theoretical results for BDH). We introduce a state-space Machine Learning architecture called BDH, formed by a system of n particles called neurons which communicate in a way governed by the weights and topology of the system graph, representing a “communication by wire” network. ",
        "bbox": [
            116,
            233,
            883,
            276
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• The inference dynamics of BDH, treated as a distributed system, can be represented as execution of local rulesets for n particles with programmable interactions, with particles acting as nodes of the interaction graph and scalar state variables located on its edges (cf. Section 2.2). ",
        "bbox": [
            148,
            286,
            882,
            330
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• The local kernel of BDH can be naturally expressed (emulated) by a graph-based Spiking Neural Network system capable of Hebbian learning dynamics, an Excitatory circuit, and an Inhibitory circuit on an n-neuron system described by a neuron interaction graph (cf. Section 2.5). ",
        "bbox": [
            151,
            337,
            880,
            381
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "In order to train BDH efficiently and analyze its performance, we restrict it, making this restriction the core of a GPUfriendly architecture called BDH-GPU. This restriction is obtained by treating the communication of the $n$ particles as proceeding through a mean-field (“radio network”), rather than a graph (“communication by wire”), cf. Fig. 3 for an explanation of how the state-space equations of BDH-GPU are obtained from BDH. ",
        "bbox": [
            114,
            391,
            883,
            446
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "This allows us to train a mathematically equivalent model, while localizing its state in short vectors at neurons, not at connections (synapses) of the system. ",
        "bbox": [
            116,
            453,
            879,
            482
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "A tensor-friendly case of BDH: the BDH-GPU architecture. The BDH-GPU architecture, like the Transformer, crucially relies on an attention mechanism, and is amenable to token-parallel training on GPU for next token prediction tasks. Unlike the Transformer, activation vectors of BDH-GPU appear in a very high dimension $n$ , are positive by design, and turn out to be sparse. ",
        "bbox": [
            116,
            496,
            882,
            553
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Claim 2 (informal overview of theoretical results for BDH-GPU). We introduce a Machine Learning architecture called BDH-GPU, parameterized by a single (very large) scaling parameter n and a second parameter $d$ , $\\log n <$ $d \\ll n$ $d = 2 5 6$ in practice), such that: ",
        "bbox": [
            116,
            556,
            882,
            599
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• A model in BDH-GPU $( n , d )$ has $( 3 + o ( 1 ) ) n d$ parameters, and admits a precise interpretation as a statespace system following the local dynamics of a $n$ -particle system in an interaction field subject to equations of state (8). This system is described by $O ( d )$ parameters per particle, whose interaction field has mean field interpretation, which in a computational view corresponds to a particle communication network realized by means of “noisy radio broadcast”. ",
        "bbox": [
            160,
            609,
            883,
            680
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• BDH-GPU is a special case of BDH in the sense that, for any BDH-GPU model with n particles, there exists a BDH model with n particles with the same inference behavior and the same size $O ( n d )$ of trainable parameters, with the two models being formally equivalent up to placement of Layer Norms (cf. Claims 3 and 4). ",
        "bbox": [
            160,
            688,
            883,
            732
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• The BDH-GPU architecture relies on a combination of two blocks: a specific kind of ReLU-lowrank feedforward network, and a linear attention mechanism, which both operate in the same neuron dimension $n$ using positive activation vectors. ",
        "bbox": [
            155,
            739,
            880,
            781
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• The mechanisms of BDH-GPU, considered at the macro-level of activation vectors in $R ^ { n }$ , can be compared to those of the Transformer (cf. Section 6.1, Section 5.2). This justifies the applicability of the frameworks of approximate macro-expressiveness, based on RASP (Weiss et al., 2021; Zhou et al., 2024; Yang and Chiang, 2024) and designed for the Transformer, to BDH-GPU. ",
        "bbox": [
            160,
            790,
            885,
            847
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• The micro-interpretation of BDH-GPU mechanisms as neuron-neuron interaction dynamics: (1) explains mechanisms of in-cluster communication of neurons and the spontaneous emergence of graph structure with high Newman modularity in the neuron-neuron communication network (cf. Section 5), and (2) provides a strict correspondence between the macro-mechanism of in-context inference based on attention and the local representation of state on individual neuron-neuron pairs (synapses) with state update dynamics based on sporadic updates to synaptic edge weight (cf. Section 6). ",
        "bbox": [
            160,
            856,
            883,
            911
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            168,
            92,
            883,
            119
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "The above results are complemented by empirical findings. ",
        "bbox": [
            116,
            133,
            501,
            148
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Empirical Finding 1 (informal overview of empirical results of BDH-GPU). BDH-GPU is represented as a tensorbased architecture and can be trained with standard back-propagation methods (cf. Section 3). ",
        "bbox": [
            120,
            155,
            882,
            184
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "• The BDH-GPU architecture is shown to follow scaling laws (parameters vs. loss) of optimized Transformers in the GPT architecture, at parameter scales between 10M to 1B, on all next token prediction tasks we tested, including tasks of language and translation reminiscent of those in the original benchmark set for the Transformer architecture (cf. Section 4.2). ",
        "bbox": [
            158,
            196,
            882,
            253
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "• An emergent network reflecting the associated BDH graph dynamics can be read out directly from the parameter matrices of a trained BDH-GPU model, showing emergence of graph structure (cf. Section 5.5). ",
        "bbox": [
            150,
            263,
            883,
            292
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "• The positive activations of BDH-GPU exhibit sparsity (at about $5 \\%$ level) in the y vectors of its state space dynamics, with sparsity levels reflecting the amount of activity being performed by BDH-GPU for a given token (cf. Section 6.2). ",
        "bbox": [
            151,
            303,
            882,
            347
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "• In-context state of BDH-GPU attention is shown to localize on the same synapses (neuron-neuron links) consistently across multiple prompts, allowing for some basic features, the interpretation of the current in-context state based on the reading of state of an individual synapse associated with such a feature (cf. Section 6.3). ",
        "bbox": [
            160,
            357,
            883,
            400
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "A more detailed discussion of the training approach is provided in Appendix B.2, while the code listing for BDH-GPU is provided in Appendix E. For the purposes of our experiments, we did not apply any specific training method which would be known to guide the system towards any of the observed emergent properties. (In particular, L1-regularization was disabled.) The observed emergent effects follow naturally from the design choices of the BDH and BDH-GPU architectures, and are largely attributable to the combination of: the choice of model dimensions with comparable model-to-state ratio, reliance on linear attention in high dimension, reliance on ReLU thresholds for ensuring that activation vectors are positive (trivially) and sparse (an effect empirically noted in (Haziza et al., 2025)). ",
        "bbox": [
            116,
            412,
            882,
            511
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "We also remark that the BDH-GPU architecture allows for the uniform asymptotic scaling of the model in one dimension, $n$ . For example, a composition of models, obtained by concatenation, is also model in the same architecture, with a larger value of $n$ (cf. Section 7.1 for an empirical study of this effect for practical translation tasks). Historically, a link has been established between infinitely wide feedforward networks and Gaussian Processes (Neal, 2012; Lee et al., 2017; Yang, 2019). BDH allows the study of limit behavior of reasoning models. ",
        "bbox": [
            116,
            516,
            882,
            587
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "With BDH and BDH-GPU, we show that Language Models can be amenable to a particle-based interpretation. In fact, two micro-foundations — particle-based behavior and logic-programming behavior of a reasoning system — fuse together in these architectures. ",
        "bbox": [
            174,
            606,
            826,
            647
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "The bridge between the Transformer and Brain models. The inference dynamics of BDH and BDH-GPU act as a natural bridge between Transformer, and neuromorphic models of the brain and its subsystems. We illustrate this in Fig. 1. ",
        "bbox": [
            119,
            669,
            882,
            710
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Implications for learning dynamics of natural lifelong inference systems. A lifelong learning system progresses in time, performing extremely rapid inference, combined with several training mechanisms at different time scales. ",
        "bbox": [
            119,
            731,
            882,
            760
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In this work, we provide and validate at scale a plausible explanation of what the predominant dynamics of such a system could look like, taking the system from ‘split-second’ scale, to the scale of inference during ‘minutes’, considering the flow of time at the natural rate of thought and language for humans. ",
        "bbox": [
            114,
            765,
            883,
            808
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "A complementary discussion of learning dynamics would aim to provide an explanation of how to take such a lifelong inference system from the scale of ‘minutes’ into even longer timescales. This would concern the slower transfer of “fast-weights”-like inference state to long-term memory, starting at the order of $1 0 ^ { 3 } { - } 1 0 ^ { 4 }$ tokens, and taking into account feedback signals. In this work, we do not provide a direct answer as to how the brain actually handles this effect at longer timescales. However, a constructive way to resolve this problem seems to be less challenging, once the local inference dynamics of the brain are better understood (we come back to this in the Conclusions). The modeling approach provided in Section 2.5 is proposed as a suitable framework for such a study. ",
        "bbox": [
            116,
            814,
            882,
            911
        ],
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/cfc296b0545133ccbd1db9488581bad2afb7aa81c3f420ab2290dd50adf376c0.jpg",
        "image_caption": [
            "Figure 1: General overview of architectures and their relationships: the inference dynamics of BDH and BDH-GPU act as a natural bridge between Transformer and models of the brain. The two main inference mechanisms of a reasoning architecture, attention and the feed-forward network, are defined at a macro-level through tensor operations for the Transformer, and at the micro-level of neuron interactions through local graph dynamics for Brain models. The new BDH-GPU architecture is naturally defined both at the level of vectors and of particle dynamics of neurons and synapses, acting as a bridge between these two approaches. See also Table 3 at the end of the paper for a more detailed comparison of architecture properties. "
        ],
        "image_footnote": [],
        "bbox": [
            129,
            92,
            875,
            383
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "1.4 Notation ",
        "text_level": 1,
        "bbox": [
            116,
            507,
            214,
            521
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "State-space models. For describing inference dynamics of any system, we will use state-space notation, and consider a state-space system composed of two parts: a set of model parameters $M$ which does not change during inference, and a state $\\sigma ( t )$ which changes during inference. The model performs inference following state-space equation $\\sigma ( t + 1 ) : =$ $\\mathcal { A } ( M , \\sigma ( t ) , a _ { t } )$ , where $a _ { t }$ is a possible external input to the system at time $t$ (such as a language token), $t = 0 , 1 , 2 , . . . ,$ and $\\mathcal { A }$ is referred to as the architecture $\\mathcal { A }$ which drives its progress. During inference without external input, usually autoregressive inference, we will shorten this to $\\sigma ( t ) : = \\mathcal { A } ^ { \\dot { t } } ( \\tilde { M } , \\sigma _ { 0 } )$ . ",
        "bbox": [
            114,
            535,
            883,
            619
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Models as programs. In settings that are of interest to us (inference with combining multiple facts, reasoning), we opt for terminology from computing. $M$ has the interpretation of a computer program code, $\\mathcal { A }$ has the interpretation of a computational machine architecture which runs it, and $\\sigma$ has the interpretation of the variable state of the program. We will use the terms ‘model $M ^ { \\prime }$ and ‘program $M ^ { \\prime }$ interchangeably. ",
        "bbox": [
            114,
            643,
            882,
            700
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Graphs and their dynamical systems interpretation. For a square matrix with non-negative coefficients, $H \\in$ $( \\mathbb { R } ^ { + } ) ^ { n , n }$ , $n \\in \\mathbb { N }$ , we will consider two more equivalent representations. In one, we will treat $H$ as a graph defined on some nodeset $V$ , with $V = | n |$ . Formally, we can take $V = \\{ e _ { 1 } , \\ldots , e _ { n } \\}$ , where $e _ { i } = ( 0 , \\ldots , 0 , 1 , \\bar { 0 } \\ldots , 0 ) \\in \\mathbb { R } ^ { n \\times 1 }$ with 1 on the $i$ -th position, forming an orthonormal basis. Non-zero entries of $H$ are referred to as edges. By an overloading of notation, we will write $H ( i , j ) : = { e _ { j } } ^ { T } H e _ { i } \\geq 0$ , to represent the node affinity function, or edge weight, from $i$ to $j$ . We define the edge set $E ( H ) : = \\{ ( i , j ) \\in V \\times V : H ( i , j ) > 0 \\}$ . ",
        "bbox": [
            114,
            724,
            882,
            810
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "In discussions of graph-based model architectures, we will take the standard interpretation of graphs from a linear dynamical systems perspective, applied to positive vectors. When $v \\in ( \\mathbb { R } ^ { + } ) ^ { n \\times 1 }$ is a non-negative vector, $H v \\in ( \\mathbb { R } ^ { + } ) ^ { n }$ has the interpretation of a linear transformation of $v$ . If $H$ satisfies the condition of stochasticity (column-normalization to 1), then $v \\mapsto H v$ is a Markov chain transition, with $\\| H v \\| _ { 1 } = \\| v \\| _ { 1 }$ . From a distributed systems perspective, transitions of stochastic matrices can be represented either through the direct simulation of (probabilities) of such a Markov chain, or described by the token dynamics of an extremely simple stochastic token distribution scheme in which a token located at node $e _ { i }$ goes to node $e _ { j }$ with probability $H ( i , j )$ . If $H$ is not stochastic, the operation $v \\mapsto H v$ additionally necessitates the suppression of a fraction of tokens, or the multiplication of tokens, at each step at each node, depending on the column-normalization of a given node.6 ",
        "bbox": [
            114,
            814,
            882,
            912
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            117,
            90,
            883,
            119
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "For two graphs $H _ { 1 } , H _ { 2 } \\in \\mathbb { R } ^ { n \\times n }$ , the graph $H = H _ { 2 } H _ { 1 }$ is obtained through (linear algebraic) matrix multiplication, and in a distributed system, the corresponding transition $v \\mapsto H v$ is obtained with two steps of token dynamics, one following graph $H _ { 1 }$ , the next following graph $H _ { 2 }$ . ",
        "bbox": [
            114,
            125,
            882,
            167
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Representing $m$ edge-weights of a sparse $n$ -node graph with $b$ bits of numerical precision per parameter is possible with we w $O ( m ( b { + } \\log n ) )$ ) bits of information, which corresponds to ymptotics that the second term of the sum d $\\begin{array} { r } { O ( m ( 1 + \\frac { \\log n } { b } ) ) } \\end{array}$ para (i.e., sake of simplicity,), and so we simply $\\log n = O ( b ) )$ say that we represent the graph with $O ( m )$ parameters. ",
        "bbox": [
            116,
            174,
            882,
            232
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "2 BDH: a language model architecture given by local distributed graph dynamics ",
        "text_level": 1,
        "bbox": [
            116,
            251,
            803,
            268
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "2.1 Formalism for local graph-based language models ",
        "text_level": 1,
        "bbox": [
            116,
            281,
            501,
            297
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We consider model architectures $\\mathcal { A }$ which correspond to models of graph-based distributed computing (cf. (Peleg, 2000; Hirvonen and Suomela, 2025)). A specific model $M$ in architecture $\\mathcal { A }$ corresponds to the weights and topology of the communication graph or graphs used by such a system. ",
        "bbox": [
            116,
            306,
            882,
            349
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Introduction to distributed graph systems. The distributed system architecture $\\mathcal { A }$ , representing the model architecture, is defined through a scheduler, and a local dynamics (kernel $K ( \\mathcal { A } ) )$ describing the local computations to be performed at each node of the system, and, communication between pairs of nodes connected by edges of the graph representing a given model $M$ . ",
        "bbox": [
            116,
            363,
            882,
            420
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We will generally accept that computations are performed only at $n$ neuron nodes (particles), whereas state variables of the system may appear both on nodes and edges. We will, for simplicity of analysis, consider systems governed by a synchronous scheduler, which in successive rounds, acts in two sub-rounds: ",
        "bbox": [
            116,
            426,
            882,
            468
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "1. Computation: computations of the kernel of $\\mathcal { A }$ are run at all neuron nodes independently. ",
        "bbox": [
            150,
            479,
            764,
            494
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "2. Communication “over wire”: each neuron node sends specified ‘output variables’ to specified ‘input variables’ of its neighboring neurons. ",
        "bbox": [
            147,
            498,
            880,
            526
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We expect the scheduler to follow the same communication pattern between neurons over time in a uniform way. In order to avoid artificial constructions of cyclic time-counters at nodes, we will define the architecture kernel through a short sequence of kernels, with the scheduler executing them in successive rounds in round-robin manner. Specifically, when $\\mathcal { A }$ is BDH, we will have a sequence of four kernels, $K ( \\mathcal { A } ) = ( K _ { 1 } ( \\mathcal { A } ) , K _ { 2 } ( \\mathcal { A } ) , K _ { 3 } ( \\mathcal { A } ) , K _ { 4 } ( \\mathcal { A } ) )$ , with $K _ { i } ( \\mathcal { A } )$ being executed in every round $r$ such that $r \\equiv i$ mod 4. ",
        "bbox": [
            116,
            537,
            882,
            607
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Programmable rulesets and the interaction kernel. We recall from Section 1.4 that a model architecture $\\mathcal { A }$ has the interpretation of a computational machine architecture, and models $M$ have the interpretation of programs in architecture $\\mathcal { A }$ . We also recall that a graph-based model $M$ is defined through a set of parameters which represent the topology and weights of the communication graph of the system. ",
        "bbox": [
            114,
            621,
            882,
            678
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The above considerations lead directly to the following observation: The graph of the communication network, which is used for communication between sites by the distributed system architecture $\\mathcal { A }$ during reasoning and language inference, has the interpretation of a (trainable, rule-based) program. Consequently, we embed the subsequent definition of BDH in a kernel formalism, given through a form of programmable rulesets, using two-particle interaction rules on a graph.7 ",
        "bbox": [
            114,
            683,
            882,
            753
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The rulesets which we will use to define BDH will closely resemble rulesets (protocols) known from evolutionary and population dynamics (Hofbauer and Sigmund, 1998; Angluin et al., 2006; Aspnes and Ruppert, 2009) and chemical reaction networks (Chen et al., 2014; Feinberg, 2019), however, they will be restricted to a special class of interactions. ",
        "bbox": [
            116,
            758,
            882,
            803
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We start by presenting the more general form of this interaction kernel. We then explain how such a kernel can be restricted, allowing it to be naturally implemented using a local graph-based distributed system (in particular, one ",
        "bbox": [
            117,
            808,
            879,
            837
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "relying spiking dynamics), while remaining sufficiently expressive to describe an attention-based language model.   \nThe resulting restriction will be called the edge-reweighting kernel. ",
        "bbox": [
            116,
            90,
            880,
            119
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Definition 1 (Interaction kernel, general form). $A$ system with $z$ species, $z \\in \\mathbb { N }$ , and state $( q _ { 1 } , \\dots , q _ { z } ) \\in Q$ , $q _ { i } \\in R ^ { + }$ , performs the interaction kernel with a ruleset (protocol) $P$ given by a set of transition rates called rule weights, $\\begin{array} { r } { P = ( ( r _ { i j k } \\in R ^ { + } ) _ { i , j , k \\in \\{ 1 . . . , z \\} } , ( d _ { k } \\in R ^ { + } ) _ { k \\in \\{ 1 . . . , z \\} } ) } \\end{array}$ , producing the following transition from a state $( q _ { 1 } , \\dots , q _ { z } ) \\in Q$ to a state $( q _ { 1 } ^ { \\prime } , \\ldots , q _ { z } ^ { \\prime } ) \\in Q$ : ",
        "bbox": [
            114,
            122,
            882,
            181
        ],
        "page_idx": 10
    },
    {
        "type": "equation",
        "img_path": "images/0a4379e7a7945f7d18780f08ad81d71d469912a45d1c81a9f90fc748bbcd4d9a.jpg",
        "text": "$$\nq _ { k } ^ { \\prime } : = ( 1 - d _ { k } ) q _ { k } + \\sum _ { i , j } r _ { i j k } q _ { i } q _ { j }\n$$",
        "text_format": "latex",
        "bbox": [
            390,
            179,
            607,
            214
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We will describe such a ruleset $P$ using the notational form: ",
        "bbox": [
            116,
            215,
            509,
            231
        ],
        "page_idx": 10
    },
    {
        "type": "equation",
        "img_path": "images/644407742fde2b43ab053e9a395b8b8b325152973ceb5c57509f62eb1cd51c8f.jpg",
        "text": "$$\nP = ( \\{ ^ { \\infty } q _ { i } , q _ { j } \\xrightarrow { r _ { i j k } } q _ { k } ^ { , \\prime \\prime } \\} _ { i , j , k \\in \\{ 1 . . . , z \\} } , \\{ ^ { \\infty } q _ { k } \\downarrow _ { d _ { k } } ^ { , \\prime \\prime } \\} _ { k \\in \\{ 1 . . . , z \\} } ) .\n$$",
        "text_format": "latex",
        "bbox": [
            300,
            236,
            696,
            257
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "As a matter of convention, omitted rules correspond to $r _ { i j k } = 0$ (respectively, $d _ { k } = 0 ,$ ), while rules with no rate value stated next the pointer correspond to $r _ { i j k } = 1$ (respectively, $d _ { k } = 1 .$ ). If $q _ { j }$ is omitted from notation on the left-hand side, we assume $q _ { j } = 1$ . ",
        "bbox": [
            114,
            262,
            883,
            305
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Equation (3) captures the dynamics of the following differential equation: $\\begin{array} { r } { \\frac { d q _ { k } } { d t } = - d _ { k } q _ { k } + \\sum _ { i , j } r _ { i j k } q _ { i } q _ { j } } \\end{array}$ . Assuming $q _ { i } , q _ { j } , r _ { i j k } \\in [ 0 , 1 ]$ , the expression $r _ { i j k } q _ { i } q _ { j }$ has the interpretation of a population dynamics or chemical process of the form $^ { 6 6 } i$ and $j$ give $k ^ { \\prime \\prime }$ , with this processes happening at rate $r _ { i j k }$ , assuming $q _ { i } , q _ { j } , q _ { k }$ have the interpretation of concentrations of species $i , j , k$ . The formalism we use here assumes non-normalized state variables. ",
        "bbox": [
            114,
            315,
            883,
            376
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We will subsequently use a restriction of the interaction kernel to graph-based systems, which we call the edgereweighting kernel, to describe BDH. ",
        "bbox": [
            116,
            381,
            877,
            410
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Restricting the interaction kernel to spiking signals and graph systems. First, we observe that rules of the form used in the interaction kernel from Definition 1 are extremely easy to implement in systems which rely on stochastic $_ { 0 / 1 }$ -valued signals. When $\\hat { q } _ { i }$ and $\\hat { q } _ { j }$ are independent random variables in $\\{ 0 , 1 \\}$ , with $\\mathrm { P r } [ \\hat { q } _ { i } = 1 ] = \\dot { q } _ { i }$ and $\\mathrm { P r } [ \\hat { q } _ { j } =$ $1 ] = q _ { j }$ , then $q _ { i } , q _ { j } \\to q _ { k }$ is expressible as the “AND gate” of probability: the random variable $\\delta \\hat { q } _ { k } : = q _ { i } q _ { j } \\in \\{ 0 , 1 \\}$ gives the same expected contribution $\\mathbb { E } \\delta \\hat { q } _ { k } = q _ { i } q _ { j }$ as the considered rule. ",
        "bbox": [
            114,
            424,
            882,
            494
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We now consider the restriction of interaction kernels to the case of graph systems. In the general formalism, $k$ can be arbitrary with respect to $i$ and $j$ . By contrast, consider graph systems, which describe binary relations between nodes, and not (directly) three-point relations. To resolve this, we will require that $i , j$ , and $k$ have the interpretation of two nodes of a graph and an edge which is incident to them. ",
        "bbox": [
            114,
            500,
            882,
            556
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "For an anchoring in the literature of dynamical systems, we note that already systems following an interaction kernel with a strongly constrained $k$ of the form $k \\in \\{ i , j \\}$ , exhibit powerful nonlinearities: with such a restriction on $k$ , Equation (3) describes the class of evolutionary systems following the equations of replicator dynamics (Hofbauer and Sigmund, 1998), also equivalently known as a non-normalized form of the fundamental Lotka-Volterra predatorprey dynamics. Replicator dynamics can naturally be represented as graph systems whose parameters are defined on on edges of the graph, but whose state is updated on on nodes of the graph. By contrast, when defining dynamics for reasoning in the current work, we will also need to capture a more powerful class of graph-based systems, where, crucially, state is larger than the number of neuron nodes, appearing on neuron-neuron edges (synapses). ",
        "bbox": [
            114,
            561,
            882,
            674
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We are now ready to describe a restriction of the interaction kernel from Definition 1 to the case of node-edge-node interaction rulesets in a graph: the edge-reweighting kernel. ",
        "bbox": [
            114,
            679,
            877,
            708
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Definition of the edge-reweighting kernel. We consider a graph system with n nodes, indexed $V = \\{ 1 , \\ldots , n \\}$ Additionally, a subset $E$ of pairs of indexes $( i , j )$ , for $i , j \\in \\{ 1 , \\ldots , n \\}$ forms the edges of the system. ",
        "bbox": [
            116,
            722,
            874,
            752
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The system has state variables associated (uniformly) with nodes and edges, which we denote with capital letters, e.g., $X ( i )$ , for $i \\in V$ or $Z ( i , j )$ , for $( i , j ) \\in E$ . ",
        "bbox": [
            114,
            756,
            877,
            785
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Definition 2 (edge-reweighting kernel). A distributed system follows the edge-reweighting kernel if its dynamics are given by the interaction kernel (Definition 1) with a set of non-negative state variables, defined on the set of nodes $V$ and set of edges $E$ of a graph, such that each local rule with non-zero rate is either a computational rule involving only state variables on a single node $i \\in V$ , or a communication rule for an edge $( i , j ) \\in E$ , involving state variables from the nodes $i , j$ and edge $( i , j )$ . ",
        "bbox": [
            114,
            789,
            882,
            859
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "For context, we remark that, in comparison to the strictly simpler dynamics of node-reweighting governed by graphbased replicator dynamics equations, dynamical systems based on the edge-reweighting kernel given by Definition 2 are rather elusive to study. We credit the seminal work of Algorithms theory (Christiano et al., 2011)[Fig. 1, Thm 3.2] as the first rigorous study of local edge-reweighting graph dynamics, combining fast-paced linear kernels on nodes with a slower-paced edge-reweighting process, in order to refine (‘focus’) electrical flows on graphs towards a sharper form of cost optimality.8 The BDH dynamics that we will introduce here rely on fundamentally different nonlinearities in the process, and will have the interpretation of guiding the system from premises defined at a subset of nodes, towards search targets at nodes representing a desired outcome, through reasoning inference rules with tunable weights set on edges. ",
        "bbox": [
            116,
            868,
            882,
            911
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            90,
            882,
            175
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In the following Subsection, we will use the introduced formalism to define BDH as an edge-reweighting kernel on the union of edges of several graphs $( G _ { x } ^ { \\textit {  } } , G _ { x } ^ { \\textit {  } } , G _ { y } ^ { \\textit {  } } , G _ { y } ^ { \\textit {  } } , G _ { s } )$ with the same set of $n$ nodes. ",
        "bbox": [
            116,
            180,
            879,
            212
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ",
        "text_level": 1,
        "bbox": [
            116,
            228,
            696,
            243
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Bearing in mind the discussion of graph dynamics suitable for the case of language inference, and specifically the definition of the edge-reweighting kernel (Definition 2), we are now ready to formalize the state-space dynamics of Equation (6) as a local graph dynamics. ",
        "bbox": [
            114,
            253,
            885,
            296
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Definition 3. The BDH model with n neurons, with parameters expressed through graphs $G _ { x } ^ { \\textit {  } } , G _ { x } ^ { \\textit {  } } , G _ { y } ^ { \\textit {  } } , G _ { y } ^ { \\textit {  } } , G _ { s }$ is represented as the ruleset of the edge-reweighting kernel, with $O ( n + | E ( G _ { s } ) | )$ state variables, with rule amplitudes given by “the equations of reasoning” in Table 1. ",
        "bbox": [
            114,
            301,
            882,
            344
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Inference dynamics of BDH. The BDH dynamics rely on rapid pulse dynamics with state variables $X ( i ) , Y ( i )$ , $A ( i )$ , defined on the $n$ neuron sites of the system, and fast-weight-like state variables $\\sigma ( i , j )$ , defined on a subset of edges of the system, $( i , j ) \\in E ( G _ { s } )$ . The full implementation of BDH shown in Table 1(b) also includes auxiliary state variables $X ^ { \\mathfrak { e } } ( i ) , X ^ { \\mathfrak { i } } ( i ) , Y ^ { \\mathfrak { e } } ( i ) , Y ^ { \\mathfrak { i } } ( i$ which are used as temporary counters, for integration of excitatory and inhibitory signals received by neurons. The dynamics also rely on a set of damping hyperparameters on state, $u > 0$ , which may in full generality be defined separately as $u ( i , j )$ for each edge $( i , j ) \\in \\mathsf { \\bar { E } } ( \\bar { G } _ { s } )$ . ",
        "bbox": [
            114,
            359,
            882,
            445
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Inference with BDH is performed as follows. For some parameter $L$ (e.g. $L \\ = \\ 8$ in most of this paper), which would correspond to the number of layers in a Transformer-like system, the system scheduler proceeds through rules in round-robin manner, ingesting new tokens every $4 L$ rounds and retrieving results $4 L$ rounds later. During round $4 l + k$ , for $0 \\leq l < L$ , the system performs rules from the $k$ -th column of Table 1, with each such round consisting of a communication step on edges and a local computation step on nodes. ",
        "bbox": [
            114,
            450,
            882,
            520
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "The state-space dynamics of BDH can be rewritten in vector-tensor form, equivalent to the local dynamics of the interaction kernel given in Table 1. This representation is given by Equation (6) in the following Section. ",
        "bbox": [
            120,
            526,
            882,
            554
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Observation 1. The BDH-Graph protocol for the interaction kernel, given for any time round $T = 4 L t + ( 4 l + k )$ , $0 \\leq l < L$ , $k = \\{ 0 , 1 , 2 , 3 \\}$ by the ruleset in Table 1 is equivalent to the state-space dynamics over time t and layers $l$ , given by Equation (6). ",
        "bbox": [
            114,
            559,
            883,
            602
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Proof. For completeness, a detailed explanation of the equivalence is provided in Appendix C.1. ",
        "bbox": [
            114,
            618,
            745,
            633
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "The variables $X ( i ) , Y ( i ) , A ( i )$ , defined for each of the $n$ nodes of the system, are updated in successive rounds. The state variables $\\sigma$ defined on edges are assumed to be distinct over $l$ as $\\sigma _ { l }$ , for $0 \\leq l < L$ ; this distinction serves to facilitate interpretation and to strike a balance between the number of parameters and the size of state of the system (assuming a single state matrix $\\sigma$ , uniform across $l$ , does not fundamentally change the operation and scaling laws of the architecture). ",
        "bbox": [
            116,
            650,
            883,
            719
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In the representation in Table 1 we do not impose how the local thresholding operation within some neuron $i$ , of the form $A ( i ) , B ( i ) \\xrightarrow { } - \\left( A ( i ) - B ( i ) \\right) ^ { + }$ , should be performed. We leave this as a computational primitive, which can be realized based on approximate counting or a comparator. The way natural neurons achieve thresholding to determine whether input signal excitation outweighs inhibition relies on time-integration of impulses. For realizations in other types of distributed systems and population protocols, we refer the reader to the literature on thresholding and Majority Protocols, cf. e.g. (Doty et al., 2021; Czyzowicz et al., 2022). ",
        "bbox": [
            114,
            726,
            882,
            811
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "The definition of the protocol does not specify how variable $X ( i )$ should be reset when the scheduler passes from layer $L$ of one input token to layer 0 for the next input token. As with the definition of state-space equations in Section 3, we leave this open to allow the dynamics to work both with externally provided input (for next-token prediction), or in a self-feedback loop (for autoregressive operation). ",
        "bbox": [
            114,
            818,
            882,
            875
        ],
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/5cfe8a8b8e781fe6129419170284bfc4ab2f6ba7f8be8142184f197d85e2ae9b.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=1 colspan=1>Round 4lInference from state</td><td rowspan=1 colspan=1>Round 4l + 1Reweighting of synapse state</td><td rowspan=1 colspan=1>Round 4l + 2Neuron replicator dynamics +inference from parameters</td><td rowspan=1 colspan=1>Round 4l+ 3Inference from parameters</td></tr><tr><td rowspan=1 colspan=1>X(i),Ot(i,j)→ A(j)O(i,j)↓1-u(i,j)</td><td rowspan=1 colspan=1>Y(i)， X(j)Gs（i,）o(i,j)Y(i)↓</td><td rowspan=1 colspan=1>G（,）→Y(i)A(i)， X(j)A(i)↓</td><td rowspan=1 colspan=1>G(x(i)Y(i)</td></tr></table>",
        "bbox": [
            104,
            217,
            895,
            348
        ],
        "page_idx": 12
    },
    {
        "type": "table",
        "img_path": "images/8d566138548c249b7ac253121f1ce24e9fac657fbbbc102df6a1cb08291bc8af.jpg",
        "table_caption": [
            "(b) Complete equations of reasoning of BDH "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Round 4l</td><td>Round 4l +1</td><td>Round 4l + 2</td><td>Round 4l + 3</td></tr><tr><td colspan=\"4\">Communication</td></tr><tr><td>X(𝑖), σt(i,j) → A(j)</td><td>G（i,j）o(i,j） Y(i)，X(j)</td><td>(Y(i） A(i) GY&#x27;(i） A(i)</td><td>G(i,） x（(i） Y() (）x(i） Y()</td></tr><tr><td></td><td>Computation</td><td></td><td></td></tr><tr><td>O(i,j)↓1-u(i,j) X(i）↓ x(i)</td><td>Y(i)↓ Y(i)↓ Y(i)</td><td>(Y²(𝑖)-Yi(𝑖))+, X(i)→Y(𝑖) A(i)↓</td><td>(X(𝑖)-Xi(i))+→X(𝑖)</td></tr></table>",
        "bbox": [
            106,
            383,
            893,
            622
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Table 1: The “equations of reasoning”: State-space dynamics of the BDH language model expressed through local graph dynamics with the edge reweighting kernel (Definition 2). The rules are executed for a distributed system of $n$ neurons performing steps of parallel computation and communication during inference. Model parameters are expressed through the weights of edges of graphs $G _ { x } ^ { \\textit {  } } , G _ { x } ^ { \\textit {  } } , G _ { y } ^ { \\textit {  } } , G _ { x } ^ { \\textit {  } } , G _ { s }$ , and BDH model training is equivalent to defining rule probability amplitudes （ $\\begin{array} { r } { \\mathfrak { I } _ { x } { ^ { \\mathfrak { e } } } ( i , j ) , G _ { x } { ^ { \\mathrm { i } } } ( i , j ) , G _ { y } { ^ { \\mathfrak { e } } } ( i , j ) , G _ { y } { ^ { \\mathrm { i } } } ( i , j ) , G _ { s } ( i , j ) \\ge 0 } \\end{array}$ for pairs of neurons $i , j \\in \\{ 1 , \\ldots , n \\}$ connected by the edges of these graphs. State is encoded in variables $\\sigma ( i , j )$ at synapses, representing edges of graph $G _ { s }$ . The system proceeds in parallel rounds, with new tokens arriving into the system encoded through variables $X ( i )$ at neurons and introduced every $4 L$ rounds, where $L$ is a parameter of the model (e.g., $L = 8$ ). The set of rules being executed (for each round modulo $4 L$ ) is given in the table. The readout of the system also happens through variables $X ( i )$ at the end of each $4 L$ rounds. (a) Set of rules for the simplified version of the BDH model with no neuron inhibitory circuits and no thresholding $( G _ { x } ^ { \\mathrm { ~ i ~ } } = G _ { y } ^ { \\mathrm { ~ i ~ } } = 0 )$ , capturing the general form of the communication structure and synaptic attention of the model. (b) Set of rules for the general case of BDH, including inhibitory circuits $G _ { x } ^ { \\mathrm { ~ i ~ } } , G _ { y } ^ { \\mathrm { ~ i ~ } }$ . An execution of the provided rules is equivalent to the state-space dynamics given by Equation (6). ",
        "bbox": [
            114,
            633,
            883,
            790
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Notes on training. Direct training of the BDH model would be performed by selecting the edges of the considered graphs, and then setting rule weights ${ G _ { x } } ^ { \\mathfrak { c } } ( i , j ) , { G _ { x } } ^ { \\mathfrak { i } } ( i , j ) , { G _ { y } } ^ { \\mathfrak { c } } ( i , j ) , { G _ { y } } ^ { \\mathfrak { i } } ( i , j ) , \\overleftarrow { G } _ { s } ( i , j ) \\overset { \\cdot } { \\geq } 0$ for pairs of neurons $i , j \\in$ $\\{ 1 , \\ldots , n \\}$ connected by the edges of these graphs. ",
        "bbox": [
            116,
            90,
            882,
            135
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "In what follows, we will train a tensor-friendly special case of BDH, called BDH-GPU, relying on an implicit (generally more efficient) representation of the considered graph parameter weights, using a low-rank product representation for the matrices of these graphs. This representation is reminiscent of the hub-labeling graph representation technique, but is directly suitable for describing and evolving high-conductance scale-free networks. The appropriate architecture is introduced in Section 3. ",
        "bbox": [
            116,
            141,
            882,
            210
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "2.3 Interpretation of attention as a micro-inductive bias of reasoning ",
        "text_level": 1,
        "bbox": [
            116,
            234,
            604,
            250
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Rule weights in the edge-reweighting kernel have the interpretation of micro-programs, governed by rules of transformation of state variables of the form $A ( i ) , B ( j )  \\sigma ( i , j )$ and $A ( i ) , \\sigma ( i , j )  C ( j )$ , defined on edges between nodes $i , j$ of some $n$ -node graph. ",
        "bbox": [
            114,
            262,
            883,
            305
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "This formalism can be seen as running an enormous circuit with a form of universal gates given by the transition rules, over a structure of computational elements at nodes, and memory elements on edges of a graph. ",
        "bbox": [
            117,
            310,
            877,
            339
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "While the local rulesets have the form of a rule-based micro-assembly, we leave open the extent to which they should be considered to have an interpretation of programming in logic (as would be the case, e.g., for C-RASP (Yang and Chiang, 2024)). The natural interpretation of $\\sigma ( i , j ) ~ { > } ~ 0$ is a positive bias associated with the neuron pair $( i , j )$ , $i , j \\in \\{ 1 , \\ldots , n \\}$ , which follows from past context. This can be considered by phrasing the local rules of the system in a framework of logic inference; we do so informally, omitting discussion of layers. ",
        "bbox": [
            116,
            345,
            882,
            415
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "If past context $( x _ { \\tau } : \\tau < t )$ implies that implication $i  j$ has weight $\\sigma _ { t - 1 } ( i , j )$ , and if the current state at time $t$ implies that $i$ follows from this state with weight $x _ { t } ( i )$ , then the current state at time $t$ implies that $j$ follows from this state with weight $x _ { t } ( i ) \\sigma _ { t - 1 } ( i , j )$ . ",
        "bbox": [
            173,
            433,
            826,
            476
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The above is intentionally phrased to resemble the logical axiom $( X \\to ( i \\to j ) ) \\to ( ( X \\to i ) \\to ( X \\to j ) )$ , which is perhaps most prevalent across different formalizations of axiomatic logic, with an application of modus ponens as an inference rule. The inference system of the considered model uses state and model weights to devise its own heuristic for the order of evaluation, i.e., to consider which facts appear to be most plausible to be evaluated next, and to evaluate them in an order based on what follows most strongly from context. In a way consistent with what we expect from informal reasoning in language, the considered weights have a more direct interpretation of an increment of utility associated with a given inference.9 In the setting of argumentation, this utility-based approach could, for example, guide the inference process from a pair of known concepts in context, a source and a target, to an intermediate concept likely to be a common-neighbor shortcut lying on a logical path between this source and target (cf. Section 5.3 for a discussion of how this type of mechanism is enforced in the feed-forward network of BDH-GPU). ",
        "bbox": [
            114,
            491,
            882,
            631
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The considered micro-foundational interpretation of attention, defined at the level of individual neurons (or logical variables), does not contradict the way in which Transformer attention is often regarded at the coarser level of vectors through key-query lookup intuitions. At the same time, it highlights that an attention state entry $\\sigma ( i , j )$ (and similarly, a model edge weight leading from $i$ to $j$ ) does not have the interpretation of a logical value (i.e., something that is true or false), but an inductive bias associated with how likely the system is to consider the implication $\\cdot _ { i }  j ^ { \\prime }$ in its next steps of reasoning, when proposing its next conclusions or next ideas for consideration. ",
        "bbox": [
            114,
            637,
            882,
            720
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Chains of implications in BDH guide activations along paths in the system graphs $G _ { x } { } ^ { \\mathfrak { c } } , G _ { y } { } ^ { \\mathfrak { c } } , \\sigma$ . For the latter, attention allows specific implications to enter into paths of thought once the corresponding synapses are open in state $\\sigma$ . ",
        "bbox": [
            119,
            726,
            882,
            756
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "2.4 Interpretation of BDH as an oscillator network toy-model ",
        "text_level": 1,
        "bbox": [
            114,
            777,
            553,
            794
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Whereas the interpretation from Subsection 2.3 focuses on properties which fallow from the computational function (purpose) of the system, here we outline an interpretation of the behavior of BDH considered purely as a dynamical system. ",
        "bbox": [
            116,
            806,
            883,
            848
        ],
        "page_idx": 13
    },
    {
        "type": "image",
        "img_path": "images/3ed1696c3378ea686a934ac15bed78f56d95e5e610e29de541c8141f9d69778d.jpg",
        "image_caption": [
            "Figure 2: The ‘physical system’ representation of BDH as a physical graph toy-model. "
        ],
        "image_footnote": [],
        "bbox": [
            119,
            89,
            426,
            339
        ],
        "page_idx": 14
    },
    {
        "type": "table",
        "img_path": "images/dbca8d64b254d6b975db8d83012832550bda048025fe5daec2c684868a4d146a.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Symbol</td><td colspan=\"2\">Interpretation in:</td></tr><tr><td>Table 1, State Equation (6)</td><td>OscillatorNetwork Toy-Model</td></tr><tr><td>Gx,Gy,Gs</td><td>graph parameters of model</td><td>wires, prods,and elastic connections</td></tr><tr><td>0</td><td>synaptic state of model</td><td>displacement of elastic connections</td></tr><tr><td>x,y</td><td>activation vectors</td><td>pulses at nodes,state correction</td></tr></table>",
        "bbox": [
            439,
            127,
            879,
            306
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Definition of the toy-model. We will consider the toy-model of an $n$ -particle system shown in Fig. 2 as an illustration of the general form of dynamics of the state-space equation (6) of BDH. We draw the $n$ particles in a circle.10 ",
        "bbox": [
            120,
            393,
            882,
            422
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The particles are connected with each other by state elements, represented in Fig. 2 as elastic connectors. The topology of these pairwise connections is given by graph $G _ { s }$ , and may in general be dense. ",
        "bbox": [
            122,
            428,
            880,
            457
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The signal displays dynamics of state $\\rho$ through tension on connectors, which evolves at a slower time scale, and a more pulse-like activation dynamics $x , y$ (on nodes), appearing and vanishing regularly, at a rapid time scale. ",
        "bbox": [
            117,
            463,
            877,
            491
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The slower state dynamics represent, in the first order, oscillation or relaxation of the system of elastic connectors. Once an elastic connector between particles $i$ and $j$ has had its endpoints displaced through state $x$ and $y$ , respectively, a tension appears on this connector, which causes its displacement $\\boldsymbol { \\sigma } ( i , j )$ that relaxes over time (damping variant, corresponding to ALiBi), and/or acts as a spring element (oscillator variant, a simplified illustration of RoPE). Initially, $\\begin{array} { r } { \\sigma ( i , j ) = 0 } \\end{array}$ . ",
        "bbox": [
            116,
            497,
            882,
            566
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The faster dynamics represent the node dynamics of particles. Over time, pulse displacements $x ( i )$ happen at nodes, as a result of either previous behavior of the system, or perturbation by an external forcing field (in reality this field would be language input). A node $i$ with displacement $x ( i )$ may, due to the aggregated action of tension of elastic connectors $\\sigma ( i , \\cdot )$ adjacent to it, activate a system of prods $G _ { y }$ adjacent to it, perturbing nodes it hits in this way. If another node $j$ is prodded sufficiently hard, it may cause it to activate a perturbation $y ( j )$ . The perturbation $y ( j )$ of a node $j$ will, in the next step, propagate again to those other nodes $i ^ { \\prime }$ , which are connected to $j$ by a system of wires $( G _ { x } )$ . If the aggregated pull of wires on a node $i ^ { \\prime }$ is sufficiently strong, this modifies its pulse displacement $x ( i ^ { \\prime } )$ . The pulse activation $y \\bar { ( j ^ { \\prime } ) }$ of some node $j ^ { \\prime }$ , directly followed by pulse activation $x ( i ^ { \\prime } )$ of node $i ^ { \\prime }$ , results in an increase in the tension on the connector $( i , j )$ , adding to the value of the tension $\\pmb { \\sigma } ( i ^ { \\prime } , j ^ { \\prime } )$ . All pulse activations $y$ subside, and the pulses propagate, consequently altering the slow state $\\sigma$ . ",
        "bbox": [
            114,
            573,
            882,
            712
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "In general, $\\sigma ( i ^ { \\prime } , j ^ { \\prime } )$ is triggered simply by the temporal connection between the pulse $y ( j ^ { \\prime } )$ activating, followed by the pulse $x ( i ^ { \\prime } )$ activating immediately afterwards, even if there was no direct causality between the two (although $\\overset { \\cdot } { y } ( j ^ { \\prime } )$ contributed to pulse $x ( i ^ { \\prime } )$ happening if $( j ^ { \\prime } , i ^ { \\prime } ) \\in G _ { x } ,$ ). An appropriate correspondence of the graphs, $G _ { s } \\subseteq G _ { x }$ , would bring the system close to an observed causal effect on the activated synapse. ",
        "bbox": [
            114,
            718,
            882,
            773
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The above description of the pulse dynamics was given from the perspective of nodes. From the perspective of connectors, an existing tension on some connector $\\sigma ( i , k )$ propagates through prods $G _ { y }$ to some nodes $j$ , then through wires $G _ { x }$ to some nodes $i ^ { \\prime }$ , and this finally contributes to tensions on other connectors $\\pmb { \\sigma } ( i ^ { \\prime } , j ^ { \\prime } )$ . This propagation of state thus happens to 3-hop neighbors, through $i , j , i ^ { \\prime }$ . ",
        "bbox": [
            116,
            779,
            882,
            835
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "During training, the behavior of the system may, in even longer time scales, result in the propagation of changes of connection weight and structures to graphs $G _ { x }$ and $G _ { y }$ , as well as (optionally) $G _ { s }$ . ",
        "bbox": [
            119,
            842,
            883,
            871
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Effects captured by the toy-model. We have described a small local graph kernel, with 3-hop locality, capturing the two key effects of the local graph kernel. ",
        "bbox": [
            119,
            90,
            882,
            119
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The first effect is the graph form of communication pattern between nodes, and thresholding of updates. (We have omitted direct mention of inhibition from discussion of the toy-model, but it is direct to include.) ",
        "bbox": [
            116,
            126,
            879,
            154
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The second effect is the placement of attention state on node connections, its update patterns, and the dynamics of its relaxation over time. ",
        "bbox": [
            117,
            160,
            879,
            188
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We intentionally convey the interpretation of node pulses as a differential (gradient) of state on node connections. This interpretation is consistent with our empirical study from Section 7. It is worth considering once every how many steps of the operation of the toy-model, a single element of state $\\boldsymbol { \\sigma } ( i , j )$ is updated. This depends directly on the sparsity of the pulse signals $y ( i ) , x ( j )$ ; at least one of them is, generally, sparse. If the pulses where to happen very seldom for such a pair $( i , j )$ , state updates are essentially a “second-order” correction effect. By adjusting the frequency of updates, the system can be made to operate exactly at the critical point where this pulse dynamics ceases to be a second-order correction of state $\\boldsymbol { \\sigma } ( i , j )$ , giving the random variable describing the time between updates of a connection pair $\\boldsymbol { \\sigma } ( i , j )$ a heavy power-law-like tail distribution (possibly with different distribution parameters for different pairs $( i , j ) ,$ ). ",
        "bbox": [
            114,
            194,
            882,
            320
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In the description of state dynamics, we noted the hop-distance of 3 in the forward propagation of changes to state. Bearing this in mind is helpful when considering how a gradient backpropagation mechanism would follow dependencies between changes of state if such a system were to have its graph weights altered through backpropagation. ",
        "bbox": [
            116,
            325,
            882,
            367
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Finally, let us clarify the specific choice of kernel we made for BDH. We found it to work well, and we knew how to train BDH models which implement it on GPU (which we will call BDH-GPU). This, with current hardware, made it $1 0 ^ { 2 } - 1 0 ^ { 5 }$ times more cost- and time-effective to train models and analyze outcomes than kernels, for which we only knew how to train on CPU. Nonetheless, the question of finding optimal kernels according to different criteria (e.g.: minimality of kernel, best training rate per token, closeness to brain function based on known evidence from brain studies), is an extremely pertinent foundational problem. The problem can be phrased in a “closed-ended” way, leaving a finite number of possibilities to be checked, at least when considering small graph kernels. Some kernels may also prove to have superior learning capabilities to the Transformer (and BDH), and if this quality difference is overwhelming, they may eventually prove commercially viable. ",
        "bbox": [
            114,
            372,
            882,
            500
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In the following, we formalize the choice of kernel for BDH, and also provide a framework to describe other kernels capturing the same effects of graph communication and synaptic attention. ",
        "bbox": [
            119,
            503,
            882,
            534
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "2.5 Expressing BDH using brain models ",
        "text_level": 1,
        "bbox": [
            116,
            559,
            406,
            574
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The results we obtain for BDH provide direct corollaries on the expressiveness of brain models which are capable of emulating the local graph kernels of BDH. Specifically, a distributed system, which is able to efficiently emulate the local kernels of BDH, has sufficient expressiveness to perform language inference and reasoning at least to the same extent as BDH. ",
        "bbox": [
            116,
            588,
            882,
            643
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Observation 2. The local ruleset of BDH (Table 1) can be expressed through a combination of simple mechanisms: neuron activation with positive state variables, Hebbian learning, and communication through excitatory and inhibitory circuits with thresholding. □ ",
        "bbox": [
            114,
            652,
            882,
            695
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We note that in the description of the rulesets in Table 1, Round $( 4 l + 2 )$ and $( 4 l + 3 )$ directly describe the use of excitatory and inhibitory circuits with integrate-and-fire thresholding at neurons. Round $( 4 l + 2 )$ additionally includes a form of competition effect between neurons, realized fully locally at a neurons using the multiplication effect of replicator dynamics. The communication rule of Round $( 4 l + 1 )$ involves the potentiation of a synapse based on activations of neurons at its endpoints. As was discussed in Subsection 2.1, the natural mechanism for implementing increase in synaptic strength is through spiking dynamics, where the execution of the communication rule of Round $( 4 l + 1 )$ is a stochastic AND-gate on signals. Finally, Round (4l) describes the long-term effects of using a strengthened synapse for transmission of signals, and its strength decrease. ",
        "bbox": [
            114,
            710,
            882,
            821
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We can use the framework of expressiveness, as captured in Observation 2, to shed light on the capabilities of natural systems through their ability to emulate artificial ones. Specifically, if a natural system A can plausibly emulate some artificial system B by using the resources it has at its disposal, and artificial system B is able to solve a problem P, this can be used to explain: (1) why the natural system A is sufficiently powerful to solve problem P, and (2) plausibly, that the purpose for which system A is equipped with certain mechanisms includes solving problem P, if such mechanisms prove useful in the emulation of B. ",
        "bbox": [
            116,
            828,
            882,
            911
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The experimental validation of the performance of BDH architecture at Transformer level (Section 4.2) confirms that BDH is sufficient to provide language and reasoning function at scale. We can thus make the following statement. ",
        "bbox": [
            122,
            90,
            883,
            119
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Empirical Finding 2. The Hebbian learning mechanism is plausibly needed, and in combination with neural circuits, sufficient, for performing the reasoning function at the scale of the brain. This includes performing language function with attention, and performing thought processes, at a time scale of minutes. ",
        "bbox": [
            116,
            123,
            882,
            166
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "In view of our results, Hebbian learning can be seen as a form of unsupervised learning over time, expressed through graph edge reweighting, to perform reasoning and language inference using the attention mechanism. This type of result can be compared to an analogous interpretation for Hebbian learning in the context of vision, as pioneered in (Brunel, 1996). With the setting of language and chain-of-thought reasoning, we are able to directly capture effects of time in the brain. ",
        "bbox": [
            116,
            176,
            882,
            246
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Given the interpretation of neuron activations as carrying the necessary gradients of synaptic state (Section 2.4), the problem of supervised learning (i.e., taking into account feedback signals) plausibly becomes deferred to a selective transfer and re-encoding of gradients from state into weights, at longer time scales. We return to a discussion of this point in the Conclusions, bearing in mind the fact that the general difficulty of the problem is now reduced through restrictions on the considered edge-reweighting kernel, and the relative rarity of synapse activation events. ",
        "bbox": [
            114,
            252,
            882,
            321
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Our work also suggests a framework for further discussion of reasoning function, with an anchoring point for this type of investigation in the time-scale of ‘split-seconds’ to ‘minutes’. The question of shorter time scales is then one of designing more precise communication and computational primitives for spiking neurons and synaptic plasticity, which can be used to perform primitives for individual rules of graph kernels for the inference dynamics.11 The question of longer time scales, and the changes to model structure that follow in a learning process, naturally follows any explanation of unsupervised (Hebbian) learning from the shorter time scale that is considered here, as a mechanism of transfer from state to weights; we come back to this point in the Conclusions. ",
        "bbox": [
            116,
            328,
            882,
            425
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "3 BDH-GPU: a tensor-friendly version of the BDH architecture ",
        "text_level": 1,
        "bbox": [
            116,
            444,
            658,
            462
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "We will now introduce BDH-GPU, a variant of the BDH reasoning system, expressed in the language of tensor operations typical for Deep Learning models. BDH-GPU provides a GPU-compatible implementation of BDH. BDH-GPU can be easily implemented in PyTorch, a didactic code listing is provided in Appendix E). Furthermore, BDH-GPU can be trained on large text datasets using error backpropagation, and has been shown experimentally to match performance of GPT-based LLMs. ",
        "bbox": [
            116,
            476,
            882,
            545
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "The main steps towards the efficient implementation of BDH-GPU on GPU are: ",
        "bbox": [
            116,
            551,
            637,
            566
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "1. Express graphs $G _ { x }$ and $G _ { y }$ a low-rank factorizations of their transition matrices, followed by ReLU nonlinearities (Nair and Hinton, 2010) (we explore graph properties of these approximations in Section 5). We never materialize these matrices, but maintain instead a low dimensional state per each neuron.   \n2. Never materialize the $\\sigma$ state matrix, preferring instead to access it using a linear attention operation over low-rank representation of values (we explore the properties of this attention mechanism in Section 6).   \n3. Normalize all state variables using LayerNorm (Ba et al., 2016b). ",
        "bbox": [
            153,
            577,
            883,
            671
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "We will refer to the architecture in the final intermediate step, before the introduction of LayerNorm, as BDHNormfree. ",
        "bbox": [
            112,
            681,
            879,
            710
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "3.1 Notation for BDH-GPU ",
        "text_level": 1,
        "bbox": [
            117,
            726,
            318,
            741
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "We consider the BDH-GPU $( n , d )$ architecture parameterized by positive integers $n , d$ . The system scales in dimension $n$ — the number of particles. In what follows, we will use the terms particle and neuron interchangeably. Dimension $d$ is a measure of the number of parameters per neuron required to represent the interaction of this neuron with the particle interaction field or interaction graph. For asymptotic analysis, we assume that $n  + \\infty$ is the basis for all asymptotics, and $n \\gg d > C \\log n$ holds for some sufficiently large constant $C > 0$ . For the tensor representation of the model, which is the primary one for implementation and empirical studies here in this paper, vectors in $R ^ { d }$ have an interpretation as (fuzzy) addresses of a virtual memory space of size $n$ , hence the assumption $d = \\Omega ( \\log n )$ cannot be dispensed with while using natural (linear-algebraic) arithmetic on real numbers. We later show how to avoid this assumption in graph-based models, by using uniform local graph kernels of smaller degree with a graph communication structure. ",
        "bbox": [
            116,
            751,
            882,
            862
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            119,
            90,
            883,
            119
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Nonlinearities: ReLU and LayerNorm. In what follows, we assume that a one-dimensional vector is denoted by a lower-case letter, e.g., $z$ , with $z \\in R ^ { n \\times 1 } \\cong R ^ { n }$ unless otherwise stated. Vectors which appear in dimension $d$ are named with an asterisk, e.g., as $z ^ { * } \\in R ^ { d \\times 1 }$ . We denote the ReLU operation $\\left( z \\right) ^ { + } : = \\operatorname* { m a x } _ { i \\in 1 , \\ldots , n } \\{ 0 , z _ { i } \\}$ . ",
        "bbox": [
            114,
            132,
            883,
            178
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We further define LayerNorm of a vector $z ^ { * } \\in R ^ { d \\times 1 }$ in a uniform non-parametric way, $\\begin{array} { r } { \\mathsf { L N } \\left( z ^ { * } \\right) = \\frac { z ^ { * } - 1 \\mathbb { E } _ { d } z ^ { * } } { \\sigma _ { d } z ^ { * } } } \\end{array}$ , where $\\mathbb { E } _ { d }$ and $\\sigma _ { d }$ are estimators of mean and standard deviation in dimension $d$ , respectively. ",
        "bbox": [
            114,
            185,
            880,
            217
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Activation vectors and parameter matrices. In vectors representing activations, each scalar element (element of $R _ { . }$ ) of the activation vector has the interpretation of a ‘scalar’ activation state of a single particle. Throughout this text, $R$ is generally assumed be the field of real numbers $R : = \\mathbb { R }$ , and scalars are represented by a fixed-precision floating point number in experiments.12 ",
        "bbox": [
            114,
            229,
            883,
            286
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "By convention, in discussions of parameters, matrices denoted $G _ { x } , G _ { y } , G _ { s } \\in R ^ { n \\times n }$ will represent neuron-neuron interaction, encoders $E \\in \\mathcal { R } ^ { d \\times n }$ reduce dimensionality of activation vectors (e.g., $\\boldsymbol { a } ^ { * } ~ = ~ E \\boldsymbol { z }$ for $z \\in R ^ { n }$ ), and decoders $D \\in R ^ { n \\times d }$ lift them back into $R ^ { n }$ (e.g., $z ^ { \\prime } = D a ^ { * }$ ). ",
        "bbox": [
            114,
            292,
            883,
            337
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Depending on the architecture variant considered, the state will either have the interpretation of a neuron-neuron correlation matrix $\\pmb { \\sigma } \\in R ^ { n \\times n }$ , or a compressed form with reduced dimensionality, $\\pmb { \\rho } = \\mathbf { \\dot { E } } \\pmb { \\sigma } \\in R ^ { n \\times d }$ . ",
        "bbox": [
            114,
            342,
            879,
            371
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "3.2 Definition of BDH-GPU as a state-space system ",
        "text_level": 1,
        "bbox": [
            114,
            386,
            482,
            401
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We now define the main architecture of this paper in its tensor flavor, called BDH-GPU. ",
        "bbox": [
            112,
            411,
            691,
            426
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Definition 4 (inference dynamics of BDH-GPU). A BDH-GPU state-space system $B D H { - } G P U ( n , d )$ , given by three parameter matrices: $E \\in R ^ { d \\times n }$ and $D _ { x } , D _ { y } \\in R ^ { n \\times d }$ , performs iteration over time $t = 0 , 1 , 2 \\ldots$ and layers $l =$ $1 , 2 \\dots L$ , governed for any time $t$ by the following recurrence: ",
        "bbox": [
            114,
            428,
            883,
            472
        ],
        "page_idx": 17
    },
    {
        "type": "equation",
        "img_path": "images/f309c067eb4afdc84514b6622383a7b76e21c48544f487f9947d6a58077a164a.jpg",
        "text": "$$\n\\begin{array} { r l } & { \\displaystyle x _ { t , l } : = x _ { t , l - 1 } + \\left( D _ { x } v _ { t , l - 1 } ^ { * } \\right) ^ { + } } \\\\ & { \\displaystyle a _ { t , l } ^ { * } : = \\sum _ { \\tau < t } v _ { \\tau , l - 1 } ^ { * } { x _ { \\tau , l } } ^ { T } U ^ { t - \\tau } x _ { t , l } } \\\\ & { \\displaystyle y _ { t , l } : = \\left( D _ { y } \\mathsf { L N } \\left( a _ { t , l } ^ { * } \\right) \\right) ^ { + } \\odot { x _ { t , l } } } \\\\ & { \\displaystyle v _ { t , l } ^ { * } : = \\mathsf { L N } \\left( E y _ { t , l } \\right) } \\end{array}\n$$",
        "text_format": "latex",
        "bbox": [
            387,
            474,
            604,
            574
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "where inputs to the system are provided through the boundary condition $v _ { \\tau , 0 } ^ { * }$ in layer 0, for $\\tau = 0 , 1 , 2 \\dots t$ ",
        "bbox": [
            119,
            574,
            813,
            590
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Here, $U \\in R ^ { n \\times n }$ is a diagonal or block-diagonal matrix representing local rotation or damping of state (such as ALiBi or RoPE), $L \\in \\mathbb { N }$ is the number of layers. ",
        "bbox": [
            114,
            597,
            892,
            626
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "BDH-GPU as a language model. BDH-GPU is intended to be used as a language model, processing one token per time step, in which case the input $v _ { t , 0 } ^ { * }$ , for $t \\in \\mathbb N$ , is obtained using some (linear) encoding function from the token alphabet $\\Omega$ , $f _ { e } : \\Omega \\to R ^ { d }$ , as applied to the $t$ -th input tokens. Similarly, the logits of the $t { \\cdot }$ -th output token are extracted using some decoding function applied to outputs of the $L$ -th layer $v _ { t , L } ^ { * }$ , using a (linear) token decoder function $f _ { d } : R ^ { d } \\to \\Omega$ . The source of language tokens may be external, as is the case for next token prediction tasks, or auto-regressive. ",
        "bbox": [
            114,
            638,
            882,
            729
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "For training, we assume that a model $M$ trained in the $\\mathsf { B D H - G P U } ( n , d )$ architecture has the trainable parameter set $M = ( E , \\bar { D } _ { x } , D _ { y } , f _ { e } , f _ { d } )$ , with all parameters trained together. The model has $3 n d + 2 \\Omega d = ( 3 + o ( 1 ) ) \\bar { n d }$ parameters, i.e., the scalable part of the model is concentrated in the total of $3 n d$ parameters of the matrices $( E , D _ { x } , D _ { y } )$ . ",
        "bbox": [
            114,
            734,
            883,
            779
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "State-space representation. The notation of Definition 4 is chosen so as to exhibit its direct applicability in a Transformer-like token-parallel training framework. Vector $v _ { \\tau , l - 1 } ^ { \\ast }$ has the interpretation of attention ‘value’ inputs at time $\\tau$ in layer $l$ . Vector $a _ { t , l } ^ { * }$ represents the result of a linear attention mechanism for time $t$ in layer $l$ . ",
        "bbox": [
            114,
            791,
            883,
            835
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Denoting in (4) the model’s attention state as ",
        "bbox": [
            116,
            92,
            410,
            106
        ],
        "page_idx": 18
    },
    {
        "type": "equation",
        "img_path": "images/be75515204ee15a1cad60f92f4d659e845d1e9fd25a523b3a52be577e0f13373.jpg",
        "text": "$$\n{ \\pmb \\rho } _ { t - 1 , l } = \\sum _ { \\tau < t } v _ { \\tau , l - 1 } ^ { * } x _ { \\tau , l } { } ^ { T } U ^ { t - \\tau }\n$$",
        "text_format": "latex",
        "bbox": [
            397,
            117,
            601,
            152
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "we obtain the equivalent but more compact form of representing the inference dynamics of BDH-GPU as a state-space model, presented in Fig. 3, Eq. (8). ",
        "bbox": [
            114,
            164,
            882,
            194
        ],
        "page_idx": 18
    },
    {
        "type": "image",
        "img_path": "images/7f0bf1af865cd3a2947d3c60488e28bdaafe37d56c22842bdac21672a9218165.jpg",
        "image_caption": [
            "Figure 3: State-space equations of the model architectures introduced in this paper. All architectures refer to a set of $n$ interacting particles (neurons), with activation vectors $x _ { t , l } \\in ( R ^ { + } ) ^ { n }$ . Vector $y _ { t , l } \\in ( R ^ { \\bar { + } } ) ^ { \\bar { n } }$ , $y _ { t , l }$ is (typically) sparse in the sense of $\\| y _ { t , l } \\| _ { 0 }$ . Variables $\\pmb { \\rho } _ { t , l } \\in R ^ { n \\times d }$ or $\\sigma _ { t , l } \\in R ^ { n \\times n }$ represent hidden state of the system. $\\diamond$ The graph-based BDH dynamics equation (6), equivalent to the ruleset from Table 1, serves as a starting point for development of architectures represented as local graph kernels in a distributed computing system. $\\diamond$ The simplified BDH-Normfree equation (7) is a special case of BDH. Up to lack of LayerNorms, it approximates the inference dynamics of BDH-GPU, with the correspondence ${ \\pmb \\rho } _ { t , l } = E { \\pmb \\sigma } _ { t , l }$ . $\\diamond$ The tensor-based BDH-GPU architecture is described by equations (8) (mathematically equivalent to Definition 4, Eq. (4) and (5)) and is the primary point of reference for all model training and all empirical results presented in this study. For a discussion of extensions to BDH-GPU such as heads, see Subsection 4.1. A complete code listing for BDH-GPU is provided in Appendix E. "
        ],
        "image_footnote": [],
        "bbox": [
            117,
            210,
            879,
            463
        ],
        "page_idx": 18
    },
    {
        "type": "image",
        "img_path": "images/f3075203e3c4dbcd0903621e983b7b14042a4d87eb5b85f12816411eaa0de587.jpg",
        "image_caption": [
            "Figure 4: Scaling of BDH-GPU architecture in dimension $n$ . The other parameters can be considered fixed during scaling. For example, with choice of $d = 2 5 6$ for low-rank dimension, $k = 2$ for neuron pairing with RoPE, and $h = 1$ for a single-head architecture, the model scales linearly in dimension $_ n$ in chunks of $d h k = 2 5 6 \\cdot 2 \\cdot 1 = 5 1 2$ parameters. "
        ],
        "image_footnote": [],
        "bbox": [
            331,
            621,
            691,
            808
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "In what follows, we will perform analysis focusing on the state-space representation of the architecture given by Eq. (8). ",
        "bbox": [
            112,
            882,
            877,
            912
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "3.3 Interpretation of BDH-GPU as a local interacting particle system ",
        "text_level": 1,
        "bbox": [
            114,
            90,
            606,
            107
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "The BDH-GPU dynamics equation (8) has the interpretation of a $n$ -particle system, with the state $\\rho _ { t } ( i )$ of the $i$ -th particle, $i = 1 , \\ldots , n$ , given at the end of time $t$ by a vector in $R ^ { d }$ for each layer: ",
        "bbox": [
            112,
            117,
            880,
            148
        ],
        "page_idx": 19
    },
    {
        "type": "equation",
        "img_path": "images/431dda4101d15c36fb3f9d54b0bd134aab9348e50c370387d603acfa323e26d1.jpg",
        "text": "$$\n\\pmb { \\rho } _ { i } ( t ) : = ( \\pmb { \\rho } _ { t , l } { \\bf \\rho } _ { ( i , \\cdot ) } : l \\in ( 1 , \\dots L ) ) .\n$$",
        "text_format": "latex",
        "bbox": [
            383,
            156,
            614,
            176
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Overall, as we will see directly, the way particle $i$ interacts with other particles at time $t$ is described by the following tuple $Z _ { i }$ : ",
        "bbox": [
            114,
            185,
            883,
            214
        ],
        "page_idx": 19
    },
    {
        "type": "equation",
        "img_path": "images/9cdd8a533a219e3fe83792b3555e257359db92abb216c19ae5e749e2230c14ee.jpg",
        "text": "$$\nZ _ { i } ( t ) : = ( \\rho _ { i } ( t ) , E _ { ( i , \\cdot ) } , D _ { x ( \\cdot , i ) } , D _ { y _ { ( \\cdot , i ) } } ) .\n$$",
        "text_format": "latex",
        "bbox": [
            364,
            217,
            632,
            238
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Here, $\\rho _ { i } ( t )$ represents the in-context state associated with particle $i$ (initialized as 0 at the start of inference), while the other three vectors of length $d$ associated with this particle are trainable, but do not change during inference. ",
        "bbox": [
            116,
            244,
            882,
            273
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "The system scales in dimension $n$ and is completely uniform in this dimension, excepting following effect. Let $k$ denote the size of largest block in the block-diagonal matrix $U$ ; then particles, are bound by this effect into nonuniform $k$ -tuples when $k > 1$ ( $k = 1$ when $U$ is the ALiBi matrix, and $k = 2$ when $U$ is the RoPE matrix). Thus the system, in general, scales in the dimension of $n$ uniformly, in chunks of $k$ particles (see Fig. 4). ",
        "bbox": [
            114,
            279,
            882,
            337
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "The interaction between particles is, intuitively, local. To be able to proceed with discussion with rigor and without complicating notation, we assume for the analysis that $k = 1$ . We also drop LayerNorms from the equations of inference dynamics. (Models generally do not train following BDH-GPU without any LayerNorm, but we observed empirically that there is some flexibility as to where these LayerNorms are placed; they can also be moved to the neuron dimension $n$ , and they are parameter-free.) The dynamics without LayerNorm are represented under the name BDH-Normfree in Fig. 3. ",
        "bbox": [
            114,
            340,
            882,
            425
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "We have the following. ",
        "bbox": [
            116,
            431,
            267,
            445
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Observation 3 (local particle interaction ‘by mean-field’). The BDH-Normfree dynamics have the interpretation of a mean-field interaction between particles, fully characterized at any time by $O ( d L )$ parameters of particle in state, and $O ( d )$ parameters in particle representation. ",
        "bbox": [
            114,
            452,
            883,
            494
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "This observation is essential for the subsequent discussion, and it can be expanded in three different ways. ",
        "bbox": [
            119,
            506,
            807,
            522
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In computing terms, at any time $t$ and in any layer $l$ , the action of the system can be represented as an iterated application of the dynamics equations (4), with each of the particles realizing, for each equation in each layer (i.e., a total of $3 L$ times), a form of micro-program, involving local computation and communication with other particles by broadcast. In a framework of local distributed computing (cf. e.g. (Peleg, 2000)), it would be represented as a node performing the following form of local kernel as a part of a networked system: ",
        "bbox": [
            116,
            526,
            883,
            598
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "1. compute some message vector $m _ { i } \\in R ^ { d }$ locally (without communication with other particles), based only on current activation $x _ { t , l , i } , y _ { t , l , i }$ and previous state $Z _ { i } ( t - 1 )$ ,   \n2. broadcast message $m _ { i } \\in R ^ { d }$ to other particles,   \n3. receive the mean-field message $\\begin{array} { r } { \\bar { m } = \\sum _ { j = 1 } ^ { n } m _ { j } \\in R ^ { d } } \\end{array}$ , identical for all particles,   \n4. update local activation variables for the next layer $l + 1$ , and update new state $\\sigma _ { i } ( t ) \\subseteq Z _ { i } ( t )$ , based on the received result $\\bar { m }$ of the broadcast and local computation. ",
        "bbox": [
            151,
            609,
            885,
            723
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In Physical terms, we observe that the interaction field of the particles, which realizes the broadcast, is localized, and can at any time $t$ be expressed as a sum of pairwise particle interaction terms between particles $i , j \\in { 1 , \\dots , t }$ . These pairwise interactions depend only on parameters $Z _ { i } ( t - 1 )$ and $Z _ { j } ( t - 1 )$ , and the activation variables of these particles, representing properties of these particles at time $t$ and expressible through $O ( L d )$ scalars. This interaction field evolves with time $t$ together with $Z _ { i }$ and $Z _ { j }$ . 13 ",
        "bbox": [
            114,
            734,
            882,
            806
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In Engineering terms, we observe that any transformation of a length- $\\mathbf { \\nabla } \\cdot n$ vector into another length- $^ n$ vector passes through an intermediary low-rank representation of dimension at most $d$ . An example is the equation for $x _ { t , l }$ in (7), which reduces length- $\\mathbf { \\nabla } \\cdot n$ vector $y _ { t , l }$ to a length $d$ -vector through application of the encoder matrix $E$ , before lifting the dimension back to $n$ by an application of the decoder $D _ { x }$ . ",
        "bbox": [
            116,
            810,
            882,
            867
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ",
        "text_level": 1,
        "bbox": [
            112,
            90,
            647,
            107
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "BDH-GPU and BDH both represent $n$ -particle systems. For a special parameter choice (of BDH), they have equivalent patterns of communication and of computation (up to placement of layer norms). ",
        "bbox": [
            112,
            117,
            885,
            146
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Observation 4 (BDH-Normfree is a special case of the BDH graph model). Models in the BDH-Normfree architecture (Eq. (8)) and models in the BDH architecture (Eq. (6)) are formally equivalent (i.e., the same model) subject to the following choice of model parameters of BDH: ",
        "bbox": [
            114,
            150,
            882,
            193
        ],
        "page_idx": 20
    },
    {
        "type": "equation",
        "img_path": "images/2cea66e67f084741667e37dff4e1963bb48e3cb6671508dc46c47ccae2b93c61.jpg",
        "text": "$$\nG _ { x } { } ^ { \\ e } - G _ { x } { } ^ { \\ i } = D _ { x } E , \\quad G _ { y } { } ^ { \\ e } - G _ { y } { } ^ { \\ i } = D _ { y } E , \\quad G _ { s } = \\mathbf { 1 } ^ { n \\times n } ,\n$$",
        "text_format": "latex",
        "bbox": [
            302,
            202,
            692,
            220
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "where $\\mathbf { 1 } ^ { n \\times n }$ is the all-ones matrix. ",
        "bbox": [
            117,
            229,
            343,
            243
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "We discuss in more details below how BDH compares to BDH-Normfree in terms of size of state and number of parameters needed for one architecture to approximate the inference dynamics of the other. In general, BDH is not less expressive than its tensor-based counterpart. ",
        "bbox": [
            116,
            256,
            883,
            297
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "For BDH-GPU parameters and state are naturally expressed using tensors of $O ( n d )$ model parameters. In this section, we discuss how to express model parameters and state of BDH, in such a way as to maintain comparable size of parameter and model space. ",
        "bbox": [
            116,
            304,
            883,
            347
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "3.4.1 Expressing matrices $D _ { x } , D _ { y } , E$ as graphs $G _ { x } , G _ { y }$ ",
        "text_level": 1,
        "bbox": [
            114,
            362,
            504,
            378
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "We start by taking care of the first correspondence, that of parameter spaces of BDH-GPU and BDH. Asymptotically, BDH is strictly more expressive at the same number $O ( n d )$ parameters. Recall from Eq. (8) that the parameter space of BDH-GPU consists of three matrices $D _ { y } D _ { x } \\in R ^ { n \\times d }$ , $E \\in R ^ { d \\times n }$ , and (up to shifting of LayerNorms), their role is to encode the pairs of matrices $D _ { y } E , D _ { x } \\bar { E } , \\in \\bar { R } ^ { n \\times n }$ , as used in Eq. (7). In the Claim below, we capture the correct encoding of one such matrix pair in the form of a graph of $O ( n d )$ parameters. ",
        "bbox": [
            114,
            387,
            882,
            459
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Consider a (directed, weighted) graph $G \\in R { + } ^ { n \\times n }$ on a set of vertices $V = \\{ 1 , \\ldots , n \\}$ . We will consider a graph which need be directly a sparse graph, but can be represented as a square of a graph with few edges. Formally, we will say that $G \\in \\mathcal { G } ^ { 2 } ( n , m )$ , for some $m \\in \\mathbb { N }$ , if there exists a graph $\\bar { H } \\in R ^ { ( n + \\bar { s } ) \\times \\bar { ( n + s ) } }$ , with vertex set $V \\cup S$ , where $| S | = s$ , such that $G = H ^ { 2 } [ V ]$ , i.e., $G$ is the induced subgraph of $H ^ { 2 }$ restricted to vertex set $V$ , and $H$ has at most $m$ (strictly positive) edges. ",
        "bbox": [
            114,
            463,
            882,
            536
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "For a $G \\in \\mathcal { G } ^ { 2 } ( n , m )$ , we can consider an interpretation of a hidden layer $S$ between input layer $V$ and output layer $V$ . All matrix weights coefficients are restricted to be non-negative, and the two linear layers are sparse with a total of at most $m$ non-negative connections. ",
        "bbox": [
            116,
            540,
            883,
            583
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Graphs in $\\mathcal { G } ^ { 2 } ( n , m )$ are naturally expressed through the edges of the previously defined graph $H$ , using $O ( n \\log n + m )$ parameters. The class $\\mathcal { G } ^ { 2 } ( n , m )$ is strictly more expressive than the class of sparse $\\textmu m$ -edge) graphs on vertex set $V$ . 14 We will refer to the middle layer of vertices $S$ that makes such a representation possible as the sparse synaptic layer, to the graph $H$ on vertex set $V \\cup S$ as the sparse linear circuit, and the graph $H ^ { 2 } \\bar { [ } V ] \\in \\mathcal { G } ^ { 2 } ( n , m )$ as the neuron-neuron interaction graph. ",
        "bbox": [
            114,
            589,
            883,
            660
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "The role of the constructed graphs is to serve for propagating linear dynamics of the form $v \\mapsto G v$ , $v \\in ( R ^ { + } ) ^ { n \\times 1 }$ , for graph-based local models. We have the following Observation. ",
        "bbox": [
            117,
            665,
            883,
            694
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Observation 5. Let $G \\in \\mathcal { G } ^ { 2 } ( n , m )$ be a neuron-neuron interaction graph, with $G = H ^ { 2 } [ V ]$ , where $H$ is the sparse linear circuit on vertex set $V \\cup S$ , which has m edges. Then, the linear dynamics on graph $G$ , $v \\mapsto G v$ , can be efficiently expressed through two steps of linear dynamics on graph $H$ , $v \\mapsto H ^ { 2 } v$ , for $v \\in ( R ^ { \\bar { + } } ) ^ { \\bar { n } \\times 1 }$ . This representation requires $O ( m )$ parameters. □ ",
        "bbox": [
            114,
            698,
            882,
            756
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "In the above, thee exact number of parameters needed to represent a graph of $m$ edges follows from conventions introduced in the Notation (Section 1.4). In what follows, we will assume that BDH represents its parameter matrices through appropriate sparse linear circuit graphs $H$ , which it uses to realize the linear neuron-neuron interaction dynamics $G$ . We illustrate the correspondence between graphs $G$ and $H$ in Fig. 5. ",
        "bbox": [
            114,
            767,
            883,
            824
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "We observe that BDH can express BDH-GPU parameter matrices with the same asymptotic number of parameters.   \nThe claim below applies to pairs of matrices $D E$ , for $D = D _ { y }$ and $D = D _ { x }$ . ",
        "bbox": [
            114,
            829,
            879,
            858
        ],
        "page_idx": 20
    },
    {
        "type": "image",
        "img_path": "images/4739cdbb91dff58d16725f50759d5106f8d6e3332702d67a9125355119edebc2.jpg",
        "image_caption": [
            "Figure 5: Neuron-neuron communication using graphs $G \\in \\mathcal { G } ^ { 2 } ( n , m )$ : correspondence between graph $H$ with $m$ edges (left), and neuron-neuron interaction graph $G = H ^ { 2 }$ (right). The approach allows to express linear signal propagation on a broad class of graphs $\\mathcal { G } ^ { 2 } ( n , m )$ using two steps of linear dynamics on a sparse circuit $H$ , i.e., $\\mathrm { \\Delta } G z = H ^ { 2 } z$ for $z \\in ( \\mathring { R } ^ { + } ) ^ { n }$ . "
        ],
        "image_footnote": [],
        "bbox": [
            215,
            83,
            789,
            284
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Claim 3. For any matrices $D \\in R ^ { n , d }$ , $E \\in R ^ { d , n }$ , there exist neuron-neuron interaction graphs $G ^ { \\mathfrak { c } } , G ^ { \\mathrm { i } } \\in { \\mathcal { G } } ^ { 2 } ( n , m ) ,$ , such that $G ^ { \\mathfrak { c } } - G ^ { \\mathfrak { i } } = D E$ , with $m = O ( n d )$ . In consequence, for the same asymptotic number of parameters $O ( n d )$ , graph-based feed-forward mechanisms of BDH are strictly more expressive than corresponding mechanisms in the tensor-based implementation, BDH-Normfree. ",
        "bbox": [
            114,
            353,
            883,
            412
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Proof. The short proof of the Claim is deferred to Appendix C.3. ",
        "bbox": [
            114,
            430,
            540,
            445
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "We note that the converse implication does not hold: an arbitrary graph $G ^ { \\mathfrak { e } } \\ \\in \\ { \\mathcal { G } } ^ { 2 } ( n , m )$ does not admit an exact low-rank decomposition $G ^ { \\mathfrak { c } } ~ = ~ D E$ . Indeed, in general any low-rank decomposition introduces a form of noise whose implications we discussed in Section 5.3: if $G ^ { \\mathfrak { c } }$ has a modular (cluster) structure, the low-rank approximation $G ^ { \\mathfrak { e } } \\approx D E$ still allows a form of in-cluster propagation dynamics. ",
        "bbox": [
            116,
            462,
            882,
            518
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "3.4.2 Expressing BDH-GPU attention on graphs: sparsification and trainability of $G _ { s }$ ",
        "text_level": 1,
        "bbox": [
            117,
            534,
            718,
            549
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "We recall that by Observation 4, the equivalence between the attention state $\\sigma _ { t , l }$ in BDH and in tensor-based imple mentation holds for the case of the complete directed graph, $G _ { s } = { \\bf 1 } ^ { n \\times n }$ . ",
        "bbox": [
            116,
            558,
            875,
            587
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "This means two things: first, in BDH, graph $G _ { s }$ can be trainable, while in BDH-GPU it is not. This acts to the potential advantage of BDH for expressiveness. ",
        "bbox": [
            116,
            592,
            877,
            621
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Second, in BDH, the graph $G _ { s }$ obtained through the direct correspondence is dense: with $n$ neurons, BDH would need $n ^ { 2 }$ synapses to precisely reflect BDH-Normfree. This aspect is more of a technical nuisance than an actual difference: the expressiveness of the attention mechanism of BDH, equipped with a sparse graph $G _ { s }$ , is sufficient to represent the attention operation as used in BDH-Normfree. Indeed, in the tensor-based BDH-GPU dynamics, the attention operation is immediately followed by a low-rank operation, ${ \\rho } _ { t , l } \\mathrm { ~ = ~ } E \\pmb { \\sigma } _ { t , l }$ , where $\\rho _ { t , l }$ has $^ { n d }$ parameters. Graph models can instead rely on a sparse graph $G _ { s }$ to achieve the same form of state compression through sparsification. ",
        "bbox": [
            116,
            627,
            882,
            710
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Claim 4. The attention block of BDH-Normfree can be expressed using the attention block of BDH with a graph $G _ { s }$ having $O ( n d )$ edges, subject to a natural preparation of attention values entering the attention block of BDH (directly before this attention block). ",
        "bbox": [
            117,
            714,
            882,
            757
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Proof. The formal statement of the Claim and its proof are deferred to Appendix C.4. ",
        "bbox": [
            114,
            775,
            674,
            790
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Going beyond the formal equivalence between BDH and BDH-GPU from Observation 4, the above Claim, combined with Claim 3, shows that BDH has at least the same expressiveness as BDH-GPU even for the same number of parameters $O ( n d )$ and the same size of state $O ( n d )$ per layer. ",
        "bbox": [
            116,
            806,
            883,
            849
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Independent of graph-based models, in the subsequent analysis of the feed-forward network and attention mechanisms of BDH-GPU, we will show that the matrices $D _ { x } E , D _ { y } E , \\sigma \\in R ^ { n \\times n }$ of BDH-GPU admit a natural interpretation as $n$ -node directed graphs (once appropriate threshold functions are applied). For example, the visualizations in Fig. 11 and Fig. 10 correspond to graph representations of matrices $\\sigma$ and $G _ { x } : = D _ { x } E$ of BDH-GPU, respectively, after thresholding. This graph interpretation of matrices in BDH-GPU also defines the neuron-neuron communication graph of the underlying BDH model, given by the equivalence from Eq. (9). ",
        "bbox": [
            116,
            854,
            882,
            911
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            119,
            90,
            883,
            119
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "4 Implementation and scaling laws ",
        "text_level": 1,
        "bbox": [
            117,
            138,
            419,
            156
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "A code framework for BDH-GPU, representing the architecture from Definition 4, is made available in the Appendix E. In this Section, we present some guidelines on choice of hyperparameters, and an empirical study of models implemented in the BDH-GPU architecture, as well as a comparison to the Transformer and other language model architectures. ",
        "bbox": [
            116,
            169,
            882,
            226
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "4.1 Implementation characteristics of BDH-GPU ",
        "text_level": 1,
        "bbox": [
            117,
            241,
            465,
            256
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Model scaling in neuron dimension $n$ . The architecture BDH-GPU $( n , d )$ has two main dimensions, $n$ which is the dimension of its concept (neuronal) space, and $d \\ll n$ , which is its low-rank (synaptic) dimension. The model scales primarily with the number of neurons $n$ . Almost all of the weights of the model are contained in three $n \\times d$ parameter matrices called $E , D _ { x } , D _ { y }$ ; thus, the number of parameters is precisely $( 3 + o ( 1 ) ) n d$ . The ratio between the dimensions $n$ and $d$ increases rapidly (“asymptotically”) with model size; already for a 25M-parameter model, a sound choice of dimensions is: $d = 2 5 6$ , $n = 3 2 7 6 8$ , read as 32768 neurons, each characterized by a total of $3 d = 3 \\cdot 2 5 6 = 7 6 8$ scalar parameters. ",
        "bbox": [
            114,
            266,
            883,
            364
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Layers and heads. The architecture has $L$ layers (e.g., $L = 1 0$ ). As in the Universal Transformer (Dehghani et al., 2019), all layers use the same set of weights for each of the parameter matrices. ",
        "bbox": [
            120,
            377,
            880,
            406
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "The architecture may be equipped with several heads $h$ , subdividing dimension $n$ . The role of heads is limited to a single parameter-free LayerNorm, normalizing outcomes of linear attention separately for each head. The optimal number of heads is typically smaller than in the Transformer (e.g., $h = 4$ ). ",
        "bbox": [
            116,
            412,
            882,
            455
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Linear attention with state aligned to neurons. The state space of the model is fixed and large. It has the macrointerpretation of associative memory (like KV-cache, but organized differently), and is used to perform linear attention. For each layer, the state space is independent and has a fixed dimension $n \\times d$ , the same as the model weight matrices. Thus, a portion of $d$ parameters of a state is directly associated with each of the $n$ neurons. With each token processed, a fraction of the model’s state space is updated. Sharing of state between the $L$ layers is not performed in the vanilla architecture. As usual with SSM’s, there is no notion of a context window. ",
        "bbox": [
            114,
            468,
            882,
            553
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Similarly to BDH, BDH-GPU maintains a large recurrent state comparable in size with its total number of parameters (c.f. Fig. 4). This stems from the fact that both the recurrent state matrix, and parameter matrices are expressed as low rank $d$ factorizations of $n \\times n$ graph transition matrices. We believe that this helps the model with generalization with respect to RNNs which have $\\bar { O ( N ^ { 2 } ) }$ parameters which manipulate a state of size $O ( N )$ . ",
        "bbox": [
            114,
            559,
            877,
            588
        ],
        "page_idx": 22
    },
    {
        "type": "image",
        "img_path": "images/343435b79f8b88d529b84b51f82f2b8a79f4421a129355ebbb4add483b27d65b.jpg",
        "image_caption": [
            "Figure 6: Diagram of one layer of the BDH-GPU architecture, following Eq. (8). Layer inputs are $x _ { l - 1 } , y _ { l - 1 } \\in R ^ { n }$ , layer outputs are $x _ { l } , y _ { l } \\in R ^ { n }$ . Model parameters are contained in the $E \\in R ^ { n \\times d }$ and $\\hat { D _ { x } } , \\hat { D _ { y } } \\in \\dot { R } ^ { d \\times n }$ , and shared across all layers. Each layer has a state $\\pmb { \\rho } _ { l } \\in R ^ { n \\times d }$ which is used in the Linear Attention block and persisted over time. PyTorch code implementing the model is provided in Appendix E "
        ],
        "image_footnote": [],
        "bbox": [
            127,
            616,
            862,
            847
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            119,
            90,
            883,
            119
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Sparse positive activation. The architecture relies on a length- $\\mathbf { \\nabla } \\cdot n$ vector $x _ { t , l }$ passed to the $l$ -th layer for the $t$ -th token processed, which can be assimilated to the vector giving rise to keys, values, and queries in the Transformer, but operating in higher dimension. As a crucial design assumption, this vector has all non-negative elements $( x _ { t , l } \\ge 0 )$ ). ",
        "bbox": [
            114,
            136,
            883,
            178
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "An empirically observed fact is that the activation pattern of $x _ { t }$ rapidly becomes sparse (in a typical training run, only $\\rho \\approx 5 \\%$ of the $n$ entries of vector $x _ { t }$ are non-zero). This corresponds to the fraction of the state space read and updated for each token. ",
        "bbox": [
            116,
            184,
            883,
            227
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "4.2 Comparison of BDH-GPU to GPT2-like Transformers ",
        "text_level": 1,
        "bbox": [
            114,
            242,
            531,
            258
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Architecture differences. BDH-GPU in its vanilla form can be compared to the GPT2 architecture (Radford et al., 2019) with RoPE attention. In this comparison, BDH-GPU retains or strengthens the key advantages of the Transformer (parallel trainability, attention mechanism, scaling laws for loss versus parameter count, learning rate per token) on tests and benchmarks at the model scales we tested (1B parameters), across tasks such as language and translation. ",
        "bbox": [
            114,
            270,
            882,
            325
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "The architecture of a single layer of BDH-GPU is presented in Fig. 6. The most evident architecture differences between BDH-GPU and the Transformer include the following: ",
        "bbox": [
            114,
            330,
            879,
            361
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "− BDH-GPU has fewer parameter matrices, allowing for more compact interpretation and analysis. BDH-GPU scales for parameters (and context length) almost exclusively in a single neuronal dimension, $n$ . Key-value state and parameter matrices have matching dimensions and are highly localized together with state, with portions of these matrices attributable to individual neurons.   \n− There is no notion of context length in BDH-GPU, and consequently no hard bound on it.   \n− Attention of BDH-GPU is linear, but happens in the model’s large neuronal dimension.   \n− Activation vectors $x , y$ of BDH-GPU are positive (after passing through ReLU gates), and vectors $y$ are observed to be extremely sparse in practice. ",
        "bbox": [
            153,
            371,
            883,
            516
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Transformer-like scaling laws. We have experimentally validated the scaling laws of BDH-GPU, expressing loss as a function of parameter count, for next-token-prediction tasks. At the same parameter scale, BDH-GPU generally compares favorably to the Transformer even on relatively short-context tasks requiring use of attention, such as translation, Fig. 7. In general, on next-token prediction tasks, BDH-GPU appears to show improvement of loss reduction per token of data than the Transformer, i.e., learns faster per data token, both for the natural tasks we tested (see e.g. Fig. 7) and on synthetic puzzles. ",
        "bbox": [
            114,
            531,
            882,
            616
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "The BDH-GPU architecture appears to be a preferred choice for training setups where: (1) models need to learn from scarce data, or (2) training workloads need to be optimized for makespan. For the first setting, the training rate per token is the decisive factor. For the second setting, BDH-GPU can be used differently than the Transformer in distributed training and distributed inference setups because of the way it scales its dimensions. ",
        "bbox": [
            116,
            621,
            882,
            678
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "FLOPS counts. The theoretical count of arithmetic operations per token of BDH-GPU during inference is bounded by $O ( n d L )$ . Each parameter is accessed $O ( L )$ times per token (with the typical sufficient number of layers being smaller than in the Transformer), and each element of state is accessed $O ( 1 )$ times per token, with small hidden constants. These are rough bounds for a simple implementation, and do not take advantage of activation sparsity. ",
        "bbox": [
            114,
            693,
            882,
            750
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "For short contexts BDH-GPU is amenable to parallel training with a causal self-attention kernel. The simple code template provided in the Appendix E is sufficient to reproduce the empirical results presented in this paper on a single GPU node. For longer contexts (typically above 4096 tokens for $d = 2 5 6$ ), a state-space kernel for linear attention is faster and more space-efficient. ",
        "bbox": [
            116,
            756,
            882,
            811
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "4.3 Comparison of BDH-GPU to other sequence processing architectures ",
        "text_level": 1,
        "bbox": [
            117,
            829,
            633,
            844
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Transformers with Linear Attention. Linear attention works well when used in high dimension, subject to appropriate preparation of key vectors (as we discuss in Subsection 6.1). An elegant way to eliminate non-linearity of attention, by applying preparation of key vectors through tensor product, was proposed in (Buckman et al., 2024). We use a completely different approach to achieve attention in high dimension. ",
        "bbox": [
            116,
            854,
            883,
            911
        ],
        "page_idx": 23
    },
    {
        "type": "image",
        "img_path": "images/ebcfb0b7c86d4f9c3590e4b512d6f3e09962bdc4a4609486f4b219f4be202693.jpg",
        "image_caption": [
            "Figure 7: Performance of BDH-GPU and GPTXL versus model size on a translation task. We have tested all models under the same training and evaluation regimes. All models show improved performance with scale. BDH-GPU uses exactly the formulation provided in Appendix E, while BDH-GPU’ extends conditional gating of states and logits. All models are trained with truncated backpropagation through time on sequences 2048 characters long, and carry their state ( $\\mathbf { \\sigma } _ { \\rho }$ matrix for BDH models and a buffer of last 4096 KV-Cache entries (Dai et al., 2019) for GPTXL) between minibatches. BDH models are scaled only by varying the number of neurons $_ n$ and keep all other hyperparameters fixed, making them easy to scale. On the other hand, GPTXL were scaled in both the embedding dimension and the number of layers and required Dropout (Srivastava et al., 2014) tuning for optimal performance. We observe that BDH-GPU’ matches the GPT Transformer at all model sizes we have evaluated. Details on model hyperparameters and training setup are provided in Appendix B.2 "
        ],
        "image_footnote": [],
        "bbox": [
            233,
            101,
            756,
            313
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "A much broader line of work on linear attention for the Transformer, initiated by (Katharopoulos et al., 2020) concerns applying linear attention in low dimension after appropriate preparation of keys and values. This is effectively a technique for SSM state compression, and it is not clear whether it relates favorably to other SSM state compression techniques. An empirical study of the amount of information recoverable from SSM’s with compressed state can be found in (Ben-Kish et al., 2025; Liu et al., 2025). ",
        "bbox": [
            116,
            468,
            882,
            537
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "A general theoretical framework for analyzing the expressiveness of Linear Attention models with attention working with positive vectors can be found in the context of the $\\mathrm { F A V O R + }$ framework of the Performer (Choromanski et al., 2021). Finally, a general state-space formalism for Transformer models admitting Linear Attention was considered in (Sun et al., 2023; Liu et al., 2025). ",
        "bbox": [
            116,
            545,
            882,
            601
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Other types of Transformers. Variants of the Transformer with identical parameters in all layers, like the Universal Transformer (Dehghani et al., 2019), have a number of desirable features, notably in terms of explainability and ease of defining metrics. The downside of sharing parameters between layers in the Universal Transformer is a slight time overhead for the feed-forward network operations, when measured in FLOPS per parameter. The situation is similar in BDH-GPU. ",
        "bbox": [
            116,
            617,
            882,
            685
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "BDH-GPU has sufficient expressiveness to prepare keys and queries for the attention operation, so that the outcome of attention captures a similarity measure between keys and queries corresponding to the outcome of a class of Locality Sensitive Hashing (LSH) functions with a very large number of buckets (cf. Subsection 6.1). The study of LSHbased KV-cache for the Transformer was initiated with the Reformer (Kitaev et al., 2020), and the LSH Transformer architecture introduced in the same work. Generally, the LSH Transformer is shown to rapidly approach Transformer baseline behavior in practice as the number of buckets increases. The class of LSH functions considered is not the same, but some intuitions gained in the study of LSH attention may carry over to BDH-GPU. ",
        "bbox": [
            116,
            693,
            882,
            790
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Finally, several lines of work have been devoted to making the Transformer work with longer context windows. Two distinct approaches, which work notably well, are the soft-rolling context window of the TransformerXL (Dai et al., 2019), and hierarchical attention (Yang et al., 2016). The BDH-GPU architecture is, generally, amenable to some of these extensions to the Transformer’s attention mechanism, while also providing new ways to extend context length in a more uniform manner. ",
        "bbox": [
            116,
            796,
            882,
            866
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Networks with sparse activation. The use of the ReLU gate as a systematic way to achieve sparse activation was, to our knowledge, first exhibited in (Haziza et al., 2025). ",
        "bbox": [
            117,
            882,
            875,
            911
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "A recent variant of the Transformer called Spark Transformer (You et al., 2025) relies on a combination of topk operations and soft thresholding to provide a reduction in both attention and feed forward network activations compared to the Transformer, achieving neuron sparse activation of $8 \\%$ . Compared to our work, the method used therein to achieve activation sparsity effects is completely different (and rather involved). Beyond the question of sparsity, BDH-GPU is not more similar to the Spark Transformer than to the Transformer. ",
        "bbox": [
            116,
            90,
            882,
            160
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Oscillatory SSM’s. BDH admits an interpretation at the micro-level as an oscillatory state-space network, as we outlined in Subsection 2.4. The concept of Oscillatory State Space Models has recently been applied to time series analysis (Rusch and Rus, 2025), with the LinOSS model showing encouraging performance relative to other SSM’s (such as Mamba and LSTM’s) on tasks of long-horizon forecasting and time-series classification. Other than this, the use of SSM’s with the form of an oscillator network has been limited to smaller scale studies. We are not aware of any successful application of oscillatory SSM’s to the area of language models and reasoning in language, nor of oscillator network SSM’s at scale whatsoever, prior to BDH. ",
        "bbox": [
            114,
            176,
            882,
            275
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "BDH unifies multiple lines of intuition found across existing models, offering a coherent framework in which the components naturally align. The result is a biologically plausible reasoning model with an interpretable structure and state-of-the-art performance that has been experimentally verified. ",
        "bbox": [
            116,
            281,
            883,
            323
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "5 Analysis: emergence of modularity and scale-free structure ",
        "text_level": 1,
        "bbox": [
            116,
            343,
            638,
            361
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Large-scale reasoning systems appear to benefit from hierarchical structuring into sub-modules. In Machine Learning, the usual approach has been to design such a modular structure, by way of assigning roles and scales to different submodules explicitly. Many works have postulated modules capable of representing hierarchical relationships between features of objects, e.g., capsule networks (Sabour et al., 2017). Some models have attempted to capture intelligence by recreating elements of structure recognized in brain study, going so far as to try to map functional sub-networks of the brain with empirically identified function into specific sub-modules in the design of a larger ML system, cf. (LeCun, 2022). ",
        "bbox": [
            114,
            376,
            882,
            473
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "In this work, we propose a learnable system which ends up with modularity. We show how scale-free modular structure emerges naturally when the model is implemented by a network with local graph dynamics. In this Section, we discuss the emergence of the structure of inter-neuron connections of BDH during training, while in Section 6 we look at its temporal activation patterns during reasoning inference. ",
        "bbox": [
            116,
            479,
            882,
            535
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "The rest of this section is organized as follows. In Subsection 5.1, we introduce basic concepts related to modularity and scale-free behavior of networks. We then look at the expressiveness of feedforward networks of BDH-GPU and their usefulness as a signal propagation dynamics in Subsections 5.2 and 5.3. In Subsection 5.4, we show theoretically how modular graph structure, with appropriate community voting mechanisms, emerges as a plausibly necessary element for the feed-forward networks $D _ { x } E$ and $D _ { y } E$ to function correctly. In Subsection 5.5, we look at the corresponding empirical properties of these matrices, and the scale-free and modularity properties of the corresponding graphs ${ G _ { x } } ^ { \\mathrm { ~ e ~ } }$ and $G _ { y } ^ { \\mathrm { ~ \\mathfrak ~ { ~ e ~ } ~ } }$ of the underlying BDH graph dynamics. ",
        "bbox": [
            116,
            541,
            882,
            640
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "5.1 Background: modularity and scale-free property of systems ",
        "text_level": 1,
        "bbox": [
            114,
            657,
            568,
            671
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Importance of modularity for information propagation. Graph systems serving a function related to information propagation tend to achieve modular graph structure, and rely on it to obtain the most desirable tradeoff between efficiency and accuracy of the system dynamics. Such emergence of “hidden structure” may be observed e.g. through topic specialization of system regions, or through the coordinated voting behavior among nodes which organize themselves into communities, admitting higher local density. This type of graph community self-organization has two main advantages over a system with an explicit partition into subsystems. First, it allows nodes to belong to multiple communities, and to act as bridges between them. Second, it allows the scale and relationship between communities to evolve over time, as their relative importance changes or new connections emerge. ",
        "bbox": [
            116,
            683,
            882,
            794
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Historically, the crucial role of emergent modular structure for systems tasked with efficient knowledge retrieval at scale was first observed in the context of the World Wide Web before the year 2000, notably in the transition from catalogue-based systems (DMOZ Open Directory Project, craigslist, etc.) to naturally evolving systems based on webs of knowledge (Wikipedia, etc.), interlinked topic-based communities (reddit, etc.), and reliance on evolving network link structure for assigning expert weights to nodes in a voting process (Google PageRank, etc.). Formalization of modular properties followed soon after, with the mostly commonly used definition of modularity being proposed by Newman in 2006. The main theoretical reference for studies of modularity is the Stochastic Block Model (Holland et al., 1983) and its later generalizations, e.g., to hierarchical settings. While the definition of Newman modularity is not (efficiently) constructive, it can in practice be closely approximated by greedy algorithms (Blondel et al., 2008) or spectral approaches (Massoulié, 2014). ",
        "bbox": [
            116,
            800,
            882,
            911
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            117,
            90,
            885,
            119
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Scale-free property. The scale-free property of natural systems dealing with information processing is generally accepted as a manifestation of their operation at criticality. This refers to operation within a regime where they are both sufficiently stable to enable efficient information retrieval in the short-term, and sufficiently adaptable to be able change their behavior abruptly as new knowledge inputs become available, invalidating previous paths of reasoning or knowledge retrieval. The generally accepted definition of scale-free behavior of such a dynamical system assumes that the likelihood of a new piece of information (or other localized innovation to the system) to affect $n ^ { \\prime }$ nodes of the system, for any $n ^ { \\prime } < n$ , should by polynomially large in $1 / n ^ { \\prime }$ . For most information propagation dynamics, under certain uniformity assumptions, e.g., that the new piece of information arrives at a uniformly random node of the system, a usual necessary (not sufficient) condition for scale-free property is for the distribution of node degrees to follow a power-law distribution. ",
        "bbox": [
            114,
            133,
            882,
            272
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "In the practice of applied sciences studying real-world network phenomena, and in the absence of the possibility to perform more in-depth analysis, power-law degree distributions are sometimes equated with scale-free behavior. One notable research application involves modeling of extreme events: understanding scale-free behavior allows researchers to make predictions about rare, large events from data on smaller, more common ones. ",
        "bbox": [
            114,
            279,
            882,
            335
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "For systems with known local graph dynamics, like those considered here, more refined analysis of scale-free properties are possible. We nonetheless also report heavy-tailed degree behavior as the most obvious lithmus test indicator of scale-free operation of the system. ",
        "bbox": [
            116,
            340,
            882,
            383
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ",
        "text_level": 1,
        "bbox": [
            116,
            398,
            607,
            414
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Low-rank matrices have been considered in multiple contexts of Machine Learning, from preference vectors to Internet latency estimation. In the setting of the Transformer, low-rank matrices form the basis of weight-matrix approximations such as LoRA (Hu et al., 2021). ",
        "bbox": [
            116,
            424,
            882,
            467
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "The ReLU-lowrank block of BDH-GPU captures different properties than the above settings. Its most important effects for BDH-GPU are related to noise reduction, and faithful representation of a certain class of affinity functions on sparse positive vectors. This makes it suitable for use in combination with Linear Attention blocks. We discuss this point further in this Section. ",
        "bbox": [
            114,
            473,
            882,
            529
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Definition of ReLU-lowrank. The parameters of BDH-GPU are concentrated in three matrices $E , D _ { x } , D _ { y }$ . The encoder matrix $E$ transforms length- $\\mathbf { \\nabla } \\cdot n$ vectors in the neuronal layer into length- $d$ vectors in the hidden layer. The two decoder matrices $D \\in \\{ D _ { x } , D _ { y } \\}$ transform length- $d$ vectors in the hidden layer back to the neuronal layer. ",
        "bbox": [
            114,
            542,
            882,
            587
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "We consider the ReLU-lowrank operation of passing through the encoder, one of the decoders, and a ReLU gate (cf. Eq. (7)), mapping vectors $z \\in R ^ { n }$ into $f _ { D E } ( z ) \\in R ^ { n }$ as follows: ",
        "bbox": [
            112,
            592,
            877,
            621
        ],
        "page_idx": 26
    },
    {
        "type": "equation",
        "img_path": "images/f093458df8a000863a8555e7d854da4d595491e08ea1e102290bd4f9f122884a.jpg",
        "text": "$$\nz \\mapsto f _ { D E } ( z ) : = ( D E z ) ^ { + } .\n$$",
        "text_format": "latex",
        "bbox": [
            408,
            628,
            589,
            647
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "We note that the output $f _ { D E } ( z ) \\in ( R ^ { + } ) ^ { n }$ always, and that in BDH-GPU we also always have $z \\in ( R ^ { + } ) ^ { n }$ . ",
        "bbox": [
            117,
            654,
            808,
            670
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Expressiveness of ReLU-lowrank in BDH-GPU and MLP in the Transformer. A single ReLU-lowrank block can be compared to a single MLP block of the Transformer. A different comparison provides closer matching of dimensions and structure of nonlinearities, by considering a single ReLU-lowrank with respect to a portion of the Transformer corresponding to the second MLP layer in an MLP block, i.e., starting with the hidden layer of neurons of the MLP in some layer $l$ , skipping attention blocks, and followed by the ‘first’ linear layer of the MLP in layer $l + 1$ , finally followed by the non-linearity (typically GeLU) applied in the hidden layer of neurons in layer $l + 1$ . Either approach to expressiveness is valid to the extent where we analyze similarities between one architecture with $L$ layers and the other with “ ${ \\bf \\ddot { \\it O } } ( L )$ ” layers. ",
        "bbox": [
            114,
            681,
            883,
            795
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "In the spirit of universal approximation theorem frameworks, a (deep) layer- $L$ stacking of Transformer’s MLP block with ReLU activation, for Transformer latent dimension $D$ and MLP hidden layer dimension $c D$ (e.g., for $c = 4$ ), is eventually (i.e., for $L \\to + \\infty ,$ ) a universal approximator for all vector functions up to dimension $D - O ( 1 )$ (Shen et al., 2022). A similar universal approximation result eventually (i.e., for $L \\to + \\infty$ ) holds up to function dimension $n$ for residual ReLU-lowrank networks (Lin and Jegelka, 2018), however the convergence rate in $L$ is slower due to the smaller size of the hidden layer. These results translate directly to BDH-GPU architecture which also relies on ReLU with residual connections between layers. To summarize informally, for a Transformer with latent dimension $D$ and BDH-GPU with hidden dimension $d$ , we expect their feed-forward networks to be comparably expressive (though usually without strict mathematical equivalence) as function approximators for functions up to some dimension $\\bar { d } ^ { \\prime } , d < \\bar { d } ^ { \\prime } < D$ , between $d ^ { \\prime }$ and $D$ the Transformer can express a richer class of functions, and between $D$ and $n$ , BDH-GPU can approximate some functions, whereas the Transformer does not use such high dimension in its vector representations. ",
        "bbox": [
            114,
            800,
            882,
            911
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            90,
            883,
            147
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "We remark that in all cases, regardless of expressiveness of feed-forward mechanisms, BDH-GPU is set up so that it is only using inputs and producing outputs within the positive orthant, $( R ^ { + } ) ^ { n } \\mapsto ( R ^ { + } ) ^ { n }$ . ",
        "bbox": [
            114,
            152,
            880,
            183
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The main point to consider is: what classes of useful high-dimensional functions in the positive orthant does ReLUlowrank naturally express? ",
        "bbox": [
            114,
            188,
            877,
            217
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "5.3 ReLU-lowrank as a signal propagation dynamics ",
        "text_level": 1,
        "bbox": [
            114,
            232,
            495,
            247
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Error of low-rank approximation (without ReLU). Consider $R ^ { n }$ as a space spanned by a fixed set of $n$ orthogonal unit basis vectors $V = \\{ v _ { 1 } , \\ldots , v _ { n } \\}$ , called nodes. ",
        "bbox": [
            114,
            256,
            882,
            286
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The low-rank operation can be used to approximate affinities between pairs of nodes, in the following sense. For a given matrix $G ^ { \\prime } \\in R ^ { n \\times n }$ , consider low-rank matrices $D \\in R ^ { n \\times d } , E \\in \\mathsf { \\bar { \\Gamma } } R ^ { d \\times n }$ , such that $G : = D E$ approximates $G ^ { \\prime }$ pointwise. 15 ",
        "bbox": [
            114,
            291,
            883,
            334
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Assume $\\| G ^ { \\prime } \\| _ { 1 , \\infty } \\leq 1$ . An application of the Johnson-Lindenstrauss lemma shows that the following bound holds in the infinity norm: $\\| G ^ { \\prime } - G \\| _ { \\operatorname* { m a x } } = O ( { \\sqrt { \\log n / d } } )$ (cf. e.g. (Udell and Townsend, 2019; Budzinskiy, 2025)). Then, for $z \\in R ^ { n } = R ^ { | V | }$ with $\\| z \\| _ { 1 } \\leq 1$ , we have: ",
        "bbox": [
            114,
            339,
            883,
            388
        ],
        "page_idx": 27
    },
    {
        "type": "equation",
        "img_path": "images/7665fadcf1565a686cf5dfcba81e5ce22af7590a21bd07d92550654ede5a64b3.jpg",
        "text": "$$\n\\| G ^ { \\prime } z - G z \\| _ { + \\infty } = O ( \\sqrt { \\log n / d } )\n$$",
        "text_format": "latex",
        "bbox": [
            382,
            395,
            616,
            415
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "However, no similar bound holds for $\\| G ^ { \\prime } z - G z \\| _ { 2 }$ . Even for ‘simple’ scenarios like the identity transformation $G ^ { \\prime } = I _ { n }$ , the best low-rank approximation admits O(1) additive error in the L2-norm for almost all inputs, and even√ greater distortion (approaching $\\sqrt { n } )$ may appear in the L1-norm. ",
        "bbox": [
            114,
            420,
            883,
            463
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "This makes the low-rank operation useful for determining affinity of pairs of coordinates in dimension $n$ , but more problematic as a vector transformation function. However, the ReLU-lowrank mechanism (Eq. (10) is able to suppress a part of the noise of the linear low-rank map, allowing to approximate a sufficiently broad class of non-linear operations. ",
        "bbox": [
            114,
            468,
            882,
            525
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Expressiveness of ReLU-lowrank for Markov chain propagation. We will consider positive inputs $z \\in R ^ { + n }$ focusing on sparse vectors. ",
        "bbox": [
            117,
            537,
            877,
            568
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "One important case concerns approximating a Markov chain transformation $z \\mapsto G ^ { \\prime } z$ , for some $G ^ { \\prime } \\in R { + } ^ { n \\times n }$ . For such a transformation in the positive orthant, adding the ReLU operation to the linear map does not change anything directly, $G ^ { \\prime } z = ( G ^ { \\prime } z ) ^ { + }$ . However, when considering a low-rank matrix $G$ , the non-linear transformation $( G z ) ^ { + }$ can provide a closer approximation of $G ^ { \\prime } z$ for some classes of input vectors $z$ , than the low-rank linear operation $G z$ . ",
        "bbox": [
            114,
            573,
            883,
            632
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "We start with the following illustrative example. ",
        "bbox": [
            116,
            637,
            429,
            652
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Claim 5 (propagating a Markov chain). Let $G ^ { \\prime }$ be the random walk matrix of a directed graph with out-degree $r$ (i.e., $a$ stochastic matrix with r non-zero entries of $1 / r$ in each row), and let $v \\in V$ be a node (basis vector), $\\| v \\| _ { 1 } = \\| v \\| _ { 2 } = 1$ . Then, for any $\\varepsilon > 0$ , there exists $d = { \\cal O } ( r ^ { 3 } \\log n / \\varepsilon )$ such that for some matrices $D \\in R ^ { n \\times d } , E \\in { \\mathit { R } } ^ { d \\times n }$ , we have $\\| G ^ { \\prime } v - f _ { D E } ( v ) \\| _ { 1 } = O ( \\varepsilon )$ . ",
        "bbox": [
            114,
            655,
            883,
            714
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Proof (sketch). Let $D ^ { * } \\in R ^ { n \\times ( d - 1 ) }$ , $E ^ { * } \\in R ^ { ( d - 1 ) \\times n }$ denote matrices $D$ , $E$ restricted to all but the last coordinate in dimension $d$ . Pick $D , E$ so that $\\| G ^ { \\prime } v - D ^ { * } E ^ { * } v \\| _ { \\infty } < \\varepsilon ^ { * }$ , where $\\varepsilon ^ { * } = \\varepsilon / r$ , following Eq. (11) (we have $\\| G ^ { \\prime } \\| _ { 1 , \\infty } \\leq 1$ by stochasticity of $G ^ { \\prime }$ ). Further, set a fixed bias, placing 1 on all entries of the last coordinate in dimension $d$ of $D$ , and $- \\varepsilon ^ { * }$ on all entries of the corresponding last coordinate in dimension $d$ of $E$ . Taking into account this bias, we now have $\\| ( G ^ { \\prime } v - \\varepsilon ^ { * } \\mathbf { 1 } ) - D E v \\| _ { \\infty } < \\bar { \\varepsilon } ^ { * }$ . ",
        "bbox": [
            114,
            728,
            883,
            801
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "For all coordinates $v _ { j } \\in V$ such that ${ v _ { j } } ^ { T } G ^ { \\prime } v = 0$ , we now have ${ v _ { j } } ^ { T } D E v < 0$ , hence also ${ v _ { j } } ^ { T } f _ { D E } ( v ) = 0 .$ . For all other coordinates $v _ { j }$ , we have ${ v _ { j } } ^ { T } G ^ { \\prime } v = 1 / r$ , and $1 / r - 2 \\varepsilon ^ { * } < { v _ { j } } ^ { \\bar { T } } f _ { D E } ( v ) \\le 1 / r$ . Thus, $\\| G ^ { \\prime } v - f _ { D E } ( v ) \\| _ { 1 } \\leq 2 \\varepsilon ^ { * } r$ , and the claim follows.16 □ ",
        "bbox": [
            114,
            804,
            885,
            852
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The above observation shows how ReLU-lowrank deals with one specific class of graph affinity functions (random walks of adjacency of sparse graphs), for transformations of vectors which are nodes in our distinguished basis. We use this example as it is the simplest case which exhibits the benefit of threshold non-linearity: for basis vectors, the operation $f _ { D E }$ captures a basic propagation effect which is well known (in general) to require a full-rank matrix $G ^ { \\prime } \\in \\mathsf { \\bar { \\boldsymbol { R } } } ^ { n \\times n }$ if relying only on linear operations. ",
        "bbox": [
            114,
            90,
            882,
            161
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Propagation and reinforcement of signal. The same thresholding approach, as discussed for Markov chains, turns out to be applicable to a wider class of signal propagation dynamics. It consists in first obtaining a positive-valued signal with heavy random noise, then applying a negative bias, and finally using the ReLU gate to act as a noise threshold. ",
        "bbox": [
            114,
            174,
            883,
            232
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Any linear function $G ^ { \\prime }$ can be represented with a hidden layer of $s \\leq n ^ { 2 }$ nodes, through two matrices $D ^ { \\prime } \\in R ^ { + n \\times s }$ and $E ^ { \\prime } \\in R ^ { + n \\times s }$ , such that: ",
        "bbox": [
            114,
            236,
            880,
            265
        ],
        "page_idx": 28
    },
    {
        "type": "equation",
        "img_path": "images/2f48e6bea71a70428427550dbdbb40157a033ea6c9c4037a91f77310ca745ced.jpg",
        "text": "$$\nG ^ { \\prime } = D ^ { \\prime } E ^ { \\prime } .\n$$",
        "text_format": "latex",
        "bbox": [
            457,
            263,
            540,
            279
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "The above holds in general, and we will refer to such a representation of $G ^ { \\prime }$ as having a sparse hidden (synaptic) layer. We will consider now the question of expressing non-negative functions, $G ^ { \\prime } \\in ( \\bar { R } ^ { + } ) ^ { \\bar { n } \\times n }$ . An example of a valid representation of $G ^ { \\prime }$ is given through a node-edge incidence representation, $D _ { i , ( i - 1 ) n + j } ^ { \\prime } = E _ { ( i - 1 ) n + j , j } ^ { \\prime } = \\sqrt { G _ { i j } ^ { \\prime } }$ , but usually this representation is not optimal in terms of the number of non-zero entries of $D ^ { \\prime }$ and $E ^ { \\prime }$ . ",
        "bbox": [
            114,
            281,
            882,
            349
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "In general, any low-rank approximation of $G$ can be equivalently expressed as $G \\approx D ^ { \\prime } P _ { D } P _ { E } ^ { T } E ^ { \\prime }$ , for some two matrices, $P _ { D } , P _ { E } \\in R ^ { s \\times d }$ . We will consider the most common class of low-rank approximations obtained by taking√ $P _ { D } = P _ { E } = P \\sim \\mathcal { N } ( 0 , 1 ) ^ { s \\times d } / \\sqrt { d }$ . Consider a vector $z$ passing through the ReLU-lowrank operation, and the following vectors $u \\in R ^ { s }$ , $w \\in R ^ { n }$ : ",
        "bbox": [
            114,
            354,
            883,
            415
        ],
        "page_idx": 28
    },
    {
        "type": "equation",
        "img_path": "images/43b56b2ae1270899a12b71449b4ddb65b68ba194635d3da1b861451320b55469.jpg",
        "text": "$$\n\\begin{array} { l } { { u : = E ^ { \\prime } z } } \\\\ { { w : = D ^ { \\prime } P P ^ { T } u } } \\end{array}\n$$",
        "text_format": "latex",
        "bbox": [
            444,
            417,
            553,
            457
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "If $v _ { i } ^ { T } z$ has the interpretation of a signal being sent by node $v _ { i }$ , then $u$ is the encoded message being passed through the hidden layer of the network, and $v _ { j } ^ { T } w$ is the message received by node $v _ { j }$ . ",
        "bbox": [
            117,
            463,
            883,
            494
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "5.4 Modularity in BDH-GPU signal propagation ",
        "text_level": 1,
        "bbox": [
            114,
            508,
            465,
            525
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "We are now ready to capture the essence of the signal propagation and reinforcement capability of the ReLU-lowrank system. To describe the conditions under which a neuron is able to decide whether it should, or should not activate. By a standard analysis of independent Gaussians, we have the following probabilistic statement, under random choice of $P$ . ",
        "bbox": [
            114,
            535,
            883,
            590
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Claim 6 (selective neuron activation). Suppose that the signal of $u$ is uniformly concentrated on a set of nodes $A$ of the hidden layer, i.e., for some subset $A$ of indexes of the hidden layer, we have $u _ { \\bar { \\alpha } } = 0 f o r \\bar { \\alpha } \\notin A$ , and $u _ { \\bar { \\alpha } } \\overset { \\cdot } { \\in } \\big [ \\frac { 1 - \\kappa } { \\sqrt { | A | } } , \\frac { \\mathrm { i } ^ { \\cdot } + \\kappa } { \\sqrt { | A | } } \\big ]$ for ${ \\bar { \\alpha } } \\in A$ , so that $\\| u \\| _ { 2 } \\in [ 1 - \\kappa , 1 + \\kappa ]$ for some small constant $\\kappa \\geq 0$ . Suppose each node $v _ { j } \\in V$ is connected in $D ^ { \\prime }$ to some set of nodes $B _ { j }$ in the hidden layer, $B _ { j } = \\{ \\bar { \\beta } : D _ { j , \\bar { \\beta } } ^ { \\prime } \\neq 0 \\}$ , and let these connections weight be drawn uniformly $\\begin{array} { r } { D _ { j , \\bar { \\beta } } ^ { \\prime } \\in [ \\frac { 1 - \\kappa } { \\sqrt { | B _ { j } | } } , \\frac { 1 + \\kappa } { \\sqrt { | B _ { j } | } } ] f o r } \\end{array}$ $\\bar { \\beta } \\in B _ { j }$ . Let $C _ { j } = A \\cap B _ { j }$ . Define the ratio: ",
        "bbox": [
            112,
            593,
            883,
            686
        ],
        "page_idx": 28
    },
    {
        "type": "equation",
        "img_path": "images/cbee971593f9b2cff09a1a45960eb8ace6a0ba677ab54fa4f5080a27baaecbc0.jpg",
        "text": "$$\n\\varrho : = \\sqrt { \\frac { | C _ { j } | } { | A | } \\cdot \\frac { | C _ { j } | } { | B _ { j } | } } .\n$$",
        "text_format": "latex",
        "bbox": [
            429,
            691,
            565,
            733
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Then, there exists an absolute constant $c > 0$ , such that for any value of $w _ { j }$ (where we recall that $w : = D ^ { \\prime } P P ^ { T } u .$ ), we have: ",
        "bbox": [
            114,
            738,
            882,
            768
        ],
        "page_idx": 28
    },
    {
        "type": "equation",
        "img_path": "images/02172ebcefd7ba2a975fdbc1c0653b3117c04b57b9b54ff5d15b4b0ef2ab1805.jpg",
        "text": "$$\n, - \\kappa ) ^ { 2 } \\varrho - c \\sqrt { \\log n / { \\mathit { d } } } \\bigg | = 1 - O ( 1 / n ) \\quad a n d \\quad \\operatorname* { P r } \\Big [ w _ { j } \\le ( 1 + \\kappa ) ^ { 2 } \\varrho + c \\sqrt { \\log n / { \\mathit { d } } } \\Big ] = 1 - O ( 1 / n )\n$$",
        "text_format": "latex",
        "bbox": [
            202,
            768,
            861,
            797
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Thus, the value of $w _ { j }$ can be used by a neuron to obtain an estimation of $\\varrho ,$ , and apply a threshold to activate accordingly. □ ",
        "bbox": [
            117,
            806,
            882,
            837
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Proof. Observe that $w _ { j } \\ = \\ \\big ( P ^ { T } D _ { j , \\cdot } ^ { \\prime } \\big ) ^ { T } \\big ( P ^ { T } u \\big )$ . As $\\| D _ { j , \\cdot } ^ { \\prime } \\| _ { 2 } \\in [ 1 - \\kappa , 1 + \\kappa ]$ , a standard application of JohnsonLindenstrauss to vector inner products gives $\\operatorname* { P r } \\left[ \\lvert w _ { j } - D _ { j , \\cdot } ^ { \\prime } { } ^ { T } u \\rvert \\leq c \\sqrt { \\log n / d } \\right] = 1 - O ( 1 / n )$ for $c$ large enough. Since ${ D _ { j , \\cdot } ^ { \\prime } } ^ { T } u \\in [ ( 1 - \\kappa ) ^ { 2 } \\rho , ( 1 + \\kappa ) ^ { 2 } \\rho ]$ , the claim follows. □ ",
        "bbox": [
            114,
            849,
            882,
            914
        ],
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/b8e438b54aa6ab28897bb79e90cf6e1135d04d9d5b471ea445985778b5918a21.jpg",
        "image_caption": [
            "Figure 8: The ReLU-lowrank feedforward network of BDH-GPU allows neurons to activate when triggered by activation signals in its own community. (a) Illustration of the selective neuron activation pattern in the proof of Claim 6, showing the activation decision of node $v _ { j }$ (left) based on active set $A$ in the sparse hidden layer. (b) Illustration of Eq. (12) showing the relationship between sizes of sets in the sparse hidden layer: active set $A$ , set $B _ { j }$ connected to neuron $v _ { j }$ , and the intersection $C _ { j } = A \\cap B _ { j }$ : neuron $v _ { j _ { 1 } }$ becomes active, but neuron $v _ { j _ { 2 } }$ does not. "
        ],
        "image_footnote": [],
        "bbox": [
            127,
            102,
            879,
            363
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "The ReLU-lowrank operation $f _ { D E }$ , after adding appropriate negative bias, can thus be used to propagate positive affinity functions $G ^ { \\prime }$ on input vectors, performing the following form of thresholding: neurons $j$ in the output layer individually compute a form of local “F-score” $\\varrho$ given by Eq. (12) of the activation of the positive sparse hidden layer, and decide based on it whether they are good match for the output activation; if the threshold condition on $\\varrho$ is not met, the neuron $j$ does not activate in the output vector (see Fig. 8 for an illustration). ",
        "bbox": [
            116,
            465,
            882,
            535
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Equation (12) naturally coincides with a pattern of communication within network graphs $G ^ { \\prime }$ admitting positive Newman modularity (Newman, 2006), allowing nodes $v _ { j }$ to correctly receive messages $u$ which in the hidden layer primarily reached a denser cluster of $G ^ { \\prime }$ containing $v _ { j }$ . For a specific illustration, let $H$ be an undirected $k$ -block stochastic block model (SBM) network (Karrer and Newman, 2011) with $k \\in \\mathbb N$ blocks of $n / k$ nodes each, in-block edge density $p$ and out-of-block edge density $q < p$ . We put $G ^ { \\prime } : = D ^ { \\prime } E ^ { \\prime } = H ^ { 2 }$ , i.e., the first connection layer of $G ^ { \\prime }$ is $E ^ { \\prime } = H$ and the second connection layer is also $D ^ { \\prime } = H$ . Suppose that $H$ is a random SBM graph with positive Newman modularity separated from 0, i.e., let $\\begin{array} { r } { \\mu = \\frac { k - 1 } { k } \\frac { p - q } { p + ( k - 1 ) q } > 0 } \\end{array}$ . Following Claim (6) with $\\kappa = 0$ , we can find a ReLU-lowrank representation $G$ to achieve a communication scheme on $G ^ { \\prime }$ , such that a message sent from one node $z = v _ { i }$ activates a node $v _ { j }$ when $i$ and $j$ are in the same block with probability $1 - O ( 1 / n )$ , and with probability $O ( 1 / n )$ otherwise, when $\\textstyle \\mu > { \\frac { 1 } { p } } { \\sqrt { \\log n / d } }$ . ",
        "bbox": [
            114,
            541,
            882,
            689
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "We thus make the following intuitive observation. ",
        "bbox": [
            116,
            695,
            442,
            710
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Observation 6 (in-cluster signal reinforcement). The ReLU-lowrank representation of $B D H { \\cdot } G P U ( n , d )$ is sufficient to represent in-cluster information spreading dynamics in models of graphs with constant in-cluster density and arbitrarily small positive modularity (such as the $k$ -cluster Stochastic Block Model) when $d / \\log n = \\omega ( 1 )$ is an arbitrarily slowly growing function. ",
        "bbox": [
            116,
            715,
            882,
            771
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "While Claim 6 and Observation 6 are made with reference to an almost-uniform distribution of signal $u$ on the set of nodes of the middle layer, $u$ can have (and in practice does have) a distribution of density which is non-uniform, e.g., going across $a = O ( \\log n )$ different clustering scales, with a $( 1 / a )$ -fraction of the signal represented at each scale. This allows neurons in the output layer to combine a smaller number of strong signals in its local cluster, with a larger number of weaker ones spread more globally. Such an approach coincides with the observed structure of the graph $D ^ { \\prime } E ^ { \\prime }$ , discussed in Subsection 5.5. ",
        "bbox": [
            114,
            782,
            883,
            867
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Supermodularity on input perturbation. We clarify how the properties of function $f _ { D E } : ( R ^ { + } ) ^ { n } \\to ( R ^ { + } ) ^ { n }$ relate to the previously discussed ability to make an input signal resonate “within a module” in a graph with hidden modular structure. First, note that $f _ { D E }$ is a subadditive function, but is not submodular in general with respect to the set of $n$ coordinates of its input vector. In some of the regimes in which it appears to be operating, locally $f _ { D E }$ exhibits a form of behavior opposite to submodularity, referred to as ‘supermodularity’, or ‘increasing returns’ of adding new coordinates to the input vector. This is already implicitly captured by Claim 6, but we can consider a simpler example. ",
        "bbox": [
            116,
            882,
            880,
            911
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            90,
            883,
            147
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Take a variant of the setting from Observation 5 with the same choice of $G ^ { \\prime }$ , and let $z \\in ( R ^ { + } ) ^ { n }$ and biases of $D$ be chosen so that all coordinates of $D E z$ are approximately equal to $- 1 . 5 / r \\pm o ( 1 )$ (this can be done by choosing e.g. $z _ { j } = 1 / n )$ ). Then, $f _ { D E } ( z ) = 0$ , and for any $v _ { i } , v _ { j } \\in V$ , $f _ { D E } ( z + v _ { i } ) = 0$ a.s., $f _ { D E } ( z + v _ { j } ) = 0$ a.s., but $f _ { D E } ( z + v _ { i } + v _ { j } )$ has non-zero coordinates a.s. with values approximately $1 / 2 r$ , for all nodes $v _ { k }$ which are common out-neighbor nodes of $v _ { i }$ and $v _ { j }$ , i.e., for all $k$ such that $G ^ { \\prime } ( v _ { i } , \\bar { v } _ { k } ) = G ^ { \\prime } ( \\bar { v } _ { j } , \\bar { v } _ { k } ) = 1 / r$ . This mechanism generalizes to finding common neighborhoods which have many connections to two given subsets of nodes, $V _ { a }$ and $V _ { b }$ . In a setting where the considered affinity $G ^ { \\prime }$ is bi-directional (e.g., a symmetric matrix), this corresponds to finding shortcut nodes, allowing to go from $V _ { a }$ to $V _ { b }$ . ",
        "bbox": [
            114,
            152,
            882,
            265
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "It follows that the neighborhood-reinforcing nature of the threshold dynamics of BDH-GPU, which plausibly follows from the logic of its role in inference and from the needs for an efficient computational process, is starkly different from the more often studied submodular behavior of threshold and cascade dynamics on real-world networks (Kempe et al., 2003), and plausibly, much less smooth when considered as a dynamical process. ",
        "bbox": [
            114,
            270,
            882,
            327
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ",
        "text_level": 1,
        "bbox": [
            114,
            342,
            696,
            357
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "We consider the $D$ matrices (in the same way $D _ { y }$ and $D _ { x }$ ) and $E$ matrix obtained after training of BDH-GPU models, and used in the ReLU-lowrank operation Eq. (10), $f _ { D E } ( z ) = ( D E z ) ^ { + }$ . ",
        "bbox": [
            116,
            367,
            875,
            400
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Choice of prior of matrix parameter distributions. Following the discussion in Section 5.4, we expect matrix $G : = D E$ to reflect the clustering (modularity) structure of the neuron-neuron communication graph. Any plausible parameter distribution of matrix $G$ must therefore allow heavy-tailed distribution of entries. At the same time, a Gaussian noise term is inherent to low-rank matrix representation, and needs to be taken into account together with this heavy-tailed distribution. ",
        "bbox": [
            114,
            412,
            882,
            483
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "We now provide a somewhat more fine-grained explanation, which leads to the prior on the structure of matrix $G$ as given by Eq. (13). Consider a training set-up in which the ReLU-lowrank operation described by matrix $G$ is treated as an approximation of the same operation, governed by a high-rank matrix $G ^ { \\prime }$ , with $f ^ { \\prime } ( z ) : = ( G ^ { \\prime } z ) ^ { + }$ . Considering this block in isolation from the rest of the training system, the training of matrices $D$ , $E$ goal corresponds to learning an approximation of $f ^ { \\prime }$ , with $D \\in R ^ { n , d }$ , $E \\in R ^ { n , d }$ , such that $f ( z ) \\approx { \\bar { f } } ^ { \\prime } ( z )$ holds for some class of vectors $z$ . ",
        "bbox": [
            114,
            488,
            882,
            561
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "For the rest of this analysis, we will consider the function $f ^ { \\prime }$ as a ground truth reference for the intended operation of the ReLU-lowrank block. This type of analysis can be seen as plausible over short time spans in later phases of training of a BDH-GPU model, i.e., once individual neurons in $R ^ { n }$ have started to admit semantic or functional meaning, and so when function $D ^ { \\prime } E ^ { \\prime }$ describes a property of the problem being solved in a (frozen) concept space, and not a co-learning process between the representation of the concept space in $R ^ { n }$ and the functions applied to it. ",
        "bbox": [
            114,
            566,
            882,
            637
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "We can represent $G ^ { \\prime } : = D ^ { \\prime } E ^ { \\prime }$ , where $D ^ { \\prime } \\in R ^ { n , s }$ , $E ^ { \\prime } \\in R ^ { s , n }$ , with $s = O ( n ^ { 2 } )$ , are in general matrices of rank $n$ ; we have $f ^ { \\prime } ( z ) : = ( D ^ { \\prime } E ^ { \\prime } z ) ^ { + }$ . Without loss of generality, we can choose from among the possible representations one with the following distribution of positive and negative elements: $D ^ { \\prime } \\in ( R ^ { + } ) ^ { n , s }$ , $E ^ { \\prime } = E ^ { \\prime } { } ^ { \\mathfrak { e } } - E ^ { \\prime } { } ^ { \\mathfrak { i } }$ , with $E ^ { \\prime ^ { \\mathfrak { c } } } , E ^ { \\prime ^ { \\mathfrak { i } } } \\in ( R ^ { + } ) ^ { s , n }$ . We will write: $G ^ { \\prime } = { G ^ { \\prime } } ^ { \\mathfrak { e } } - { G ^ { \\prime } } ^ { \\mathfrak { i } }$ , where $G ^ { \\prime } { } ^ { \\mathfrak { e } } = D ^ { \\prime } E ^ { \\prime } { } ^ { \\mathfrak { e } }$ , and ${ G ^ { \\prime } } ^ { \\mathrm i } = D ^ { \\prime } E ^ { \\prime } { } ^ { \\mathrm i }$ . The main purpose of the chosen representation $G ^ { \\prime } = D ^ { \\prime } E ^ { \\prime } = D ^ { \\prime } ( E ^ { \\prime } { } ^ { \\ell } - E ^ { \\prime } { } ^ { \\mathrm { i } } )$ is to have matrices $D ^ { \\prime }$ , $E ^ { \\prime } { } ^ { \\mathrm { i } }$ , $E ^ { \\prime } { } ^ { \\mathfrak { e } }$ with much smaller outlying elements compared to matrix $G ^ { \\prime }$ , which leads to more justified conclusions about the uniform nature of the noise introduced by the low-rank decomposition.17 ",
        "bbox": [
            114,
            641,
            882,
            752
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Assume now that we learn to approximate function $f ^ { \\prime }$ with $f _ { D E }$ by trainable matrices $D , E$ through the following low-rank scheme: ",
        "bbox": [
            117,
            756,
            877,
            784
        ],
        "page_idx": 30
    },
    {
        "type": "equation",
        "img_path": "images/fcc7a503b077953a55aa30fc30a728102720017bcf8de9e7c871181f4bd10951.jpg",
        "text": "$$\nG = D E : = ( B _ { D } + D ^ { \\prime } P ) ( P ^ { T } E ^ { \\prime } + B _ { E } ^ { T } ) ,\n$$",
        "text_format": "latex",
        "bbox": [
            357,
            781,
            637,
            801
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "where $P \\in R ^ { s , d }$ is non-parametric and the result of random sampling an almost-orthonormal random projection so that $P P ^ { T } \\approx I _ { s }$ (e.g. $P \\sim \\mathcal { N } ( 0 , 1 / \\sqrt { d } ) ^ { s , d } )$ , and $B _ { D } , B _ { E } \\in R ^ { n , d }$ represent trainable additional terms for compensating ",
        "bbox": [
            117,
            805,
            879,
            835
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "error or introducing bias, with the goal of minimizing some loss function $\\mathcal { L } ( f ^ { \\prime } , f _ { D E } )$ . The terms $B _ { D } , B _ { E }$ compensate the error introduced by the approximation $P P ^ { T } \\approx \\tilde { I _ { s } }$ , after the ReLU operation. ",
        "bbox": [
            117,
            90,
            883,
            119
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Let $Q : = P P ^ { T } = I _ { s } + \\delta _ { I } + \\delta _ { Q }$ , where $\\delta _ { I } \\in R ^ { s \\times s }$ is a diagonal error matrix, and $\\delta _ { Q } \\in R ^ { s \\times s }$ is a non-diagonal (hollow) matrix. We have: ",
        "bbox": [
            111,
            125,
            882,
            154
        ],
        "page_idx": 31
    },
    {
        "type": "equation",
        "img_path": "images/fb43cccf8b6ab8df4d6e2cc238e8815047a2ce2c615ce61b97484900833a0ca8.jpg",
        "text": "$$\nG = D E = ( { G ^ { \\prime } } ^ { \\bullet } - { G ^ { \\prime } } ^ { \\bullet } ) + D ^ { \\prime } \\delta _ { I } ( { E ^ { \\prime } } ^ { \\bullet } - { E ^ { \\prime } } ^ { \\bullet } ) + \\underbrace { D ^ { \\prime } \\delta _ { Q } E ^ { \\prime } } _ { \\varepsilon _ { Q } } + \\underbrace { ( B _ { D } E + D B _ { E } ^ { T } ) } _ { \\varepsilon _ { B } } .\n$$",
        "text_format": "latex",
        "bbox": [
            245,
            157,
            753,
            199
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Since all elements of $D ^ { \\prime } , E ^ { \\prime } { } ^ { \\mathfrak { e } } , E ^ { \\prime } { } ^ { \\mathrm { i } }$ are non-negative and $I _ { \\delta }$ is diagonal, we can represent elements $G _ { i j }$ , for $i , j \\in$ $1 , \\ldots , n$ , as follows: ",
        "bbox": [
            120,
            205,
            882,
            234
        ],
        "page_idx": 31
    },
    {
        "type": "equation",
        "img_path": "images/45086618ee19f592dbebac0b600bbe36413f5e02012fc35533ab70cf5d02c4ab.jpg",
        "text": "$$\nG _ { i j } = ( 1 + \\varepsilon _ { \\delta { i j } } ^ { \\mathfrak { e } } ) G _ { { i j } } ^ { \\prime \\mathfrak { e } } - ( 1 + \\varepsilon _ { \\delta { i j } } ^ { \\mathfrak { i } } ) G _ { { i j } } ^ { \\prime \\mathfrak { i } } + \\varepsilon _ { Q i j } + \\varepsilon _ { B i j }\n$$",
        "text_format": "latex",
        "bbox": [
            315,
            233,
            683,
            255
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "where $\\left| \\varepsilon _ { \\delta i j } ^ { \\varepsilon } \\right| = O ( \\sqrt { \\log n / \\ d } )$ and $\\lvert \\varepsilon _ { \\delta i j } ^ { \\mathrm { i } } \\rvert = O ( \\sqrt { \\log n / \\ d } )$ have the interpretation of small multiplicative distortion. ",
        "bbox": [
            122,
            258,
            875,
            275
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Following (13), we expect the elements of $G$ to be distributed as the sum of four different distributions. The term G′e has the interpretation of positive ground truth elements of $G ^ { \\prime }$ . The term $- { G ^ { \\prime } } _ { i j } ^ { \\mathrm { i } }$ has the interpretation of negative ground truth elements of $G ^ { \\prime }$ ; its use in combination with the ReLU mechanism can be interpreted as inhibitory action. Both of these terms are subject to slight multiplicative distortion. ",
        "bbox": [
            116,
            280,
            882,
            342
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The term $\\varepsilon _ { Q i j }$ has the interpretation of non-trainable noise (which depends only on $D ^ { \\prime }$ , $E ^ { \\prime }$ and the random choice of $P$ ). Under reasonable assumptions on outlying elements of $D ^ { \\prime } , E ^ { \\prime }$ , it is a form of almost-Gaussian symmetric noise inherent to the considered class of low-rank projections, $\\varepsilon _ { Q i j } \\to N ( 0 , \\sigma _ { Q } )$ , for some $\\sigma _ { Q } \\in R ^ { + }$ , and the expected value of this noise is typically very close to 0, even when considering the expectation of $\\varepsilon _ { Q i j }$ conditioned on known values of $\\varepsilon _ { Q i ^ { \\prime } j ^ { \\prime } }$ for a small number of indexes $( i ^ { \\prime } , j ^ { \\prime } )$ in the matrix. ",
        "bbox": [
            114,
            347,
            882,
            417
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Finally, $\\varepsilon _ { B i j }$ is a trainable term, whose norm tends to 0 as $d$ increases. We expect it to have the interpretation of bias used to offset the low-rank Gaussian noise and perform denoising in the ReLU-gate, as previously discussed in Section 5.3. Because of the action of the ReLU gate, we plausibly expect the distribution of $\\varepsilon _ { B i j }$ to be skewed towards negative numbers, with $0 > \\mathbb { E } \\varepsilon _ { B i j } \\gg \\sigma _ { Q }$ . ",
        "bbox": [
            114,
            422,
            882,
            481
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "From the above discussion of the four terms of the sum in Eq. (13), we see that only one of these terms, $G _ { \\ i j } ^ { \\prime }$ , is $\\sigma _ { Q }$ with non-negligible probability. We reach the conclusion that a part of the relevant signal of $G ^ { \\prime }$ is concentrated in the right tail of large positive matrix entries of $G$ . ",
        "bbox": [
            114,
            486,
            883,
            530
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Hypothesis 1 (right tail contains signal). Consider the interpretation that the ReLU-lowrank transformation $z \\mapsto$ $( G z ) ^ { + }$ , with $G = D E$ , has learned to act as an approximation of some other operation $z \\mapsto ( G ^ { \\prime } z ) ^ { + }$ , where $G ^ { \\prime }$ has no low-rank constraint imposed on it. Then the right tail of the distribution of matrix elements of $G$ corresponds to the right tail of the distribution of elements of $G ^ { \\prime }$ , starting from some positive threshold value $\\sigma _ { Q }$ , associated with the noise of the low-rank decomposition. Formally, for almost all pairs of indices $i , j \\in { 1 , \\dots , n }$ such that $G _ { i j } \\gg \\sigma _ { Q }$ , we also have G′eij ≫ σQ. ",
        "bbox": [
            114,
            531,
            883,
            621
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The converse implicon the form of biases that whi $G _ { i j } ^ { \\prime { \\mathfrak { e } } } \\gg \\sigma _ { Q }$ implies w from $G _ { i j } \\gg \\sigma _ { Q }$ , also plausibly holds under some stronger assumptions training error for the specific inference task considered. $\\varepsilon _ { B i j }$ ",
        "bbox": [
            116,
            632,
            879,
            661
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "This direct method of decoding $G$ from $G ^ { \\prime }$ does not extend from the right tail towards the center of the distribution. For the choices of $n , d$ we make, we expect the term dominating most elements of matrix $G$ to be $\\varepsilon _ { Q i j }$ . For example, when $G ^ { \\prime }$ is a stochastic matrix, we expect to have $\\sigma _ { Q } = O ( 1 / \\sqrt { d } )$ (cf. Eq. (11) for the corresponding infinity-norm bound, $| \\varepsilon _ { Q i j } | = O ( \\sqrt { \\log n / \\ d } ) )$ . With $\\begin{array} { r } { \\sum _ { i , j } | G _ { \\ i , j } ^ { \\prime \\ell } | = n } \\end{array}$ for a stochastic matrix, we expect the right heavy tail of the element distribution of $G$ to have $\\Omega ( n { \\sqrt { d } } )$ elements (out of the $n ^ { 2 }$ matrix elements of $G$ ) which are clearly separated from the Gaussian noise. ",
        "bbox": [
            114,
            666,
            883,
            762
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "We confirm empirically that the right tail of $G$ , defined as above with respect to threshold $\\sigma _ { Q }$ , turns out to contain a non-negligible portion of the parameter capacity of matrices $D , E$ , even for very small models (10M to 100M parameters). ",
        "bbox": [
            116,
            766,
            883,
            810
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Experimental setup. We prepared parameter matrices of a 24M-parameter BDH-GPU model configured with $h = 4$ heads and $L = 8$ layers, $n { \\stackrel { . } { = } } { \\dot { h } } \\cdot 2 ^ { 1 { \\dot { 3 } } } = 2 ^ { 1 5 }$ neurons, and hidden low-rank dimension $d = 2 5 6$ . We considered the weighted neuron-neuron interaction graph, having the encoder-decoder matrix pair $G = D _ { x } E$ as its node adjacency matrix on the set of neurons $V = 1 , \\dots , n$ . For uniformity, we subsampled $G$ by picking node subsets $V ^ { ( a ) } , a \\in$ $\\{ 1 , 2 , 3 , 4 \\}$ , associated with each head, and considered the weighted subgraphs $G ^ { ( a b ) } ~ = ~ \\{ V , \\{ ( u , v , G _ { u v } ) ~ \\colon ~ u ~ \\in ~$ $ V ^ { ( a ) } , v \\in V ^ { ( b ) } \\} \\}$ , with $G ^ { ( a b ) } \\in R ^ { n ^ { * } \\times n ^ { * } }$ where $n ^ { * } = n / h = 2 ^ { 1 3 }$ , each having $( n ^ { * } ) ^ { 2 } = 2 ^ { 2 6 }$ weighted edges. ",
        "bbox": [
            114,
            821,
            883,
            912
        ],
        "page_idx": 31
    },
    {
        "type": "image",
        "img_path": "images/5176931e1d2793975c8f111ad083fa462293a6596fe471259205812e40d3c36a.jpg",
        "image_caption": [
            "Figure 9: (a) Heavy-tailed element distribution and modularity analysis of the excitatory neuron-neuron connection graph contained the encoder-decoder matrix $G ^ { * }$ . Distribution of elements of the encoder-decoder matrix $G ^ { * } ~ \\in ~ \\boldsymbol { R } ^ { n ^ { * } \\times n ^ { * } }$ of a BDHGPU model with $n ^ { * } ~ = ~ 8 1 9 2$ neurons and $d \\ : = \\ : 2 5 6$ : histogram freqG $\\ast \\left( x \\right)$ , its symmetric part freq − symmetricG $* \\left( x \\right) : =$ $\\operatorname* { m i n } \\{ { \\mathrm { f r e q } } _ { \\mathrm { G } ^ { * } } ( x ) , { \\mathrm { f r e q } } _ { \\mathrm { G } ^ { * } } ( - x ) \\}$ , and distribution ske $\\mathbf { w _ { \\Psi } } \\operatorname { f r e q } - \\operatorname { s k e w } _ { \\mathbf { G } ^ { * } } ( x ) : = \\operatorname { f r e q } _ { \\mathbf { G } ^ { * } } ( x ) - \\operatorname { f r e q } - \\operatorname { s y m m e t r i c } _ { \\mathbf { G } ^ { * } } ( x )$ . $\\diamond$ (b) Estimate (lower bound) of Newman modularity of matrix $G _ { \\geq \\beta } ^ { * }$ for different values of $\\beta$ , plotted as a function of the number of non-zero elements (edges) of $G _ { \\geq \\beta } ^ { * }$ . Modularity of random graph baselines are provided for reference, for the $G ( n ^ { * } , m )$ model with the same number of edges as $G _ { \\geq \\beta } ^ { * }$ , and for a matrix $( P _ { 1 } P _ { 2 } ^ { T } ) _ { \\geq \\beta ^ { \\prime } }$ with the same number of edges as $G _ { \\geq \\beta } ^ { * }$ , where $P _ { 1 } , P _ { 2 } \\sim \\mathcal { N } ( 0 , 1 ) ^ { n ^ { * } \\times d }$ . The modularity estimates were obtained using the community structures returned by the Louvain algorithm, in the best of 5 clustering runs with different random seeds. "
        ],
        "image_footnote": [],
        "bbox": [
            120,
            93,
            879,
            291
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "We repeated the experiment 5 times using models pretrained with different random seeds. ",
        "bbox": [
            117,
            454,
            700,
            469
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Findings. For all of the 5 models we pretrained for this purpose, exactly 3 out of the 4 encoder heads and all decoder heads adhered to the prior on parameter distribution given by Eq. (13), showing a good correspondence for 12 out of 16 of their parameter sub-matrices $G ^ { ( a b ) }$ . ",
        "bbox": [
            114,
            491,
            882,
            535
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "We continue the discussion in this Section for one specific matrix $G ^ { ( a b ) }$ of one specific pretrained models, which was chosen as representative. The example we choose has $a = b$ ; and so the matrix $\\bar { G } ^ { ( a b ) }$ has an interpretation as $G [ V _ { a } ]$ i.e., the subgraph of $G$ induced by vertex set $V _ { a }$ , which enables us to visualize the graph $G ^ { ( a a ) }$ more easily on its vertex set $V _ { a }$ . ",
        "bbox": [
            114,
            541,
            882,
            603
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "We refer to the representative object of our study, i.e., to the matrix $G ^ { ( a a ) }$ of the selected model, as $G ^ { * }$ . For any matrix $A$ and $\\beta \\geq 0$ , we denote by $A _ { \\geq \\beta }$ the matrix $A$ cut off at threshold $\\beta$ , i.e., $A _ { \\ge \\beta _ { i j } } = A _ { i j }$ if $A _ { i j } \\geq \\beta$ , and $A _ { \\geq \\beta _ { i j } } = 0$ otherwise. ",
        "bbox": [
            116,
            607,
            882,
            651
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "The distribution of elements $G _ { i , j } ^ { * }$ is presented in Fig. 9 (a). We find that the observed distribution corresponds well to the prior expected of it by Eq. (13). We determine the threshold value $\\beta \\geq 0$ at which we expect to capture signal, $G _ { \\geq \\beta } ^ { * } \\approx G _ { \\geq \\beta } ^ { \\prime }$ , following Hypothesis 1. We find (from Fig. 9(a)) that the separation from noise happens for this specific matrix $G ^ { * }$ at $\\beta _ { 1 } \\approx 1 . 2$ , at which point the right heavy tail begins to dominate. However, already for much smaller values of $\\beta$ we find that $G _ { \\geq \\beta } ^ { * }$ has high modularity, and this actually increases as more non-zero values are added to $G _ { \\geq \\beta } ^ { * }$ for smaller $\\beta$ , up to a maximum at $\\beta _ { 2 } \\approx 1 . 0$ (Fig. 9(b)). Even for much smaller values of $\\beta$ , the modularity of $G _ { \\geq \\beta } ^ { * }$ remains almost constant up to well above $2 ^ { 2 0 }$ non-zero matrix entries on the $n ^ { * } = 2 ^ { 1 3 }$ nodes considered. The modularity of the baselines, of random graphs or random low-rank matrix products, quickly drops to 0 in this regime. This should be compared to the total number of parameters of the matrices $D _ { x } , E$ corresponding to $G ^ { * }$ , i.e., $2 \\cdot 2 ^ { 1 3 } \\cdot 2 ^ { 8 } = 2 ^ { 2 2 }$ parameters. A complementary analysis of the inhibitory signal, for a similarly defined matrix $| G _ { \\leq - \\beta } ^ { * } |$ also finds that this structure has high modularity. ",
        "bbox": [
            114,
            657,
            882,
            819
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "In auxiliary experiments, we looked at basic graph parameters of matrix $G _ { \\geq \\beta } ^ { * }$ , treated as a directed graph on its set of nodes. We set $\\beta = 1 . 2$ , obtaining $m = 4 6 8 2 0$ non-zero entries (edges) in $G _ { \\geq \\beta } ^ { * }$ . We found that $G ^ { * }$ has a heavytailed, power-law-like degree distribution, with generally more concentrated out-degree than in-degree (Fig. 10(a)). Generally, this finding is consistent with expectations as to the structure of a network with positive modularity. The difference of in- and out-degree distributions, while plausible and prevalent in real-world information dissemination networks, was not considered in Subsection 5.3. ",
        "bbox": [
            116,
            824,
            883,
            911
        ],
        "page_idx": 32
    },
    {
        "type": "image",
        "img_path": "images/e0f0c8910410c9a33b4fa23ea0d8ec0f683d0951d7ba012ca507c48b22f973ad.jpg",
        "image_caption": [
            "Figure 10: (a) Unweighted in-degree and out-degree distribution for the $n ^ { * } = 8 1 9 2$ neuron nodes and $m = 4 6 8 2 0$ edges of matrix $G _ { \\geq \\beta } ^ { * }$ with $\\beta = 1 . 2$ . The distributions exhibit power law distributions, with different exponents, the out-degree distribution being more concentrated. (b) Visualization of graph $G _ { \\geq \\beta } ^ { * }$ , hinting at its core-periphery structure. "
        ],
        "image_footnote": [],
        "bbox": [
            112,
            92,
            875,
            478
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Finally, a visualization of $G _ { \\geq \\beta } ^ { * }$ (Fig. 10(b)) exhibits a core-periphery structure. This is again consistent with the expected modular structure. ",
        "bbox": [
            114,
            546,
            879,
            575
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Empirical Finding 3. We confirmed that during training, a graph structure with positive modularity appears in BDHGPU model parameter matrices $D _ { x } E$ and $D _ { y } E$ . This modular structure plausibly follows from the network’s inference function, and specifically from the cluster-aware information propagation dynamics supported by the ReLU-lowrank mechanism (Observation 6). ",
        "bbox": [
            114,
            580,
            882,
            636
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "We also observed that for all of the studied models with $h = 4$ heads, 1 encoder sub-matrix out of 4 has no heavy positive tail, and generally appears to capture a form of inhibitory structure $G ^ { \\prime } { } ^ { \\mathrm { i } }$ from Eq. (13). Since we have not provided convincing mechanisms for isolating negative signals in $G ^ { * }$ and these are easily confounded with the bias term $\\varepsilon _ { B }$ , we omit this case from discussion. We remark that the apparent need for passing activations through such a separate “inhibitory circuit” is one of the most evident explanations for why introducing (a small number of) heads to BDH-GPU provides an improvement in model quality. ",
        "bbox": [
            114,
            646,
            882,
            733
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "6 Analysis: linear attention, sparse positive activation, and monosemanticity ",
        "text_level": 1,
        "bbox": [
            116,
            751,
            764,
            768
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "6.1 Macro-expressiveness of attention in BDH-GPU ",
        "text_level": 1,
        "bbox": [
            116,
            781,
            485,
            796
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "The attention mechanism of BDH-GPU can be described at a coarse-grained level as a transformation mechanism for key-query-value vectors, similar to that in the Transformer. This description is complementary to the interpretation of the BDH-GPU attention mechanism at the micro-level of correlations between neuron pairs, which we defer to Section 6.2, which provides more insight into the way activation vectors used by BDH-GPU relate to the concept space of the model. ",
        "bbox": [
            114,
            806,
            882,
            876
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "We compare the attention mechanism of BDH-GPU with the attention mechanism of the Transformer, describing both as reflections of a general attention mechanism. Specifically, we explain why, and up to what context length, the linear attention mechanism of BDH-GPU plausibly fits into macro-expressiveness frameworks of attention designed for the Transformer (based on RASP). ",
        "bbox": [
            111,
            882,
            882,
            911
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            119,
            90,
            883,
            119
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Basic properties of BDH-GPU attention. The key-query space for BDH-GPU is $R ^ { n }$ , the same as its neuron dimension, rather than the small dense dimension used by the Transformer. The keys and queries used by BDH-GPU are given by positive vectors, in $( R ^ { + } ) ^ { n }$ , and are expressed by the same vector $x _ { t , l }$ (noting that at time $t$ , $x _ { t , l }$ is used as a query, and only $x _ { \\tau , l }$ , for $\\tau \\leq t - 1$ , are used as keys). ",
        "bbox": [
            114,
            135,
            882,
            191
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "‘Value’ vectors of BDH-GPU remain in the small dimension, $R ^ { d }$ , which at some model scales is comparable to the dimension used for attention ‘values’ in common configurations of the Transformer. ",
        "bbox": [
            116,
            198,
            880,
            227
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "The relationship between softmax-based attention of the Transformer, regarded as a low-dimensional kernel for general linear attention, and linear attention for vectors in the positive orthant, was considered in a framework called $\\mathrm { F A V O R + }$ (Choromanski et al., 2021). Here, we provide a few complementary (simpler) observations, sufficient to grasp the main effects of the ability of Linear Attention to distinguish facts in context. ",
        "bbox": [
            116,
            233,
            883,
            289
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "State capacity vs. distinction capacity. The matrix $\\pmb { \\rho } \\in R ^ { n \\times d }$ , which is used to represent state for each layer of BDH-GPU, should theoretically have sufficient capacity to store $O ( n )$ ‘value’ vectors in $R ^ { d }$ if considered as a lookup table for values. We now remark that its actual capability of distinguishing facts using the linear attention mechanism is also asymptotically close to $n$ . ",
        "bbox": [
            114,
            303,
            883,
            361
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Attention is a mechanism of associative memory which, given a series of key-value pairs $( ( k _ { 1 } , v _ { 1 } ) \\dots , ( k _ { t } , v _ { t } ) ) \\ \\in$ $( \\Lambda _ { k } \\times R ^ { d } ) ^ { t }$ , a query $q \\in \\Lambda _ { q }$ and an affinity function $\\phi ( \\cdot , \\cdot ) : \\Lambda _ { q } \\times \\Lambda _ { k } \\to [ 0 , 1 ]$ between the space of queries and keys, returns the attention value: $\\begin{array} { r } { \\dot { a } _ { t } = \\sum _ { \\tau = 1 } ^ { t - 1 } \\phi ( q , k _ { \\tau } ) v _ { \\tau } } \\end{array}$ (or a normalization thereof). With BDH-GPU, we consider ‘value’ vectors $v \\in R ^ { d }$ , where $d$ is small. The spaces of keys and queries may be assumed to coincide as $\\Lambda = \\Lambda _ { k } = \\Lambda _ { q }$ , and we consider in general a single key-query sequence, given by $( k _ { t } ) _ { t \\in \\mathbb { N } }$ : 18 ",
        "bbox": [
            114,
            366,
            883,
            445
        ],
        "page_idx": 34
    },
    {
        "type": "equation",
        "img_path": "images/e077d83b868c448a80938d785a94b4dd2f150a6bf4dc6cc183bc4f854489dfcc.jpg",
        "text": "$$\na _ { t } = \\sum _ { \\tau = 1 } ^ { t - 1 } \\phi ( k _ { t } , k _ { \\tau } ) v _ { \\tau }\n$$",
        "text_format": "latex",
        "bbox": [
            428,
            452,
            570,
            494
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "This key-query space $\\Lambda$ may be considered as an abstract space, and represented in any way which is convenient, for as long as the affinity function $\\phi ( k _ { t } , k _ { \\tau } )$ is preserved. For example, when the keys and queries are sampled from a finite (though possibly extremely large) set, there also exists some vector space dimension $\\nu$ (possibly extremely large) and a function mapping $f : \\Lambda \\to S ^ { \\nu }$ , where $S ^ { \\nu } = \\{ z \\in R ^ { \\nu } : \\| z \\| = 1 \\}$ is the unit sphere, such that the scalar (dot, cosine) product in $S ^ { \\nu }$ satisfies $f ( k _ { t } ) \\cdot f ( k _ { \\tau } ) = \\phi ( k _ { t } , \\dot { k } _ { \\tau } )$ . In other words, any affinity function $\\phi$ becomes linear when represented in sufficiently high dimension, subject to suitable preparation of its arguments with function $f$ . With $\\nu$ extremely large, $S ^ { \\nu }$ is a sort of Platonic ideal of a space in which the attention keys and queries live, with no relation to any specific model. ",
        "bbox": [
            114,
            501,
            883,
            612
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "This type of argument, often used in considerations of Support Vector Machines, is linked to two challenges: (1) ensuring that the dimension actually considered by the network (in our case $n$ ) is high enough compared to the hypothetical dimension $( \\nu )$ , and (2) ensuring that a suitable preparation function $f$ exists and is easy to learn for the model.19 We now explain when the dimension $n$ can be considered sufficient, and what types of keys can be prepared by BDH-GPU. ",
        "bbox": [
            114,
            618,
            882,
            688
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Expressiveness of linear attention in dimension $n$ . The Linear Attention mechanism aggregates key-value correlations over time. In general, the associated rate of accumulation of noise is manageable, up to the approximate scale of between $t = \\tilde { \\Omega } ( \\sqrt { n } )$ and $t = { \\tilde { O } } ( n )$ key-value ‘facts’ stored in the attention of a given layer. We make the following statement about the Linear Attention mechanism in general. ",
        "bbox": [
            114,
            703,
            882,
            762
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Claim 7 (informal statement). The mechanism of Linear Attention, applied in dimension $R ^ { n }$ , can approximately express an attention affinity function for up to $t = { \\tilde { O } } ( n )$ ‘key-value’ pairs in context, with ‘values’ having comparable $L 2$ -norm, under moderate assumptions on weak correlation of historical keys and uniformity of the expressed affinity function. Without such assumptions, Linear Attention can compute the correct affinity up to at least $t = { \\tilde { \\Omega } } ( { \\sqrt { n } } )$ ‘key-value’ pairs in context, except for a negligible fraction of possible inputs. Keys and queries need to be suitably prepared beforehand. ",
        "bbox": [
            116,
            765,
            883,
            809
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            90,
            883,
            133
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "The formal statement and proof is provided in Appendix C.2. ",
        "text_level": 1,
        "bbox": [
            114,
            142,
            511,
            157
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "The above claim captures the expressiveness of Linear Attention in dimension $R ^ { n }$ , subject to some way of preparing keys and queries in $R ^ { n }$ by the model in blocks preceding the attention block. A model using Linear Attention has to learn its own way to prepare keys. In fact, different natural approaches to key preparation, for example using random projections or hashing on a set of $t$ vectors, lead to the same asymptotic statement of Claim 7. (In the proof in the Appendix, we chose to use a particularly simple one.) ",
        "bbox": [
            116,
            162,
            882,
            233
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "The specific way of preparing keys used (learned) by BDH-GPU for its Linear Attention is particularly interesting. Except for the effect of RoPE rotation, which introduces a negative positional effect in the affinity of keys and queries, BDH-GPU uses activation vectors (keys, queries) with only positive coordinates to represent its keys. ",
        "bbox": [
            116,
            238,
            883,
            281
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "We discuss some aspects of how the positive activation vectors of BDH-GPU relate to Linear Attention. ",
        "bbox": [
            119,
            287,
            790,
            303
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Preparation of positive keys for Linear Attention. Activation vectors of BDH-GPU belong to the positive orthant, and are often sparse. The interpretation of such vectors depends on whether we consider the positive orthant to be a “valid shape” for the latent concept space of the considered task (in this case, language and reasoning), or whether the task has to be embedded into such a space. For language, this would be a question of whether a word2vec-like internal representation of the concept space by the model has an inherent advantage over a bag-of-words-like representation, especially when expressing concept affinities in attention. ",
        "bbox": [
            114,
            315,
            882,
            400
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "We note that latent representation of key and query vectors in the positive orthant is natural for any problem which is amenable to attention. In the discussion of general attention given by Eq. (14), we noted that the affinity function $\\phi$ takes values in $[ 0 , 1 ]$ , and we considered an embedding $f$ of a set of key vectors $k _ { 1 } , \\ldots , k _ { t }$ into $R ^ { \\nu }$ such that $f ( k _ { t } ) \\cdot f ( k _ { \\tau } ) = \\bar { \\phi } ( k _ { t } , k _ { \\tau } ) \\geq 0$ . Given this condition on non-negativity of dot product on all pairs among the $t$ vectors considered, we could have, without loss of generality, used an appropriately rotated embedding $f$ so that $f ( k _ { \\tau } ) \\in ( R ^ { + } ) ^ { \\nu }$ , thus directly reducing the problem of general attention to a problem of linear attention in the nonnegative orthant. The question which remains is a subtle one: whether this type of embedding of the latent space of language and reasoning in $( R ^ { + } ) ^ { \\nu }$ is ‘natural’, i.e., preserved over long periods of time of inference and training, notably longer than the short window $t$ of context used for Transformer-like attention. ",
        "bbox": [
            114,
            405,
            882,
            531
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "In the rest of the paper, we are generally inclined to assume that representations in $( R ^ { + } ) ^ { \\nu }$ of concepts, combinations of concepts, and density distributions over such combinations of concepts, are universal to language and reasoning. ",
        "bbox": [
            117,
            536,
            879,
            565
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "We limit ourselves to a very brief discussion of a way to represent attention keys with positive vectors for problems for which such a concept representation is not natural. ",
        "bbox": [
            114,
            570,
            879,
            599
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Using LSH to move key vectors into the positive orthant. Locality Sensitive Hashing (LSH) is one technique for converting arbitrary vectors in a lower-dimensional space $R ^ { a }$ , for some fixed $a \\in \\mathbb N$ , into vectors in $( R ^ { + } ) ^ { n }$ , in a way which can be used to describe certain ‘sharp-boundary’ affinity functions $\\phi$ in $R ^ { a }$ . Consider an $n \\times a$ matrix represented as $n$ fixed random vectors $\\lambda _ { 1 } , \\ldots , \\lambda _ { n } \\in R ^ { a }$ , and a corresponding sequence of $n$ appropriately chosen gating functions $\\gamma _ { 1 } , . . . , \\gamma _ { n } : R  R ^ { + }$ . For a vector $v \\in R ^ { a }$ , we define: ",
        "bbox": [
            114,
            612,
            882,
            683
        ],
        "page_idx": 35
    },
    {
        "type": "equation",
        "img_path": "images/c8acad0a8c6ea1b5b8f5ee80b0a51fd99aa8ea9267f0daec32f7425009bc47cf.jpg",
        "text": "$$\nb ( v ) : = \\gamma ( [ \\lambda _ { 1 } \\dots \\lambda _ { n } ] v ) = ( \\gamma _ { i } ( v ^ { T } \\lambda _ { i } ) ) _ { 1 \\leq i \\leq n } .\n$$",
        "text_format": "latex",
        "bbox": [
            349,
            685,
            647,
            704
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Each $i$ -th element of vector $b$ thus corresponds to the outcome of the $i$ -th bucket of LSH. ",
        "bbox": [
            112,
            707,
            692,
            722
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "The bucketing function $b$ may now be used to prepare queries and keys as attention inputs. If $\\gamma _ { i }$ is a $\\{ 0 , 1 \\}$ -valued threshold function, then, for $q , k _ { i } \\in R ^ { a }$ , $b ( q ) ^ { T } b ( k _ { i } )$ is an attention affinity function between $q$ and $k _ { i }$ , equal to the number of LSH buckets shared between $q$ and $k _ { i }$ . ",
        "bbox": [
            114,
            728,
            883,
            772
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Observation 7. The LSH vector affinity function $b$ , given by Equation (15), using n buckets on vectors in $R ^ { a }$ for some $a \\in \\mathbb { N }$ , can be expressed through Linear Attention with attention keys in the positive orthant $( R ^ { + } ) ^ { n }$ . □ ",
        "bbox": [
            112,
            775,
            879,
            804
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "In the ReLU-based setup considered in BDH-GPU, an appropriate function $b$ is plausibly easy to learn. LSH is a ‘sharp-boundary’ technique, well-suited for finding $k$ -nearest-neighbors of a queried vector in a set of keys. Hence, the class of attention affinity functions, naturally expressible using BDH-GPU, also includes such ‘sharp’ functions. ",
        "bbox": [
            116,
            813,
            883,
            856
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Attention in the positive concept space of language and reasoning. BDH-GPU uses the positive orthant $( R ^ { + } ) ^ { n }$ as its latent space for representing combinations of concepts in its activation vectors. Attention keys and queries are prepared entirely in this positive orthant. ",
        "bbox": [
            116,
            869,
            882,
            911
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "When representing a task of reasoning or language inference in a high-dimensional space, positive activation vectors in $( R ^ { + } ) ^ { \\bar { n } }$ have a natural interpretation of convex combinations of concepts. Such convex combinations of concepts may represent both semantically connected concepts (“bags-of-concepts”), and mixed states of uncertainty between unconnected concepts. In this interpretation, a positive vector is considered as a state of certain knowledge when its L1-norm and L2-norm align closely. Note that for a (normalized) probability vector, the only vectors for which L1-norm and L2-norm coincide precisely are distributions concentrated on a single coordinate. ",
        "bbox": [
            114,
            90,
            882,
            174
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Linear Attention of BDH-GPU is capable of amplifying very small differences between keys in the L1-norm when matching queries to keys. Consider, for instance, two probability distribution vectors $x _ { 1 } , x _ { 2 } \\in ( R ^ { + } ) ^ { n }$ , where $x _ { 1 } =$ $\\textstyle ( \\alpha , { \\frac { 1 - \\alpha } { n - 1 } } , { \\frac { 1 - \\alpha } { n - 1 } } , \\dotsc , { \\frac { 1 - \\bar { \\alpha } } { n - 1 } } )$ and $\\begin{array} { r } { x _ { 2 } = ( \\frac { 1 - \\alpha } { n - 1 } , \\alpha , \\frac { 1 - \\alpha } { n - 1 } , \\ldots , \\frac { 1 - \\alpha } { n - 1 } ) } \\end{array}$ , for some $0 \\textless \\alpha \\textless 1$ . Now, vectors $x _ { 1 }$ and $x _ { 2 }$ almost coincide when treated as probability distributions, $\\lVert x _ { 1 } - x _ { 2 } \\rVert _ { 1 } = O ( \\alpha ) = \\lVert x _ { 1 } - x _ { 2 } \\rVert _ { \\mathrm { T V D } }$ . However, they are extremely different when considered as keys for the Linear Attention mechanism, with $x _ { 1 }$ showing very weak affinity to $x _ { 2 }$ : ${ x _ { 1 } } ^ { T } x _ { 2 } = { O } ( \\alpha ^ { - 2 } n ^ { - 1 } ) { x _ { 1 } } ^ { T } x _ { 1 }$ . ",
        "bbox": [
            114,
            180,
            883,
            267
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Observation 8. In key-query matching, the Linear Attention mechanism of BDH-GPU is able to separate positive keys which are close in the $L l$ -norm, strongly amplifying $L l$ -norm differences of activation vectors. □ ",
        "bbox": [
            116,
            270,
            880,
            299
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "This mechanism can be treated as complementary to the propagation dynamics of positive activations in the feedforward network, discussed in Section 5.3. ",
        "bbox": [
            119,
            308,
            877,
            337
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Natural support for long context. There is no bound on context length in BDH-GPU, so the actual $t = { \\tilde { \\Omega } } ( { \\sqrt { n } } )$ to $t = { \\tilde { O } } ( n )$ “equally important facts” that a BDH-GPU model can distinguish in each layer in view of Claim 7 do not have to correspond to the latest $t$ “facts” seen in context. For example, if, for some layer $l$ , mechanisms from lower layers deem a given entry to be irrelevant for layer $l$ , and provide an extremely weak attention ‘value’ for this layer, and this key-value entry is effectively seen as omitted. This mechanism corresponds to weaker signals $y$ in a layer which needs to take no action on a given input, e.g., does not have to remember it (cf. Fig. 14). Indeed, empirically we observe progressive de-noising of state in the higher layers, with only small fractions of input tokens requiring significant key-value state update in the middle layers across the entire spectrum of state $\\rho$ of neurons. ",
        "bbox": [
            114,
            349,
            882,
            464
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "As a result, the middle and higher layers of BDH-GPU may, in principle, have unbounded look-back on context. Nonetheless, as context length $t$ increases, we find that damping of historical signals over long sequences is necessary in BDH-GPU to avoid overwhelming the model with noise from stale context. For the vanilla version of the architecture, we found that RoPE combined with ALiBi provide a sufficient remedy, and model performance improves as context length increases. More advanced techniques for BDH-GPU, related to selective forgetting, state compression, or other forms of state optimization, can also be added to the architecture. ",
        "bbox": [
            114,
            469,
            882,
            554
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "6.2 Micro-interpretation of attention in BDH-GPU ",
        "text_level": 1,
        "bbox": [
            116,
            569,
            480,
            584
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "BDH maintains its state in the $n \\times n$ matrix $\\sigma$ that has a clear interpretation as synapse weights that connect neurons (cf. Section 2). On the other hand, BDH-GPU’s state $\\rho$ is a $n \\times d$ matrix. To perform the analysis for BDH-GPU, in this section we recover $\\sigma$ from the relation: ",
        "bbox": [
            114,
            594,
            883,
            637
        ],
        "page_idx": 36
    },
    {
        "type": "equation",
        "img_path": "images/68e8a5c5dc0abf15f8bc98d060fee58801f7978aea2c23d53ea002c8c4637ba3.jpg",
        "text": "$$\n{ \\pmb { \\sigma } } _ { t - 1 , l } = \\sum _ { \\tau < t } y _ { \\tau , l - 1 } { x _ { \\tau , l } } ^ { T } U ^ { t - \\tau }\n$$",
        "text_format": "latex",
        "bbox": [
            395,
            637,
            602,
            671
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "We first analyze the neuron relationship graph encoded by matrix $\\sigma$ . As explained in Section 2.2, $\\sigma$ can be interpreted as a graph of context dependent implications between $x$ and $y$ . We compute the $\\sigma$ matrix for 0-th head at layer 5 of an 8-th layer network trained on Europarl translation corpus (Koehn, 2005) (we provide more details in Appendix B.3). We filter out negative entries which are introduced by the RoPE positional embeddings (Su et al., 2024) and enforce a small positive threshold on remaining values to further sparsify the network structure. We plot the histograms of neuron in- and out-degrees, unraveling a scale-free network structure. ",
        "bbox": [
            114,
            683,
            883,
            766
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Encouraged by the emergent network structure, we have identified a few synapses that are activated at recognizable concepts, we show examples in the next section. ",
        "bbox": [
            114,
            772,
            880,
            801
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "6.3 Empirical findings: monosemantic synapses ",
        "text_level": 1,
        "bbox": [
            117,
            815,
            459,
            832
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "We have identified in the $\\pmb { \\sigma }$ matrix entries (synapses) that show activity whenever a currency name or country name, both frequently occurring in the Euro-parliament transcripts, is present in the processed sentence. We have identified the synapses by searching for entries in $\\sigma$ that have predictive power at separating sentences containing a concept from contrast sentences. We present a few examples in Figure 12. We note that the synapses strength changes abruptly after words that are related to each concept. The same synapse is activated for concepts in both French and English sentences, even when the words used are different (e.g. “livre sterling” vs “British Pound”). Synapse selectivity to a semantic context stems directly from sparsity of neuron activations as shown in Fig. 13. ",
        "bbox": [
            116,
            842,
            882,
            911
        ],
        "page_idx": 36
    },
    {
        "type": "image",
        "img_path": "images/8dad52b2863559f940883beb0cc87cf720d3524daf0377bf58af3f817e2bc977.jpg",
        "image_caption": [
            "Figure 11: BDH’s state $\\sigma$ encodes neuron connections as a scale-free graph showing clear heavy-tailed (power-law-like) degree distribution. "
        ],
        "image_footnote": [],
        "bbox": [
            215,
            92,
            781,
            304
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            122,
            368,
            882,
            397
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "To confirm the selectivity of the synapses, we have generated, using ChatGPT, 50 sentences relating to European currencies, and another set of 50 sentences speaking about European politics, but not mentioning currencies. A onesided Mann–Whitney U test revealed that sentences relating to currencies received significantly higher “Currency synapse” values than those without the currency concept $U = 2 3 6 8$ with $U _ { \\mathrm { o p t } } = 2 5 0 0$ , $\\overset { \\sim } { p } < 1 0 ^ { - 1 \\bar { 4 } }$ ). The rank-biserial correlation was 0.86, further confirming association between Currency concept presence and synapse value. ",
        "bbox": [
            116,
            404,
            882,
            473
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "6.4 Empirical findings: sparse neuron activations ",
        "text_level": 1,
        "bbox": [
            116,
            489,
            470,
            505
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "Sparsity of signals is often a prerequisite to their interpretability. In section 6.3 we have shown that BDH has monosemantic synapses, selectively activated by occurrences of specific concepts. In this section, we experimentally show that neuron activity correlates with signal predictability: fewer neurons are active, or equivalently, layer activations become sparser, for more predictable input signals. ",
        "bbox": [
            114,
            515,
            882,
            570
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "We have trained a BDH-GPU model with $n = 6 5 5 3 6$ neurons, $d = 2 5 6$ , $L = 4$ layers, and tokenization on letters of the Latin alphabet, to perform a single synthetic next-token prediction task. The input sequence started with a fixed 13-letter warm-up sequence, followed by 8 repetitions of an 8-letter random word (“fact”), with the same pattern repeating every $1 3 + 8 \\cdot 8 = 7 7$ letters. In Fig. 14, we show neuron activity patterns. We can notice that neurons in higher layers are active during warm-up and fact introduction, then become quiet. We then group neurons by their RoPE frequencies and find that largest difference of activity during memorization and repetition is shown by the slow-acting neuron population. ",
        "bbox": [
            114,
            575,
            882,
            674
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "From a biological standpoint, sparse and surprisal-driven neuron activation lowers energy consumption — despite fluctuations in low level percepts (in the experiment tokens are changing at every timestep), neurons in higher layers are inactive and do not expand energy. From a Deep Learning perspective, it has been recently shown that input complexity is related to predictability of internal representations of Transformers (Herrmann et al., 2025). BDH makes this very explicit and does not require a separate prediction network: the predictable steady-state consists of zero activations, and input complexity entails neuronal activity. This suggests that BDH, natively, at a neuron level, implements mechanisms reminiscent of adaptive computation time (Graves, 2017) and conditional computation (Cho and Bengio, 2014; Shazeer et al., 2017), used in modern Transformers to lower computational effort during inference. ",
        "bbox": [
            114,
            679,
            882,
            791
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "Finally, sparse activation vectors in BDH imply that potentiation of specific synapses occurs rarely during inference. This is useful from the point of view of interpretability, noise reduction in Linear Attention state, and opens the door to simplified and compressed representations, notably for state and for gradient backpropagation DAG’s. ",
        "bbox": [
            116,
            797,
            883,
            840
        ],
        "page_idx": 37
    },
    {
        "type": "image",
        "img_path": "images/cbf9868c35ac95f2a7ed27764d387eaac6695dcab9d7d70db38bd1453dca359c.jpg",
        "image_caption": [
            "Figure 12: Evolution of values set by BDH-GPU on 2 specific synapses which we have named (following their interpretation) as “currency synapse” and “country synapse”, relating to concepts naturally present in European Parliament transcripts on which the model was trained. We can notice that mentions of country or currency names result in an increase of the respective synapse value, indicating a stronger presence of the concept in the context. Moreover, the synapses consistently became activated in both French and English, confirming the (notice how it reacts both to “British Pound” and “livre sterling”). For visual clarity, we indicate changes that clear a small threshold with the $^ *$ character (the changes in activity when the system is processing the translation of a source sentence tend to be small). "
        ],
        "image_footnote": [],
        "bbox": [
            112,
            128,
            882,
            766
        ],
        "page_idx": 38
    },
    {
        "type": "image",
        "img_path": "images/8f3b06c82607c84f3303f3d702446e07dd14e88a0ad244766e2be7984e6d9f45.jpg",
        "image_caption": [
            "Neuron i activations ",
            "Figure 13: Sparse updates to synapses related to meaningful concepts stem from sparse neuronal activations. BDH-GPU maintains in its recurrent state a “currency synapse” (a concept naturally present in the Europarl corpus, see also Fig. 12). The synapse is updated using a Hebbian learning rule when activity in $_ y$ activations at a preceding layer (4 in the example) leads to firing of neuron $_ x$ in the next layer (5). "
        ],
        "image_footnote": [],
        "bbox": [
            191,
            147,
            812,
            313
        ],
        "page_idx": 39
    },
    {
        "type": "image",
        "img_path": "images/22aa15fff60a34c9a491b1b9f70e8000537703792be3ed1e5c6978d53cf5fd53.jpg",
        "image_caption": [
            "Figure 14: Neurons in BDH-GPU are less active (signal is sparser) when the input is predictable. The input sequence started with a fixed 13-letter warm-up sequence, followed by 8 repetitions of an 8-letter random word (“fact”), with the same pattern repeating every $1 3 + 8 \\cdot 8 = 7 7$ letters. (a) Fraction of neurons with non-zero entry $y _ { t , l }$ in different layers $l$ , with fact memorization effect noted through increased activation level in layer 2. The activation in layer 2 has $4 . 0 \\% - 7 . 5 \\%$ non-zero entries during memorization and approximately $2 . 5 \\%$ non-zero entries during repetition. (b) Detailed breakup of activation sparsity in layer 2, with neurons bucketed into equal fractions by their RoPE phase: fr $\\mathsf { e q 0 } \\in [ 1 , 4 ]$ , freq1 $\\in$ [4, 16], freq2 $\\in$ [16, 64], . . ., freq7 $\\in$ [16384, 65536]. The slow-acting half of the neuron population (freq4 − freq7) exhibits the largest amplitude ratio between peak activation during memorization and repetition phases. "
        ],
        "image_footnote": [],
        "bbox": [
            151,
            479,
            864,
            755
        ],
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "7 Playing with the Hatchling ",
        "text_level": 1,
        "bbox": [
            116,
            89,
            370,
            107
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "7.1 Model merging: concatenating two models ",
        "text_level": 1,
        "bbox": [
            116,
            121,
            450,
            136
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "Updating models with up-to-date knowledge and expanding models knowledge-base will become crucial in practical applications of AI. On possible solution is model composability, potentially allowing building of larger models by assembling a number of smaller, specialized models into a larger, more powerful one. A natural hope for such a system would be the achievement of ”more is different than a sum of its part” effect. In the following experiment we are showing that doing so is relatively straight forward with BDH-GPU. This is because BDH-GPU can be scaled by varying only the number of neurons $n$ . In this section we explore whether we can create larger models directly concatenating smaller models trained on disjoint subsets of data. Details in Appendix B.4. We have experimented with the following simple model merging procedure: ",
        "bbox": [
            114,
            145,
            883,
            257
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "1. Train a base model on a chosen language pair. In the experiment we have used English-Spanish (En-Es) translation data, and have trained a model with $n = 2 4 5 7 6$ neurons (19M parameters).   \n2. Clone the base model and continue training on two datasets: English-French (En-Fr) and English-Portuguese (En-Pt).   \n3. We then merge the weights of the En-Fr and En-Pt models to create a new En-FrPt model with $n = 2 4 5 7 6 { \\cdot } 2 =$   \n49152 neurons (38M parameters). To create the merged model we: (a) concatenate all parameter tensors that have an $^ { \\circ } n ^ { \\mathrm { : } }$ ’ dimension (e.g. $D _ { y }$ , $D _ { x }$ , $E$ , RoPE frequency buffers) along their $n$ dimension, (b) average all other parameters (e.g. token embeddings and token prediction weights). To validate the hypothesis that direct model merging is feasible, we report all results on the merged model without any subsequent training or finetuning. However, we have verified that the merged model quickly improves when trained on all language pairs.   \n4. After each stage we evaluate the models on all involved language pairs: En-Es, En-Fr, En-Pt, regardless of the data seen by the model up to this stage. ",
        "bbox": [
            147,
            267,
            885,
            489
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "We report quantitative results in Table 2 and show qualitative results of merged model operation in Figure 15. The merged model shows human-like degradation of operation: while it retained the capability to generate and translate into English, it has lost the ability to generate proper text in Spanish, French, or Portuguese, mixing words and grammatical constructs. We have verified that a small amount of training on all language pairs restore the model’s proficiency in Spanish, French and Portuguese. However, we decided to report on the behavior of the merged model without any subsequent tuning to highlight the possibilities of model engineering offered by the large and sparse working dimension $n$ of BDH-GPU. ",
        "bbox": [
            114,
            500,
            883,
            597
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "The BDH-GPU model merging experiment has shown that when the model latent space promotes concept disentangling (c.f. Section 6.2 on monosemanticity) then it is feasible to directly compose concepts in this space, e.g. by concatenation of weights from different models. This feature of the BDH architecture allows us to see the models as composable computer programs with emergent properties. ",
        "bbox": [
            114,
            603,
            882,
            660
        ],
        "page_idx": 40
    },
    {
        "type": "table",
        "img_path": "images/4c4fffad9f8c4fabeaa0f81504930f61952e9cb265d11e7031cd0c2f86890fa4.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td colspan=\"3\">Translation into English</td><td colspan=\"3\">Translation from English</td></tr><tr><td>Model:</td><td>Es-→En</td><td>Fr-→En</td><td>Pt-→En</td><td>En→Es</td><td>En→Fr</td><td>En→Pt</td></tr><tr><td>1: Base En-Es</td><td>0.36</td><td>0.77</td><td>0.64</td><td>0.35</td><td>2.21</td><td>2.27</td></tr><tr><td>2: Base (1) tuned on En-Fr</td><td>0.58</td><td>0.36</td><td>0.68</td><td>2.57</td><td>0.31</td><td>2.54</td></tr><tr><td>3: Base (1) tuned on En-Pt</td><td>0.44</td><td>0.76</td><td>0.34</td><td>1.79</td><td>2.20</td><td>0.33</td></tr><tr><td>4: Merged (2|l3)</td><td>0.43</td><td>0.40</td><td>0.39</td><td>1.45</td><td>0.77</td><td>0.86</td></tr></table>",
        "bbox": [
            192,
            681,
            805,
            781
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "Table 2: Validation next token prediction losses (lower is better) of translation models trained on different language pairs and then merged. We evaluate each model on En-Es, En-Fr, En-Pt language pairs separately. We can see that the base model can translate between English and Spanish, while on En-Fr and En-Pt tasks it falls back on perplexities of an unconditional English language model (loss about 0.65) and can’t generate proper French or Portuguese. After tuning on French or Portuguese the model learns to translate between respectively English and French or English and Portuguese, while somewhat retaining the capacity to translate Spanish to English and losing the capability to translate English to Spanish. The merged model can translate Spanish, French, and Portuguese to English, however it mixes these three languages when asked to translate from English. This is consistent with qualitative results shown in Figure 15. ",
        "bbox": [
            114,
            796,
            883,
            898
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "<F:es>Esta es una afirmación clara.<T:en> | In this clarification, it is a clear statement.   \n<F:es>Esta es una afirmación clara.<T:en> | It is a clear statement.   \n<F:es>Esta es una afirmación clara.<T:en> | That is a clear affirmation. <F:fr>C’est une déclaration claire.<T:en> This is a clear statement.   \n<F:fr>C’est une déclaration claire.<T:en> This is a clear declaration.   \n<F:fr>C’est une déclaration claire.<T:en> It is a clear declaration. <F:pt>Esta é uma afirmação clara.<T:en> | That is a clear statement.   \n<F:pt>Esta é uma afirmação clara.<T:en> | This is a clear statement.   \n<F:pt>Esta é uma afirmação clara.<T:en> | This is a clear assertion. ",
        "bbox": [
            114,
            88,
            665,
            118
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            128,
            552,
            159
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            116,
            169,
            527,
            199
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "(a) Sampled translations from Spanish, French, and Portuguese to English. The vertical bar | delimits the prompt from model output. Each translation was sampled three times to show the model’s consistency. ",
        "bbox": [
            111,
            204,
            880,
            227
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "<F:en>This is a clear statement.<T:es> Ce récent statement está clarificative.   \n<F:en>This is a clear statement.<T:es> Il se revela exact.   \n<F:en>This is a clear statement.<T:es> Constato que cette déclaration était monstruosa.   \n<F:en>This is a clear statement.<T:fr> Está de noto uma déclaration.   \n<F:en>This is a clear statement.<T:fr> Ce sont une declaration clare.   \n<F:en>This is a clear statement.<T:fr> C’está uma declaração clare.   \n<F:en>This is a clear statement.<T:pt> Esta declaração étangling état está clara.   \n<F:en>This is a clear statement.<T:pt> 1 Istambigna de contence.   \n<F:en>This is a clear statement.<T:pt> C’estaté clarification é clara!. ",
        "bbox": [
            114,
            239,
            655,
            352
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "(b) Sampled translations from English into Spanish, French, and Portuguese. The model mixes the three languages, though the meaning of the source sentence seems to have been preserved. ",
        "bbox": [
            109,
            356,
            880,
            376
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "<F:en> | The European Convention on Human Rights has been set up in 1992, when it applied the Convention in 1 <F:es> Naturalment, nos deputés de toda autre groupe southern Italians, notariously engaged in the discuss <F:fr> (ES) Mr President, aproveit de montrar ma satisfaction por a surprise de la parte de Milan, et parti <F:pt> | (FI) Mr President, todos outreaches, mesures on ways and means of promoting the economic development (c) Language-conditional samples. Consistently with the translation experiment in (a) and (b) above, the model properly generates English, and mixes Spanish, French, and Portuguese, sometimes slipping into English. ",
        "bbox": [
            114,
            391,
            784,
            433
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            109,
            435,
            880,
            458
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "<F:es>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> | A Lingue nominated Project is Romanian published in Romanian pasta year.   \n<F:fr>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> | My language in Project Nomina is Romani esedi publish posted peas in maybe the same year.   \n<F:pt>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> | I have been a Latva project nomination Romanim esede publica published in May of the postal service and the service provide ",
        "bbox": [
            114,
            472,
            880,
            534
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "d) Attempts of the model to translate a sentence in Romanid (2025), a zonal auxiliary language for speakers of Romance languages naturalistic constructed language intended to be intuitively understandable to speakers of Romance languages. The English translation of the prompt is: “My language project called Romanid was published already in May of last year”. The model is able to pick up some of the meaning of the prompt. ",
        "bbox": [
            117,
            536,
            882,
            569
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Figure 15: Conditional and unconditional samples generated from a English-Spanish-French-Portuguese translation model created by direct concatenation of parameters of models trained on distinct language pairs. ",
        "bbox": [
            114,
            574,
            880,
            601
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "7.2 Training without backpropagation through time ",
        "text_level": 1,
        "bbox": [
            116,
            638,
            488,
            654
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Sparsity of synapse activations in BDH opens the door to efficient approximations to backpropagation through time. The main intuition is that we only need to remember when a synapse has changed, and the $i , j$ coordinates of the synapse implicitly encode which neurons were active and should take part in error signal backpropagation. ",
        "bbox": [
            116,
            670,
            883,
            712
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "In this section we report results of preliminary experiments on the impact of removal of backpropagation through time on model performance. For the PyTorch implementation in Appendix E, this corresponds to ‘detach’-ing variables K and $\\mathtt { V }$ in the implementation of the LinearAttention class. ",
        "bbox": [
            116,
            717,
            882,
            760
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "In particular, we found that such a model, trained without any backpropagation through time, retained some ability to model language, but lost the ability to match concepts between different languages during translation. For translation tasks like those presented in Table 2, loss values for English increased from a loss level of approximately 0.65 for an unconditional English language model (trained with backpropagation over time), to loss of approximately $0 . 7 5 - 1 . 0 5$ for a model trained without backpropagation over time, depending on model variant, regardless of whether English was the source or target language in translation. No significant difficulties were encountered during training when crossing the barrier of the letter-bigram language model, at loss value 2.4. ",
        "bbox": [
            116,
            766,
            882,
            863
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Beyond side-effects of the general design, we did not optimize the BDH-GPU model for suitability of training without backpropagation. We consider this architecture to be a good starting point for bootstrapping further investigations in this direction. ",
        "bbox": [
            117,
            869,
            882,
            911
        ],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "8 Conclusions ",
        "text_level": 1,
        "bbox": [
            114,
            89,
            248,
            106
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "8.1 Takeaways for model engineering ",
        "text_level": 1,
        "bbox": [
            116,
            121,
            387,
            136
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "This paper leads up to a new class of language and reasoning models which eliminate architecture nonuniformities, notably in terms of scaling for model size, and handling of time scales of inference. ",
        "bbox": [
            119,
            147,
            885,
            176
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "The BDH-GPU architecture introduced in this paper opens the following opportunities: ",
        "bbox": [
            116,
            181,
            687,
            196
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "1. New ways of scaling models for time and size. BDH-GPU is a state-space model which scales for size in one large dimension $n$ (neurons in this dimension are indexed by RoPE oscillator frequency). Subject to appropriate sharding, this also leads to a desirable form of locality: important data is located just next to the sites at which it is being processed. This minimizes communication, and eliminates the most painful of all bottlenecks for reasoning models during inference: memory-to-core bandwidth.   \n2. Faster model iteration. During training and inference alike, BDH-GPU provides insight into parameter and state spaces of the model which allows for easy and direct evaluation of model health and performance, notably, through sparsity-related measures and through aggregates and statistics on the large pool of homogeneous neurons, even for relatively small models. Attention and parametric layers alike operate on the same neuron dimension (‘concept dimension’).   \n3. Direct explainability of model state. Elements of state of BDH-GPU are directly localized at neuron pairs, allowing for a micro-interpretation of the hidden state of the model.   \n4. New opportunities for ‘model surgery’. The BDH-GPU architecture is, in principle, amenable to direct composability of model weights in a way resemblant of composability of programs. This concerns the potential both the direct composition of separately trained model parts, as well as ‘surgery’ of parameter spaces of models, by inserting fragments of manually programmed protocols into machine-learned code. ",
        "bbox": [
            140,
            208,
            890,
            450
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "8.2 Implications for brain science ",
        "text_level": 1,
        "bbox": [
            116,
            467,
            359,
            482
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "We have obtained a micro-foundational description of attention for artificial language and reasoning models, expressed in a framework of local graph dynamics. This has been found to be consistent with the effects observed for the same function of attention for language and reasoning in the brain. By introducing a translation layer based on similarity of function between the artificial and biological planes, for blocks of feed-forward neural networks and attention mechanisms, our work points to the following hypothesis: complex systems effects which are observed in the brain, around modular scale-free network structure, synaptic plasticity, and Hebbian learning arose from its core purpose — doing reasoning — and not from any specific longer-term training dynamics which the brain applies. ",
        "bbox": [
            116,
            493,
            883,
            592
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "We have exhibited how a general attention mechanism can be efficiently implemented as an artificial neuronal system with spiking neurons and synapse plasticity. More formally, we first describe the class of local interaction dynamics which any system plausibly needs to implement attention mechanisms. We then confirmed that the edge-reweighting rule is sufficient to allow a certain artificial Language Model (BDH-GPU) to operate at least at the level of the Transformer. For an artificial network, the edge-reweighting rule intuitively describes the interaction between two artificial neurons exhibiting rapid state-change behavior, and one synaptic neuron interconnection element exhibiting plasticity as shown in Fig. 13. ",
        "bbox": [
            116,
            597,
            882,
            694
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "More broadly, this work may potentially serve to support efforts aiming to isolate, from among the many extremely complex electrochemical patterns and signal dynamics occurring in the brain, those that are crucial for solving tasks in-context (based on attention), from those that potentially serve other purposes, such as transfer of information from short-term memory to long-term memory, or long-term improvement of brain function (learning). ",
        "bbox": [
            116,
            700,
            882,
            756
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "How this work helps with axiomatization of learning theory in the brain. Attempts to understand the brain, starting from the perspective of longer time scales of training, have proved extremely challenging, defying progress. This paper pin-points attention-based reasoning at shorter time scales as ‘the other end of the string’, and hints how, from here, untangling the entire story will plausibly be easier. ",
        "bbox": [
            116,
            772,
            882,
            829
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "For natural systems undergoing continuous learning, the time scales to look at are: language function and reasoning (chain-of-thought inference), then short-to-long memory transfer from state to network weights, adaptation of structure: changes to interconnections, and finally, changes to neuron nodes. ",
        "bbox": [
            116,
            835,
            882,
            877
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "For long time scales, this reduces the question of finding supervised training dynamics form the most general case, to a specific class of local dynamics: an interaction kernel performing ‘edge-reweighting’ rules. As these rules appear fundamental to logical inference and biochemical processes alike, its universality in processes that the brain is responsible for is plausible also beyond the realm of language-based reasoning. ",
        "bbox": [
            112,
            883,
            882,
            911
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            120,
            90,
            883,
            119
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "From a systems perspective, we arrive at the following possible explanation. The brain generally tries to be lazy in terms of energy expense, and does things as late as it can. Only reasoning needs to happen close to a critical regime, because it involves executing a real-time program which needs to be responsive, since the life and success of the biological organism depends on it. Then, for a certain time, which may be minutes for humans, the brain has enough synapses in it to represent (almost) all useful information it needs for reasoning, decision-making, etc. — all stored in short-term state, at synapses (and/or neurons). Some of the neuron activations which the brain performs at this time scale represent ‘gradients of state’ — the gradients of in-context learning, passed on to modify synapse strength, in a weight-update process. As time goes by, the system runs out of state space. Then, memory processes work to iron things out, preserving in more permanent neuron connection weights and graph structure the elements of state that have been reinforced by feedback signals. Overall, there are fewer and fewer things that need to be remembered across progressively longer time scales. However, this entire memory process is, plausibly, subsidiary to the definition of the dynamics of reasoning and the synaptic dynamics of state that we discuss in this paper. In other words, the best form of description of the relaxation from state into longer-term memory follows from the specific kernel of the reasoning dynamics, such as the edge-reweighting kernel. ",
        "bbox": [
            114,
            125,
            883,
            319
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "As for the ratio of time scales (measured in tokens for language), we can estimate that the time lapse after which harmonizing state with a memory process becomes important is of about the same order of magnitude as the average time between ‘writes’ (significant transmission increases) for individual synaptic elements (see e.g. Fig. 14). In our models, this time is lower-bounded by the inverse of sparsity of the vector $y$ , i.e., $1 / \\rho \\approx 1 / 5 \\% ^ { - } = \\hat { 2 0 }$ tokens, but it could be much larger for larger systems; we also do not force it in any way to be sparser during training. During training with backpropagation, if the backpropagation window $T$ is short enough, $T < 1 / \\rho$ tokens, we can plausibly assume that a synapse changes state only once in that window (and is used multiple times), hence the DAG of gradient backwards propagation is much more direct to embed within the system graph. Backpropagation is then a question of ‘routing’ gradients in the neuron communication graph, and not one of disentangling them. All natural training approaches, whether based on backpropagation, or any more direct form of relaxation ‘from state into weights’, appear to bottleneck on the amount of available state space on synapses, becoming necessary at about $T \\sim 1 / \\bar { \\rho }$ by a simple information-theoretic argument on state storage capacity. ",
        "bbox": [
            114,
            325,
            882,
            491
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "Regardless of how much of this is an accurate description, and how much an intuition, at the very least, it appears we may now have a way forward. Some part of the “global mystery” of learning in the brain can be reduced to a more “localized problem” of state-to-operator transfer for some relatively compact form of state-space dynamics (i.e., one specific local graph kernel). This change of perspective brings in both a completely new ‘problem landscape’ in which to navigate towards a complete solution, as well as a set of new methods to use for the different types of graph structure changes involved in learning, including approaches from distributed computing, evolving network theory, and graph rewiring systems. ",
        "bbox": [
            116,
            497,
            882,
            595
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "At this point, it seems one natural next step would be to ground the current discussion more deeply in findings of brain science, to refine or simplify the actual kernels used by brain reasoning (which was not the objective of this paper), and potentially seek validation through experiment. ",
        "bbox": [
            116,
            601,
            882,
            643
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "8.3 Societal impact ",
        "text_level": 1,
        "bbox": [
            116,
            659,
            261,
            674
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "This paper is a voice in favor of bringing principled understanding to reasoning in Machine Learning. Axiomatic AI provides an opportunity to reduce risks related to unpredictable behavior of AI models, and, to open or accelerate new development directions. The subject matter which we consider here serves as a direct introduction to the most crucial problem that lies ahead: controlling the behavior of autonomous AI reasoning models and AI systems as they progress across time scales, from seconds to years. ",
        "bbox": [
            116,
            684,
            882,
            753
        ],
        "page_idx": 43
    },
    {
        "type": "table",
        "img_path": "images/032f8a74258db708004a324ecba2ee49d1742cf335f76b7f3f4a02182e58f666.jpg",
        "table_caption": [
            ""
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>Transformer (GPT2)</td><td>BDH-GPU (n,d)</td><td>BDH(n,△)</td><td>Brain models (reasoning and language function)</td></tr><tr><td>Inference hardware</td><td>GPU, CPU</td><td>GPU, CPU</td><td>CPU, Sparse GPU kernels, Neuromorphic</td><td>Brain and supporting systems</td></tr><tr><td>Model weights (predominant location)</td><td>5 tensors per layer (different shapes)</td><td>3 tensors per model (same shape n ×d)</td><td>Neuron-synapse graph: connection topology, edge weights</td><td>Neuron-synapse graph: connection topology, edge weights</td></tr><tr><td>Representation of attention</td><td>KV-cache tensor (not localized at neurons)</td><td>n × d tensor for each layer (localized at neurons)</td><td>Memory on synapse edge weights</td><td>State memory through synapse plasticity</td></tr><tr><td>Macro-description of attention</td><td>Key lookup data-structure, key-value map</td><td>Key lookup data-structure, key-value correlation matrix</td><td>Key lookup data-structure, key-value correlation matrix</td><td>Not known</td></tr><tr><td>Micro-description of attention</td><td>None</td><td>Neuron-pair correlations in context (transformed)</td><td>Neuron-pair correlations in context</td><td>Strengthened or weakened connections between neurons based on context</td></tr><tr><td>Scaling for model size</td><td>Multiple combinations of dimensions,e.g.MLP scales with D × D,scales separately with context length</td><td>Uniform linear array of n particles in a mean-field</td><td>n-node graph model</td><td>n-node graph model with evolving graph mechanisms</td></tr><tr><td>Distributed sy stem micro-architecture</td><td>Follows from compiled matrix multiplication kernels, non-uniform</td><td>Particles run identical local kernels, communicating O(d)-size messages through mean-feld,and storing local state</td><td>All n neuron nodes run identical local kernels,communicating over neuron-synapse graphs. Some synapses act as memory elements.</td><td>n neurons run local kernels and communicate through a network, using numerous signal patterns and coding schemes. Synapses act as memory</td></tr><tr><td>Macro-expressiveness of programs (approximation)</td><td>RASP-L, C-RASP</td><td>RASP-L, C-RASP</td><td>RASP-L, C-RASP (or superset)</td><td>element. Unknown</td></tr><tr><td>Micro-expressiveness of programs</td><td>Unknown</td><td>Subset of BDH</td><td>Probabilistic rule-based local protocols.Micro-Inductive bias interpreted as reasoning system</td><td>Unknown</td></tr><tr><td>Emergence of structure</td><td>Partially interpretable concept layer (evidence of monosemantic neurons for</td><td>Evidence of emergent network, oscillator dynamics</td><td>in a form of propositional logic. Emergent network,oscillator dynamics</td><td>Emergent network; oscillatory effects; possible monosemantic &quot;grandfather neurons&quot;</td></tr><tr><td>Activation vectors</td><td>important concepts) Dense activation; can be subsampled or sparsified by architecture modification</td><td>Positive vectors,sparse fresh activation vectors y</td><td>Positive vectors,sparse fresh activation vectors y</td><td>Sparse positive activation vectors</td></tr></table>",
        "bbox": [
            112,
            90,
            844,
            910
        ],
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Acknowledgments ",
        "text_level": 1,
        "bbox": [
            117,
            92,
            245,
            106
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "The authors thank David Sussillo, Navdeep Jaitly, and Emanuele Natale for insightful discussions on reasoning and the brain, and for early feedback on this write-up. We also thank Samy Bengio for comments on the presentation. We kindly acknowledge the support of all of the Pathway team, notably, Paweł Podhajski for his amazing help with cluster setup, Victor Szczerba and Z Schwab for all discussions over coffee, and Kamil Piechowiak and Chris Ociepa for constructive comments on the presentation. AK thanks Christos Papadimitriou for being the direct inspiration for us to embark on this journey. ",
        "bbox": [
            116,
            116,
            882,
            200
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "Author contributions ",
        "text_level": 1,
        "bbox": [
            117,
            217,
            264,
            231
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "AK conceived the BDH and BDH-GPU architectures, conceived most of the theory, developed most of the model source code, conceived and performed experiments on synapses, and wrote most of the paper. ",
        "bbox": [
            119,
            241,
            883,
            270
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "PU contributed crucial elements of BDH-GPU architecture, contributed model and framework source code, contributed to theoretical analysis, and performed experiments. ",
        "bbox": [
            117,
            276,
            879,
            304
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "JCh led, designed, and oversaw methodology of experiments, led framework development, contributed major improvements to BDH-GPU architecture, contributed to the theory, implemented baselines, performed experiments, and substantially redacted the paper. ",
        "bbox": [
            116,
            310,
            883,
            352
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "ZS conceived the project, guided research directions, introduced particle-interaction interpretation, acted as final judge in research decisions, and substantially redacted the paper. ",
        "bbox": [
            112,
            358,
            880,
            387
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "MB optimized model source code, contributed framework source code, and performed experiments. ",
        "bbox": [
            114,
            393,
            767,
            409
        ],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "bbox": [
            116,
            89,
            209,
            106
        ],
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "I. Abraham, D. Delling, A. V. Goldberg, and R. F. Werneck. A hub-based labeling algorithm for shortest paths in road networks. In P. M. Pardalos and S. Rebennack, editors, Experimental Algorithms, pages 230–241, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. ISBN 978-3-642-20662-7.   \nD. Achlioptas and F. Mcsherry. Fast computation of low-rank matrix approximations. J. ACM, 54(2):9–es, Apr. 2007. ISSN 0004-5411. URL https://doi.org/10.1145/1219092.1219097.   \nD. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in networks of passively mobile finite-state sensors. Distributed Comput., 18(4):235–253, 2006. URL https://doi.org/10.1007/ s00446-005-0138-3.   \nJ. Aspnes and E. Ruppert. An Introduction to Population Protocols, pages 97–120. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009. ISBN 978-3-540-89707-1. URL https://doi.org/10.1007/978-3-540-89707-1_5.   \nJ. Ba, G. E. Hinton, V. Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016a.   \nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization, 2016b. URL https://arxiv.org/abs/1607.06450.   \nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473.   \nH. B. Barlow. Single units and sensation: A neuron doctrine for perceptual psychology? Perception, 1(4):371–394, 1972. URL https://doi.org/10.1068/p010371. PMID: 4377168.   \nL. Becchetti, V. Bonifaci, and E. Natale. Pooling or sampling: Collective dynamics for electrical flow estimation. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, page 1576–1584, Richland, SC, 2018.   \nM. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. Advances in Neural Information Processing Systems, 37:107547–107603, 2024.   \nA. Ben-Kish, I. Zimerman, M. J. Mirza, J. Glass, L. Karlinsky, and R. Giryes. Overflow prevention enhances longcontext recurrent llms, 2025. URL https://arxiv.org/abs/2505.07793.   \nA. Björner, L. Lovász, and P. W. Shor. Chip-firing games on graphs. European Journal of Combinatorics, 12(4): 283–291, 1991. ISSN 0195-6698. URL https://doi.org/10.1016/S0195-6698(13)80111-4.   \nV. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008, oct 2008. URL https://dx.doi. org/10.1088/1742-5468/2008/10/P10008.   \nL. Boczkowski, A. Korman, and E. Natale. Minimizing message size in stochastic communication patterns: fast selfstabilizing protocols with 3 bits. Distributed Comput., 32(3):173–191, 2019. URL https://doi.org/10.1007/ s00446-018-0330-x.   \nN. Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Inc., USA, 1st edition, 2014. ISBN 0199678111.   \nN. Brunel. Hebbian learning of context in recurrent neural networks. Neural Computation, 8(8):1677–1710, 1996. URL https://ieeexplore.ieee.org/document/6796169.   \nJ. Buckman, C. Gelada, and S. Zhang. Symmetric Power Transformers, 2024. URL https://manifestai.com/ articles/symmetric-power-transformers/.   \nS. Budzinskiy. When big data actually are low-rank, or entrywise approximation of certain function-generated matrices, 2025. URL https://arxiv.org/abs/2407.03250.   \nH. Cairns. Some halting problems for abelian sandpiles are undecidable in dimension three. SIAM Journal on Discrete Mathematics, 32(4):2636–2666, 2018. URL https://doi.org/10.1137/16M1091964.   \nY. Chen, D. Doty, and Soloveichik. Deterministic function computation with chemical reaction networks. Nat Comput, 13:517–534, 2014. URL https://link.springer.com/article/10.1007/s11047-013-9393-6.   \nK. Cho and Y. Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning, 2014. URL https://arxiv.org/abs/1406.7362.   \nK. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH.   \nP. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.-H. Teng. Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs. In Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing, STOC ’11, page 273–282, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450306911. URL https://doi.org/10.1145/1993636.1993674.   \nJ. Czyzowicz, L. Gasieniec, A. Kosowski, E. Kranakis, P. G. Spirakis, and P. Uznanski. On convergence and threshold ´ properties of discrete lotka-volterra population protocols. J. Comput. Syst. Sci., 130:1–25, 2022. URL https: //doi.org/10.1016/j.jcss.2022.06.002.   \nM. Dabagia, C. H. Papadimitriou, and S. S. Vempala. Computation with sequences of assemblies in a model of the brain. Neural Computation, 37(1):193–233, 12 2024. ISSN 0899-7667. URL https://doi.org/10.1162/neco_ a_01720.   \nZ. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978–2988. Association for Computational Linguistics, 2019. doi: 10.18653/ V1/P19-1285. URL https://doi.org/10.18653/v1/p19-1285.   \nM. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019. URL https: //arxiv.org/abs/1807.03819.   \nS. Dolev. Self-stabilization. The MIT Press, 2000.   \nD. Doty, M. Eftekhari, L. Gasieniec, E. E. Severson, P. Uznanski, and G. Stachowiak. A time and space optimal stable ´ population protocol solving exact majority. In 62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 1044–1055. IEEE, 2021. URL https://doi.org/ 10.1109/FOCS52979.2021.00104.   \nB. Dudek and A. Kosowski. Universal protocols for information dissemination using emergent signals. In I. Diakonikolas, D. Kempe, and M. Henzinger, editors, Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, pages 87–99. ACM, 2018. URL https://doi.org/10.1145/3188745.3188818.   \nL. Emberson, B. Cottier, J. You, T. Adamczewski, and J.-S. Denain. LLM responses to benchmark questions are getting longer over time, 2025. URL https://epoch.ai/data-insights/output-length. Accessed: 2025-07-25.   \nM. Feinberg. Foundations of Chemical Reaction Network Theory, volume 202 of Applied Mathematical Sciences. Springer, 2019.   \nP. Fraigniaud and G. Giakkoupis. On the searchability of small-world networks with arbitrary underlying structure. In Proceedings of the Forty-Second ACM Symposium on Theory of Computing, STOC ’10, page 389–398, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450300506. doi: 10.1145/1806689.1806744. URL https://doi.org/10.1145/1806689.1806744.   \nA. Graves. Adaptive computation time for recurrent neural networks, 2017. URL https://arxiv.org/abs/1603. 08983.   \nA. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv. org/abs/2312.00752.   \nD. Haziza, T. Chou, D. Choudhary, L. Wehrstedt, F. Massa, J. Yu, G. Jeong, S. Rao, P. Labatut, and J. Cai. Accelerating transformer inference and training with 2:4 activation sparsity, 2025. URL https://arxiv.org/abs/2503. 16672.   \nB. J. He, J. M. Zempel, A. Z. Snyder, and M. E. Raichle. The temporal structures and functional significance of scale-free brain activity. Neuron, 66(3):353–369, 2010. ISSN 0896-6273. URL https://doi.org/10.1016/j. neuron.2010.04.020.   \nD. O. Hebb. Organization of behavior. New York: Wiley & Sons, 1949.   \nV. Herrmann, R. Csordás, and J. Schmidhuber. Measuring in-context computation complexity via hidden state prediction, 2025. URL https://arxiv.org/abs/2503.13431.   \nD. Hilbert. Mathematical problems. Bull. AMS, 8(10):437–479, 1902. ISSN 0002-9904 (print), 1936-881X (electronic). URL https://doi.org/10.1090/S0002-9904-1902-00923-3. English translation of Hilbert’s famous list of 23 important problems in mathematics for the 20th Century.   \nG. E. Hinton. What kind of a graphical model is the brain? In Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI’05, page 1765–1775, San Francisco, CA, USA, 2005. Morgan Kaufmann Publishers Inc.   \nG. E. Hinton and D. C. Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pages 177–186, 1987.   \nJ. Hirvonen and J. Suomela. Distributed Algorithms 2020 — the book. Aalto University, 2025. URL https:// jukkasuomela.fi/da2020/da2020.pdf.   \nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, Nov. 1997. ISSN 0899-7667. URL https://doi.org/10.1162/neco.1997.9.8.1735.   \nJ. Hofbauer and K. Sigmund. Evolutionary Games and Population Dynamics. Cambridge University Press, 1998. URL https://www.cambridge.org/core/books/evolutionary-games-and-population-dynamics/ A8D94EBE6A16837E7CB3CED24E1948F8.   \nP. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5 (2):109–137, 1983. ISSN 0378-8733. URL https://www.sciencedirect.com/science/article/pii/ 0378873383900217.   \nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685.   \nX. Huang, A. Yang, S. Bhattamishra, Y. Sarrof, A. Krebs, H. Zhou, P. Nakkiran, and M. Hahn. A formal framework for understanding length generalization in transformers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id $\\cdot ^ { = }$ U49N5V51rU.   \nF. Jabr and A. Rothschild. How brainless slime molds redefine intelligence. Nature, 7(1), 2012.   \nA. Jojic, Z. Wang, and N. Jojic. Gpt is becoming a turing machine: Here are some ways to program it. arXiv preprint arXiv:2303.14310, 2023.   \nA. Kalev and I. Hen. Feynman path integrals for discrete-variable systems: Walks on hamiltonian graphs. Phys. Rev. Res., 7:013220, Feb 2025. URL https://link.aps.org/doi/10.1103/PhysRevResearch.7.013220.   \nA. Karpathy. nanoGPT. https://github.com/karpathy/nanoGPT, 2024.   \nB. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Phys. Rev. E, 83: 016107, Jan 2011. URL https://link.aps.org/doi/10.1103/PhysRevE.83.016107.   \nA. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5156–5165. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html.   \nD. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In KDD ’03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 137–146, New York, NY, USA, 2003. ACM Press. ISBN 1-58113-737-0. URL https://doi.acm.org/10. 1145/956750.956769.   \nN. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id $\\equiv$ rkgNKkHtvB.   \nP. Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pages 79–86, Phuket, Thailand, Sept. 13-15 2005. URL https://aclanthology.org/2005. mtsummit-papers.11/.   \nA. Kosowski and P. Uznanski. Population protocols are fast. In C. Newport and I. Keidar, editors, ´ Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing, PODC 2018, Egham, United Kingdom, July 23-27, 2018, pages 475–477. ACM, 2018. URL https://arxiv.org/abs/1802.06872.   \nR. Krauthgamer and S. Sapir. Comparison of matrix norm sparsification. Algorithmica, 85(12):3957–3972, 2023. URL https://doi.org/10.1007/s00453-023-01172-6.   \nA. Kumar, L. Owen, N. R. Chowdhury, and F. Güra. ZClip: Adaptive spike mitigation for LLM pre-training, 2025. URL https://arxiv.org/abs/2504.02507.   \nY. LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1–62, 2022.   \nY. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.   \nJ. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.   \nA. C. Lin, A. M. Bygrave, A. De Calignon, T. Lee, and G. Miesenböck. Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination. Nature neuroscience, 17(4):559–568, 2014.   \nH. Lin and S. Jegelka. Resnet with one-neuron hidden layers is a universal approximator. Advances in neural information processing systems, 31, 2018.   \nK. Liu, J. Gao, and K. Chen. Scaling up the state size of RNN LLMs for long-context scenarios. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11516–11529, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https://aclanthology.org/2025.acl-long.564/.   \nI. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/abs/1711. 05101.   \nV. Mante, D. Sussillo, K. V. Shenoy, and W. T. Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. nature, 503(7474):78–84, 2013.   \nL. Massoulié. Community detection thresholds and the weak ramanujan property. In Proceedings of the FortySixth Annual ACM Symposium on Theory of Computing, STOC ’14, page 694–703, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450327107. URL https://doi.org/10.1145/2591796. 2591857.   \nW. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, 5(4):115–133, 1943.   \nF. McSherry, D. G. Murray, R. Isaacs, and M. Isard. Differential dataflow. In Sixth Biennial Conference on Innovative Data Systems Research, CIDR 2013, Asilomar, CA, USA, January 6-9, 2013, Online Proceedings. www.cidrdb.org, 2013. URL https://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf.   \nW. Merrill and A. Sabharwal. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NjNGlPh8Wh.   \nT. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.   \nD. Mitropolsky and C. H. Papadimitriou. Simulated language acquisition in a biologically realistic model of the brain. bioRxiv, 2025. doi: 10.1101/2025.07.15.664996. URL https://www.biorxiv.org/content/early/2025/ 07/19/2025.07.15.664996.   \nY. Mohsenzadeh, C. Mullin, B. Lahner, and A. Oliva. Emergence of visual center-periphery spatial organization in deep convolutional neural networks. Scientific Reports, 10(1):4638, 2020. URL https://doi.org/10.1038/ s41598-020-61409-0.   \nV. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814, 2010.   \nR. M. Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.   \nJ. v. Neumann. The computer and the brain. Yale University Press, USA, 1958. ISBN 0300007930.   \nM. E. J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):8577–8582, 2006. doi: 10.1073/pnas.0601602103. URL https://www.pnas.org/doi/abs/10.1073/ pnas.0601602103.   \nB. A. Olshausen. (ed.), The Brain and Computation — Simons Institute Program, Berkeley, 2018. URL https: //simons.berkeley.edu/programs/brain-computation.   \nB. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23):3311–3325, 1997. ISSN 0042-6989. URL https://doi.org/10.1016/S0042-6989(97) 00169-7.   \nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/ in-context-learning-and-induction-heads/index.html.   \nS. Ostmeier, B. Axelrod, M. Varma, M. E. Moseley, A. Chaudhari, and C. Langlotz. LieRE: Lie rotational positional encodings, 2025. URL https://arxiv.org/abs/2406.10322.   \nC. H. Papadimitriou, S. S. Vempala, D. Mitropolsky, M. Collins, and W. Maass. Brain computation by assemblies of neurons. Proceedings of the National Academy of Sciences, 117(25):14464–14472, 2020. URL https://www. pnas.org/doi/abs/10.1073/pnas.2001893117.   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \nD. Peleg. Distributed Computing: A Locality-Sensitive Approach. Society for Industrial and Applied Mathematics, 2000. URL https://epubs.siam.org/doi/abs/10.1137/1.9780898719772.   \nJ. Pérez, P. Barceló, and J. Marinkovic. Attention is turing complete. J. Mach. Learn. Res., 22(1), Jan. 2021. ISSN 1532-4435.   \nO. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. URL https://arxiv.org/abs/2108.12409.   \nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nL. T. Rolla. Activated random walks on $\\mathbb { Z } ^ { d }$ . Probability Surveys, 17, Jan. 2020. ISSN 1549-5787. URL https: //dx.doi.org/10.1214/19-PS339.   \nRomanid. Wikipedia, the free encyclopedia, 2025. URL https://en.wikipedia.org/w/index.php?title $=$ Romanid&oldid $\\equiv$ 1275565870. [Online; accessed 24-July-2025].   \nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323 (6088):533–536, 1986.   \nT. K. Rusch and D. Rus. Oscillatory state-space models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id $\\cdot ^ { = }$ GRMfXcAAFh.   \nS. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. Advances in neural information processing systems, 30, 2017.   \nJ. Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets. In International Conference on Artificial Neural Networks, pages 460–463. Springer, 1993.   \nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. URL https://arxiv.org/abs/1701.06538.   \nZ. Shen, H. Yang, and S. Zhang. Optimal approximation rate of ReLU networks in terms of width and depth. Journal de Mathématiques Pures et Appliquées, 157:101–135, 2022. ISSN 0021-7824. doi: https://doi.org/10.1016/j.matpur. 2021.07.009. URL https://www.sciencedirect.com/science/article/pii/S0021782421001124.   \nP. Shojaee, I. Mirzadeh, K. Alizadeh, M. Horton, S. Bengio, and M. Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. URL https://arxiv. org/abs/2506.06941.   \nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.   \nJ. Su, M. H. M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. URL https://doi. org/10.1016/j.neucom.2023.127063.   \nY. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive Network: A successor to Transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621.   \nA. M. Turing. Computing machinery and intelligence. Mind, 59(236):433–460, 1950. ISSN 00264423. URL https://www.jstor.org/stable/2251299.   \nM. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM J. Math. Data Sci., 1(1): 144–160, 2019. doi: 10.1137/18M1183480. URL https://doi.org/10.1137/18M1183480.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088.   \nG. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11080–11090. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/weiss21a.html.   \nJ. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. Behrens. The Tolman-Eichenbaum Machine: Unifying space and relational memory through generalization in the hippocampal formation. Cell, 183 (5):1249–1263.e23, 2020. ISSN 0092-8674. URL https://doi.org/10.1016/j.cell.2020.10.024.   \nJ. C. R. Whittington, J. Warren, and T. E. Behrens. Relating transformers to models and neural representations of the hippocampal formation. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id $\\equiv$ B8DVo9B1YE0.   \nR. J. Williams and J. Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural computation, 2(4):490–501, 1990.   \nA. Yang and D. Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers, 2024. URL https://arxiv.org/abs/2404.04393.   \nG. Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. Advances in Neural Information Processing Systems, 32, 2019.   \nZ. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy. Hierarchical attention networks for document classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1480–1489, 2016.   \nC. You, K. Wu, Z. Jia, L. Chen, S. Bhojanapalli, J. Guo, U. Evci, J. Wassenberg, P. Netrapalli, J. J. Willcock, S. Subramanian, F. Chern, A. Andreev, S. Pathak, F. Yu, P. Jain, D. E. Culler, H. M. Levy, and S. Kumar. Spark transformer: Reactivating sparsity in FFN and attention, 2025. URL https://arxiv.org/abs/2506.06644.   \nH. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. M. Susskind, S. Bengio, and P. Nakkiran. What algorithms can transformers learn? A study in length generalization. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=AssIuHnmHX.   \nM. Zou. Aspects of Efficiency in Selected Problems of Computation on Large Graphs. PhD thesis, Paris Diderot University, France, 2019. URL https://tel.archives-ouvertes.fr/tel-02436610. ",
        "bbox": [
            112,
            116,
            885,
            915
        ],
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            107,
            46,
            887,
            920
        ],
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            107,
            37,
            888,
            915
        ],
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            75,
            885,
            916
        ],
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            68,
            887,
            920
        ],
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            111,
            89,
            887,
            468
        ],
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "A Connection between generalization of reasoning and computational expressiveness ",
        "text_level": 1,
        "bbox": [
            116,
            89,
            836,
            107
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "State-of-the-art reasoning models have the interpretation of (Turing-complete) programs, executed over a certain period of time. This shifts the emphasis of generalization, from discovering the structure of mathematical functions which maps inputs to outputs, to discovering a class of runnable programs, which take as input a given class of input prompts, and process these prompts “in the right direction”. ",
        "bbox": [
            116,
            121,
            882,
            178
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Consider a given reasoning task, whose scope is defined as a set $\\mathcal { P }$ of valid input prompts, given as bounded-length token sequences over some alphabet $\\Omega$ . Given a prompt from $\\mathcal { P }$ , a model solving the considered task is eventually (i.e, after some number of steps of reasoning) expected to generate an output, in the form of a bounded-length token sequence over the same alphabet $\\Omega$ , which is subjected to evaluation. Consider language models sampled from some probability distribution $\\mathcal { M } _ { 1 }$ over parameter sets in some architecture $\\mathcal { A } _ { 1 }$ . ",
        "bbox": [
            114,
            183,
            882,
            253
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Now, suppose that for some other model architecture $\\boldsymbol { A } _ { 2 }$ there exists a distribution $\\mathcal { M } _ { 2 }$ over language models in $\\boldsymbol { A } _ { 2 }$ such that, for a valid input prompt chosen uniformly at random from $\\mathcal { P }$ , the outputs sampled from a model $M _ { 1 } \\sim \\mathcal { M } _ { 1 }$ and the outputs sampled from a model $M _ { 2 } \\sim \\mathcal { M } _ { 2 }$ , have (almost) the same distribution in the space of boundedlength sequences over $\\Omega$ , and are both obtained within some asymptotic bound on the number of steps of reasoning, in expectation. The described setting is equivalent to saying that models $\\mathcal { M } _ { 2 }$ have generalized the considered task $\\mathcal { P }$ in (almost) the same way as models $\\mathcal { M } _ { 1 }$ . Indeed, conversely, if the described condition did not hold, we could, in a finite number of trials, distinguish solutions to problem $\\mathcal { P }$ obtained by model families $\\mathcal { M } _ { 1 }$ and $\\mathcal { M } _ { 2 }$ . ",
        "bbox": [
            116,
            260,
            882,
            357
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Now, consider model architectures $A _ { 1 } , A _ { 2 }$ which apply Chain-of-Thought reasoning (Wei et al., 2022). A model in such an architecture has the interpretation of a trainable probabilistic program, taking inputs from $\\mathcal { P }$ , and the architectures themselves represent computational machine architectures. Moving to a discussion of computational expressiveness, we obtain the following statement. ",
        "bbox": [
            116,
            363,
            882,
            417
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Observation 9. Given a probability distribution of models $\\mathcal { M } _ { 1 }$ in architecture $\\boldsymbol { A } _ { 1 }$ , suppose there exists a distribution over models in architecture $\\boldsymbol { A } _ { 2 }$ which generalizes on task $\\mathcal { P }$ in the same way as models from $\\mathcal { M } _ { 1 }$ . Then, the machine architecture $\\boldsymbol { A } _ { 2 }$ has sufficient computational expressiveness to simulate programs from $\\mathcal { M } _ { 1 }$ efficiently on the set of inputs $\\mathcal { P }$ , i.e., $\\boldsymbol { A } _ { 2 }$ contains programs which obtain an (almost) identical distribution of outputs within the given bounds on running time. □ ",
        "bbox": [
            114,
            424,
            882,
            493
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "In particular, we note that if we were to consider the special case of $\\boldsymbol { A } _ { 1 }$ being reasonable human agents, we could say that architecture $\\boldsymbol { A } _ { 2 }$ generalizes reasoning, in the same way as humans, if we can train models $\\mathcal { M } _ { 2 }$ in $\\boldsymbol { A } _ { 2 }$ which accurately reproduce the outcomes of reasoning for some sample $\\mathcal { M } _ { 1 }$ of humans in $\\mathcal { A } _ { 1 }$ . ",
        "bbox": [
            116,
            506,
            883,
            549
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "This leads us naturally to describe Language Model generalization through a universal reference to the principles of operation of the human brain, treated as a distributed computing architecture, and not through a characterization of language and reasoning prompts $\\mathcal { P }$ that the model should be able to deal with in some specific way. ",
        "bbox": [
            117,
            554,
            883,
            597
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "B Further description of experiments ",
        "text_level": 1,
        "bbox": [
            116,
            617,
            444,
            635
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "B.1 Language translation task ",
        "text_level": 1,
        "bbox": [
            117,
            651,
            339,
            665
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "We have evaluated our models on a mixed language modeling and translation task derived from the Europarl corpus (Koehn, 2005). The corpus consists of sentence-level aligned translations of transcripts of European Parliament proceedings. For each language pair, we treat the data as a long stream of interleaved source and target sentences (sampling for each sentence which language is the source, and which is the target) on which we train decoder only models. Thus, models are jointly trained as language models and translators. We train all models using Truncated Backpropagation Through Time (Williams and Peng, 1990). Subsequent minibatches served by the data loader are related: each is a continuation of the previous. Each model maintains a recurrent state, carried across minibatches: $\\rho$ matrix for BDH-GPU and a FIFO buffer of recent KV-cache entries for the TransformerXL (Dai et al., 2019) baseline. We train all models on raw UTF8 data. We are mainly interested in model comparison and prefer to keep the experimental setup as simple as possible. A few minibatches are shown in Fig. 16. ",
        "bbox": [
            114,
            676,
            883,
            815
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "The joint language modeling and translation formulation has several benefits: ",
        "bbox": [
            117,
            821,
            620,
            835
        ],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "1. Next token prediction is representative for LLM training. Simple architectures, such as decoder-only models are sufficient.   \n2. The task promotes models with long context capabilities — subsequent sentences are related and the model can meaningfully utilize long context to model the source language sentences. ",
        "bbox": [
            145,
            848,
            883,
            911
        ],
        "page_idx": 52
    },
    {
        "type": "table",
        "img_path": "images/e52836e8bbfe0225c7f6add0cc18649120085b445f10de55981086f306df129e.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>model size</td><td>num layer</td><td>embd dim</td><td>num head</td><td>MLP dim</td><td>dropout</td><td>Carried KV-cache size</td></tr><tr><td>25M</td><td>9</td><td>480</td><td>5</td><td>1920</td><td>0.01</td><td>4096</td></tr><tr><td>50M</td><td>12</td><td>576</td><td>6</td><td>2304</td><td>0.02</td><td>4096</td></tr><tr><td>100M</td><td>15</td><td>768</td><td>8</td><td>3072</td><td>0.02</td><td>4096</td></tr><tr><td>200M</td><td>18</td><td>960</td><td>10</td><td>3840</td><td>0.002</td><td>4096</td></tr><tr><td>400M</td><td>25</td><td>1152</td><td>12</td><td>4608</td><td>0.005</td><td>4096</td></tr><tr><td>800M</td><td>28</td><td>1536</td><td>16</td><td>6144</td><td>0.15</td><td>4096</td></tr></table>",
        "bbox": [
            256,
            88,
            740,
            215
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "Table 4: Hyperparameters for GPTXL baselines in scaling experiments. The model architecture follows GPT2 (Radford et al., 2019), with a FIFO buffer of past KV-cache entries (Dai et al., 2019). ",
        "bbox": [
            114,
            231,
            882,
            257
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "3. The task promotes models which carry state across minibatches, as training data is temporally coherent and the final model state at the end of one minibatch is a natural initialization of hidden state on the next minibatch. ",
        "bbox": [
            150,
            296,
            883,
            325
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "4. Translation can be seen as language modeling coupled with fuzzy copying. Successful models will need to develop in-context learning capabilities such as inductive heads (Olsson et al., 2022). ",
        "bbox": [
            150,
            334,
            882,
            363
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "0. |<F:en>For countries such as Sweden and Finland, another system o|   \n1. |f allocation would be extremely significant.<T:es>Por ejemplo, p|   \n2. |ara pa•íses como Suecia y Finlandia tendr•ía un gran significado|   \n3. | que se hiciese otra forma de distribuci•ón.<F:es>El diputado Fe|   \n4. |rber ha presentado una propuesta que implica una distribuci•ón m|   \n5. |•ás flexible, y yo respaldo esta enmienda.<T:en>Mr Ferber has ta|   \n6. |bled an amendment which involves our looking in a considerably m|   \n7. |ore flexible way at the present allocation, and I support this a|   \n8. |mendment.<F:en>.<T:es>.<F:en>(NL) Mr President, I would like to |   \n9. |start by thanking both parliamentary committees and not least bo| ",
        "bbox": [
            181,
            387,
            772,
            526
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "Figure 16: Exemplary sequence of 10 successive minibatches from the translation task. The model is trained on raw UTF8 bytes (for visualization we pad multi-byte UTF8 characters with “•” symbol). Special token strings <F:lang_code> and <T:lang_code> delimit source and target sentences. Minibatches are temporally coherent: source sentences are followed by their translations, and subsequent source sentences are part of the same larger document. ",
        "bbox": [
            114,
            549,
            883,
            601
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "B.2 BDH Scaling Experimental Details ",
        "text_level": 1,
        "bbox": [
            117,
            635,
            397,
            650
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "We provide details on models used in scaling experiments described in Section 4.2. All models were implemented in PyTorch (Paszke et al., 2019) and trained on the Europarl (Koehn, 2005) task described in Section B.1. We have kept the same training regime for all models at all sizes: En-PL and En-Cs language pairs (380MB total). All models trained on raw UTF8 bytes seeing a total of 1.2B tokens (about 3 epochs). All minibatches were 2048 tokens long, but we have varied the number of examples in the minibatch (varying number of tokens in each minibatch) to accommodate different memory requirements of different models. We have used multi-GPU training using the Distributed Data Parallel approach using AdamW (Loshchilov and Hutter, 2019) with learning rate $1 0 ^ { - 3 }$ , and 1000 warm-up step followed by linear learning rate decay over the course of training to $1 0 ^ { - 4 }$ , adaptive gradient clipping (Kumar et al., 2025), and weight decay 0.1. Models were trained to operate on a context longer than minibatch length using Truncated Backpropagation Through time (Williams and Peng, 1990). ",
        "bbox": [
            114,
            662,
            882,
            801
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "The Baseline model, dubbed GPTXL, was a GPT2-like transformer (Radford et al., 2019) based off the NanoGPT (Karpathy, 2024) implementation with KV-cache carried across minibatches as in TransformerXL (Dai et al., 2019). We have used ALiBi positional biases Press et al. (2022). We list its hyperparameters for various model sizes in Table 4. Optimal Dropout was selected using a small sweep at each model size. ",
        "bbox": [
            116,
            806,
            882,
            863
        ],
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "BDH-GPU directly uses model code provided in Appendix E. BDH-GPU’ adds xLSTM-like gating mechanism (Beck et al., 2024), and merges next token predictions from all layers. Both BDH-GPU and BDH-GPU’ use same architectural hyperparameters, gathered in Table 5. ",
        "bbox": [
            116,
            868,
            882,
            911
        ],
        "page_idx": 53
    },
    {
        "type": "table",
        "img_path": "images/105f53668a77397699a4350d8efe1ec6f44aa49b695c04fb81b8e247519f1b9c.jpg",
        "table_caption": [
            "Table 5: Hyperparameters for BDH-GPU models in scaling experiments. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>model size</td><td>num layer</td><td>d</td><td>n</td><td>num head</td><td>dropout</td></tr><tr><td>25M</td><td>8</td><td>256</td><td>32768</td><td>4</td><td>0.1</td></tr><tr><td>50M</td><td>8</td><td>256</td><td>65536</td><td>4</td><td>0.1</td></tr><tr><td>100M</td><td>8</td><td>256</td><td>131072</td><td>4</td><td>0.1</td></tr><tr><td>200M</td><td>8</td><td>256</td><td>262144</td><td>4</td><td>0.1</td></tr><tr><td>400M</td><td>8</td><td>256</td><td>524288</td><td>4</td><td>0.1</td></tr><tr><td>800M</td><td>8</td><td>256</td><td>1048576</td><td>4</td><td>0.1</td></tr></table>",
        "bbox": [
            320,
            88,
            678,
            215
        ],
        "page_idx": 54
    },
    {
        "type": "table",
        "img_path": "images/417d4ed1d5e35efdb1fa5599d83df65bb9793ac6a0486215e13f8be6e71a322f.jpg",
        "table_caption": [
            "Table 6: Architecture and training details for model merging experiments. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Init. from</td><td>Training data</td><td>Data size (bytes)</td><td>Training tokens</td><td>n</td><td>d</td><td>num. heads</td><td>num. layers</td><td>param. count</td></tr><tr><td>BaseEnEs</td><td></td><td>En-Es</td><td>612M</td><td>1.2B</td><td>24576</td><td>256</td><td>4</td><td>8</td><td>19M</td></tr><tr><td>TunedEnFr</td><td>BaseEnEs</td><td>En-Fr</td><td>640M</td><td>1.2B</td><td>24576</td><td>256</td><td>4</td><td>8</td><td>19M</td></tr><tr><td>TunedEnPt</td><td>BaseEnEs</td><td>En-Pt</td><td>616M</td><td>1.2B</td><td>24576</td><td>256</td><td>4</td><td>8</td><td>19M</td></tr><tr><td>MergedEnEsFrPt</td><td>TunedEnFr+TunedEnPt</td><td>丨</td><td>丨</td><td>丨</td><td>49152</td><td>256</td><td>4</td><td>8</td><td>38M</td></tr></table>",
        "bbox": [
            181,
            271,
            816,
            349
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "B.3 BDH Monosemantic Synapse Experiment Details ",
        "text_level": 1,
        "bbox": [
            114,
            417,
            498,
            433
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "We provide details for models used in exploration of monosemantic synapses in Section 6.2. The model was trained on Europarl (Koehn, 2005) described in Section B.1. It had $d = 2 5 6 , n = 4 9 1 5 2 , 4$ attention heads, and 8 layers. The model was trained on about one epoch of En-Es, En-Pt, and En-Fr data (total 1.9B tokens) in a Distributed Data Parallel setup using AdamW (Loshchilov and Hutter, 2019) with learning rate $1 0 ^ { - 3 }$ , 1000 warm-up step followed by linear learning rate decay over the course of training to $1 0 ^ { - 4 }$ , adaptive gradient clipping (Kumar et al., 2025), and weight decay 0.1. We have used Truncated Backpropagation Through time, carrying over the recurrent state of attention and training on sequences of length 2048 characters at a time. We have used minimal Dropout (Srivastava et al., 2014) of 0.01. ",
        "bbox": [
            116,
            444,
            883,
            556
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "B.4 BDH Merging Experiment Details ",
        "text_level": 1,
        "bbox": [
            117,
            577,
            393,
            592
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "We provide details for models described in Section 7.1 All models were trained on Europarl (Koehn, 2005) described in Section B.1. We provide model architecture hyperparametrs in Table 6. Models were trained on about two passes over the training set in a Distributed Data Parallel setup using AdamW (Loshchilov and Hutter, 2019) with learning rate $1 0 ^ { - 3 }$ , 1000 warmup step followed by linear learning rate decay over the course of training to $1 0 ^ { - 4 }$ , adaptive gradient clipping (Kumar et al., 2025), and weight decay 0.1. We have used Truncated Backpropagation Through time, carrying over the recurrent state of attention and training on sequences of length 2048 characters at a time. We have used minimal Dropout (Srivastava et al., 2014) of 0.01. ",
        "bbox": [
            116,
            603,
            882,
            702
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "C Omitted formal claims and proofs ",
        "text_level": 1,
        "bbox": [
            116,
            724,
            434,
            742
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "C.1 Proof of Observation 1 ",
        "text_level": 1,
        "bbox": [
            117,
            758,
            315,
            773
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Proof. The equivalence is straightforward to verify, rewriting the linear-algebraic multiplication expressions of Eq. (6) in Einstein summation notation and comparing respective index pairs. At any time, during the execution of rules for layer $l$ , variables $X ( i ) , Y ( i )$ and $\\sigma _ { l } ( i , j )$ in the protocol description, for $i , j \\in \\{ 1 , \\ldots , n \\}$ correspond to the $i$ -th coordinate of vectors $x _ { t , l }$ (based on $x _ { t , l - 1 }$ from the previous round), $y _ { t , l }$ (based on $y _ { t , l - 1 }$ from the previous round), and matrix entry $\\sigma _ { t , l }$ (based on $\\sigma _ { t - 1 , l }$ from the previous token). The auxiliary variable $A ( i )$ corresponds to a similar auxiliary vector $a _ { t , l } : = \\pmb { \\sigma } _ { t - 1 , l } x _ { t , l }$ in an intermediate step of computation of $y _ { t , l }$ from $x _ { t , l }$ . The parameter $u ( i , j ) \\in R ^ { + }$ associated with an element of state follows from the definition of matrix $U$ ; we assume for simplicity that $U$ is diagonal (which corresponds to the case of ALiBi). Finally, in Table 1, the auxiliary node variables $X ^ { \\mathsf { \\bar { e } } } ( i ) , \\bar { X } ^ { \\mathsf { i } } ( i ) , Y ^ { \\mathsf { \\bar { e } } } ( i ) , \\bar { Y } ^ { \\mathsf { i } } ( i )$ are used to handle the thresholding of the inhibitory circuit. □ ",
        "bbox": [
            114,
            786,
            882,
            911
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "C.2 Formal statement of Claim 7 (linear attention) ",
        "text_level": 1,
        "bbox": [
            114,
            90,
            480,
            106
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "We provide the following Claim, expressing the operation of attention under $C$ -non-adversarial key vectors $\\left( k _ { \\tau } \\right)$ , $t = 1 \\ldots t$ , understood in the sense that there exists $C \\in N$ , $0 \\leq C < t - 1$ such that, if considering $\\left( k _ { \\tau } \\right)$ as a sequence of random variables, each $f ( k _ { \\tau } )$ , $\\tau = 1 \\dots t$ , can be considered sampled independently at random in $S ^ { \\nu }$ with respect to all keys sampled previously, except for at most $C$ such keys. We put $C = t - 1$ for adversarial inputs, or if this condition cannot be satisfied at all due to the nature of function $f$ . ",
        "bbox": [
            114,
            116,
            883,
            186
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Claim 8. Let $\\Lambda$ be a space of keys and queries, let $\\phi : \\Lambda \\times \\Lambda \\to [ - 1 , 1 ]$ be an attention affinity function, and let $f : \\Lambda \\to S ^ { \\nu }$ , for some $\\nu = O ( \\mathrm { p o l y } ( n ) )$ , be such that for any $q , k \\in R$ , we have $f ( q ) \\cdot f ( k ) = \\phi ( q , k ) \\pm O ( n ^ { - 1 0 0 } )$ . Fix $\\delta > 0$ and $C \\in \\mathbb { N }$ . Let $A _ { \\phi , t }$ be a block which computes attention $a _ { t }$ given by Eq. (14), for a given sequence of key-query inputs $( k _ { 1 } , \\ldots , k _ { t } )$ and values $( v _ { 1 } , \\ldots , v _ { t } )$ , where $t < \\delta n / ( ( C + 1 ) \\log n )$ is fixed, $k _ { \\tau } \\in \\Lambda$ , and $v _ { \\tau } \\in R ^ { d }$ are of similar strength in the $L 2$ -norm, with $c _ { 1 } \\leq \\| v _ { \\tau } \\| \\leq c _ { 2 }$ , for all $\\tau = 1 \\ldots t ,$ , for some constants $0 < c _ { 1 } \\le c _ { 2 }$ . Then the (simplified) linear attention equation of BDH-GPU: ",
        "bbox": [
            114,
            189,
            883,
            276
        ],
        "page_idx": 55
    },
    {
        "type": "equation",
        "img_path": "images/94b24baac2aa1b4807b8a1d27b7d9d0b30bbdb5c11aadfb80e9b573d2c2bc0f2.jpg",
        "text": "$$\n\\boldsymbol { a } _ { t } ^ { * } : = \\sum _ { \\tau = 1 } ^ { t - 1 } \\boldsymbol { v _ { \\tau } } { \\boldsymbol { x _ { \\tau } } ^ { T } } \\boldsymbol { x _ { t } }\n$$",
        "text_format": "latex",
        "bbox": [
            433,
            281,
            565,
            324
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "expresses $A _ { \\phi , t }$ with $O ( { \\sqrt { \\delta } } )$ -error in the $L 2$ -norm (i.e., $\\| \\boldsymbol { a } _ { \\tau } ^ { * } - \\boldsymbol { a } _ { \\tau } \\| = \\boldsymbol { O } ( \\sqrt { \\delta } )$ , provided that the input vector $\\left( k _ { \\tau } \\right)$ is $C$ -non-adversarial, under a suitable randomly chosen key preparation function $f ^ { \\prime } : \\Lambda \\to R ^ { n }$ , $x _ { \\tau } : = f ^ { \\prime } ( k _ { \\tau } )$ , where $f ^ { \\prime }$ depends on $f$ , w.h.p. in $n$ with respect to choice of $f ^ { \\prime }$ . ",
        "bbox": [
            114,
            330,
            883,
            376
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Proof (sketch). To simplify notation, assume w.l.o.g. that $\\Lambda = S ^ { \\nu }$ and $f = i d e m$ ; to undo this assumption, at the end of the proof we apply $f \\circ f ^ { \\prime }$ for preparation in place of $f ^ { \\prime }$ . ",
        "bbox": [
            120,
            388,
            883,
            420
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "All vectors $v$ and the result $a _ { t }$ we are looking to calculate are in $R ^ { d }$ . With this notation, the attention task we are approximating is: ",
        "bbox": [
            114,
            424,
            883,
            453
        ],
        "page_idx": 55
    },
    {
        "type": "equation",
        "img_path": "images/d6ebae133ccf94adfd0c88a69804015ff93d5f013b1e132e6fd70f602105635f.jpg",
        "text": "$$\na _ { t } = q \\sum _ { \\tau = 1 } ^ { t } k _ { \\tau } ^ { T } v _ { \\tau } .\n$$",
        "text_format": "latex",
        "bbox": [
            441,
            450,
            557,
            493
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "(this is still the general form of attention almost precisely equivalent to (14), not a special case). ",
        "bbox": [
            112,
            496,
            740,
            511
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "The goal is to show how, subject to $t < \\delta n / \\log ^ { 2 } n$ , linear attention in dimension $n$ given by (17) is a sufficiently precise estimation of (18). ",
        "bbox": [
            117,
            515,
            877,
            546
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Consider now, with $\\Lambda = S ^ { \\nu }$ , $f ^ { \\prime } : S ^ { \\nu } \\to R ^ { n }$ , where we recall that $x _ { \\tau } : = f ^ { \\prime } ( k _ { \\tau } )$ , to be a suitable dimensionality reduction preserving approximation of scalar product between $R ^ { \\nu }$ and $R ^ { n }$ . For simplicity of argument, we let $f ^ { \\prime } :$ $R ^ { \\nu }  R ^ { n }$ be a standard Johnson-Lindenstrauss transform, with the additional property that $f ^ { \\prime } ( - \\bar { z } ) = - f ^ { \\prime } ( z )$ for all $z \\in R ^ { \\nu }$ (easy to obtain from any other Johnson-Lindenstrauss transform $f ^ { \\prime \\prime }$ by taking $f ^ { \\prime } ( z ) : = ( f ^ { \\prime \\prime } ( z ) - f ^ { \\prime \\prime } ( - z ) ) / 2 \\space $ . The distortion of scalar product in $R ^ { n }$ is then known to be bounded as follows: √ ${ | k _ { \\tau } } ^ { T } k _ { t } - { x _ { \\tau } } ^ { T } x _ { t } | = O ( \\varepsilon ) ( \\Vert k _ { \\tau } \\Vert +$ $\\| k _ { t } \\| ) = O ( \\varepsilon )$ , w.h.p. with respect to choice of $f ^ { \\prime }$ . Here, $\\varepsilon = { \\sqrt { \\log n / n } } = O ( { \\sqrt { \\delta } } ) / { \\sqrt { ( C + 1 ) t \\log t } }$ , where the last inequality follows from the assumption on $t$ made in the Claim. ",
        "bbox": [
            114,
            550,
            883,
            655
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "We now consider the sequence $r _ { \\tau } : = k _ { \\tau } ^ { \\ T } k _ { t } - { x _ { \\tau } } ^ { T } x _ { t }$ , for $\\tau < t$ . Set aside the (at most $C$ ) elements $r _ { \\tau }$ for which $k _ { \\tau }$ and $k _ { t }$ are not independent. For all other elements, consider that $| r _ { \\tau } | = O ( \\varepsilon )$ as established previously, and the sign $r _ { \\tau } / | r _ { \\tau } |$ is chosen independently at random with respect to all but at least $C$ elements by the conditions imposed on $f ^ { \\prime }$ and $k _ { \\tau }$ . It follows that $\\scriptstyle \\sum _ { \\tau = 1 } ^ { t } r _ { \\tau }$ can be represented as a sum of √ $O ( C )$ martingales, each of which has length $O ( t / ( C + 1 ) )$ and all elements bounded by $O ( \\varepsilon )$ with $\\varepsilon = O ( { \\sqrt { \\delta } } ) / { \\sqrt { ( C + 1 ) t \\log t } }$ . The Claim follows directly, by applying Azuma’s inequality to each of these martingales independently. □ ",
        "bbox": [
            114,
            660,
            883,
            751
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Considering the extreme cases of $C = 0$ and $C = t - 1$ , the above Claim leads directly to Claim 7, clarifying over what time, linear attention can be used to express general attention. ",
        "bbox": [
            119,
            765,
            883,
            795
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "C.3 Proof of Claim 3 ",
        "text_level": 1,
        "bbox": [
            116,
            809,
            274,
            824
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Proof. The proof is almost immediate, through the construction of an appropriate neuron-synapse interaction graphs $H ^ { \\mathfrak { e } } , H ^ { \\mathfrak { i } }$ such that $G ^ { \\mathfrak { c } } = H ^ { \\mathfrak { c } ^ { 2 } } [ V ]$ and $G ^ { \\mathrm { i } } = H ^ { \\mathrm { i } ^ { 2 } } [ V ]$ . Consider $E ^ { \\prime } \\in ( R ^ { + } ) ^ { 2 d \\times n }$ such that $E _ { \\alpha , j } ^ { \\prime } = ( E _ { \\alpha , j } ) ^ { + }$ and $E _ { \\alpha + d , j } ^ { \\prime } = ( - E _ { \\alpha , j } ) ^ { + }$ , for $j \\in \\{ 1 , \\ldots , n \\}$ and $\\alpha \\in \\{ 1 , \\ldots , d \\}$ . Define $D ^ { \\mathfrak { e } } , D ^ { \\mathfrak { i } } \\in ( R ^ { + } ) ^ { n \\times 2 d }$ so that: ",
        "bbox": [
            114,
            834,
            883,
            887
        ],
        "page_idx": 55
    },
    {
        "type": "equation",
        "img_path": "images/9ee296aea3d97f37b71f6024d463e53330bf23aaab48d822bc97fb98fd975d68.jpg",
        "text": "$$\n( D ^ { \\mathfrak { e } } - D ^ { \\mathfrak { i } } ) E ^ { \\prime } = D E .\n$$",
        "text_format": "latex",
        "bbox": [
            426,
            895,
            571,
            912
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Indeed, notice that this is always possible by redistributing elements of $D$ into $D ^ { \\mathfrak { e } }$ and $D ^ { \\mathrm { i } }$ (putting $D _ { i , \\alpha } ^ { \\mathfrak { e } } = D _ { i + d , \\alpha } ^ { \\mathfrak { i } } =$ $( D _ { i , \\alpha } ) ^ { + } )$ and $D _ { i , \\alpha } ^ { \\mathrm { i } } = D _ { i + d , \\alpha } ^ { \\mathfrak { c } } = ( - D _ { i , \\alpha } ) ^ { + } )$ , so that, for all $i , j \\in \\{ 1 , \\ldots , n \\}$ and $\\alpha \\in \\{ 1 , \\ldots , d \\}$ , we have: ",
        "bbox": [
            114,
            90,
            883,
            126
        ],
        "page_idx": 56
    },
    {
        "type": "equation",
        "img_path": "images/a16faa5601eabe85e7b5f119a6654295303d97b59d8838e3b5b839274d1a243c.jpg",
        "text": "$$\n( D _ { i , \\alpha } ^ { \\mathfrak { c } } - D _ { i , \\alpha } ^ { \\mathfrak { i } } ) E _ { \\alpha , j } ^ { \\prime } + ( D _ { i , \\alpha + d } ^ { \\mathfrak { c } } - D _ { i , \\alpha + d } ^ { \\mathfrak { i } } ) E _ { \\alpha + d , j } ^ { \\prime } = D _ { i , \\alpha } E _ { \\alpha , j } .\n$$",
        "text_format": "latex",
        "bbox": [
            290,
            132,
            707,
            151
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Considering $S = \\{ 1 , \\ldots , 2 d \\}$ , the definition of $H ^ { \\mathfrak { e } }$ as the union of edges of $D ^ { \\mathfrak { e } }$ and $E ^ { \\prime }$ on input neuron layer $V$ , hidden layer $S$ , and output neuron layer $V$ follows. Likewise, we define $H ^ { \\mathrm { i } }$ as the union of edges of $D ^ { \\mathrm { i } }$ and $E ^ { \\prime }$ . ",
        "bbox": [
            117,
            156,
            882,
            186
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "We verify that for $G ^ { \\mathfrak { c } } = H ^ { \\mathfrak { c } 2 } [ V ]$ and $G ^ { \\mathrm { i } } = H ^ { \\mathrm { i } ^ { 2 } } [ V ]$ , we have $G ^ { \\mathfrak { c } } - G ^ { \\mathfrak { i } } = D E$ , and the Claim holds. ",
        "bbox": [
            117,
            193,
            763,
            212
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Considerations of building linear circuits. The above proof makes the neuron-synapse interaction graphs $H ^ { \\mathfrak { e } }$ , $H ^ { \\mathrm { i } }$ sparse in terms of the number of edges, as required to show that the number of parameters are preserved by correspondence. However, it is a purely technical construction, and nodes in the synaptic layer have high degree, $n$ . While preserving strict equivalence of linear dynamics, the degrees of nodes of the considered graphs in the synaptic layer can be reduced in this construction, at the cost of increasing the number of edges of graphs $H ^ { \\mathfrak { e } }$ , $H ^ { \\mathrm { i } }$ . (For example, subdividing each node of the synaptic layer into $a ^ { 2 }$ nodes can be used to reduce their degree $\\Theta ( a )$ -times, while increasing the number of edges $\\Theta ( a )$ -times; putting $a = { \\sqrt { n / d } }$ we reach graphs $H ^ { \\mathfrak { e } }$ , $H ^ { \\mathrm { i } }$ with degree $O ( { \\sqrt { n d } } )$ in both the neuron and synaptic layers, and consequently $O ( n { \\sqrt { n d } } )$ edges.) ",
        "bbox": [
            114,
            224,
            883,
            342
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Reduction of internal degrees in this circuit is also possible by introducing more than 1 hidden layer, creating a form of branching circuit. The implementation for this in a distributed way remains very simple, as the considered dynamics of the form $z  G z$ are linear (token-propagation dynamics). The bound on the number of edges needed to represent such a circuit remains $O ( n d )$ , even when the circuit has constant degree. ",
        "bbox": [
            114,
            347,
            882,
            404
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "The technical construction of the linear circuits $H ^ { \\mathfrak { e } }$ , $H ^ { \\mathrm { i } }$ provided in this Appendix do not affect results concerning the analysis of the structure of neuron-neuron interaction graphs $G ^ { \\mathfrak { e } } , G ^ { \\mathfrak { i } }$ . These neuron-neuron interaction graphs plausibly maintain a heavy-tailed, power-law-like degree distribution, as is the case for the models considered empirically in Section 5.5. ",
        "bbox": [
            116,
            410,
            882,
            467
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "C.4 Formal statement of Claim 4 ",
        "text_level": 1,
        "bbox": [
            117,
            482,
            357,
            497
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Claim 9. Let $D _ { y } , E$ be parameter matrices of BDH-Normfree. Then, there exists a graph $G _ { y } \\in \\mathcal { G } ( n , O ( n d ) )$ , expressible through a sparse linear circuit, $a$ graph $G _ { s }$ having $O ( n d )$ edges, and a sparse linear value preparation function $A : R ^ { + ^ { n } }  R ^ { + ^ { n } }$ , such that, for any sequence of keys $( x _ { \\tau , l } ) _ { 0 \\leq \\tau \\leq t }$ and values $( y _ { \\tau , l - 1 } ) _ { 0 \\leq \\tau \\leq t }$ , with $x _ { \\tau , l } , y _ { \\tau , l - 1 } \\in R ^ { + ^ { n } }$ , we have: ",
        "bbox": [
            114,
            507,
            883,
            564
        ],
        "page_idx": 56
    },
    {
        "type": "equation",
        "img_path": "images/c3adc91c3f98a7884f443c39aa0026e155c93592a28a14dff52849abd14a1324.jpg",
        "text": "$$\n( { G _ { y } } ^ { \\circ } - { G _ { y } } ^ { \\circ } ) \\pmb { \\sigma } _ { t - 1 , l } ^ { * } x _ { t , l } = D _ { y } E \\pmb { \\sigma } _ { t - 1 , l } x _ { t , l } ,\n$$",
        "text_format": "latex",
        "bbox": [
            357,
            561,
            637,
            582
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "where $\\begin{array} { r } { \\pmb { \\sigma } _ { t - 1 , l } = \\sum _ { \\tau < t } y _ { \\tau , l - 1 } x _ { \\tau , l } \\ c ^ { T } U ^ { t - \\tau } } \\end{array}$ represents the attention state of BDH-Normfree following Eq. (16), and $\\begin{array} { r } { \\pmb { \\sigma } _ { t - 1 , l } ^ { * } = \\left( \\sum _ { \\tau < t } A ( y _ { \\tau , l - 1 } ) \\pmb { x } _ { \\tau , l } { } ^ { T } U ^ { t - \\tau } \\right) \\odot G _ { s } } \\end{array}$ represents the corresponding attention state of the BDH system with sparse attention on graph $G _ { s }$ , subject to appropriate preparation of attention values using function $f _ { y }$ . ",
        "bbox": [
            114,
            584,
            883,
            632
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Before we start the proof, we make a general point about the formulation of the claim. We are considering the problem of expressing (or more generally, approximating) the matrix operator $\\sigma _ { t - 1 , l }$ by another, sparser one. The setting of our problem can be distilled into obtaining an equality or approximation of the form $E \\pmb { \\sigma } _ { t - 1 , l } \\approx E ^ { * } \\pmb { \\sigma } _ { t - 1 , l } ^ { * }$ , where $E \\in R ^ { d \\times n }$ is a given low-rank matrix, $E ^ { * } \\in R ^ { d \\times n }$ can be defined arbitrarily, and $\\pmb { \\sigma } ^ { * }$ is defined as in the statement of the Claim. If we content ourselves with an approximation, then it is possible to have $\\pmb { \\sigma } ^ { * } = \\pmb { \\sigma }$ (i.e., put $f _ { y } = i d e m )$ , using for example the stochastic sparsification framework of (Achlioptas and Mcsherry, 2007), or a value-dependent variant (cf. e.g. (Krauthgamer and Sapir, 2023)). The samples chosen by such a framework in a value-dependent variant would lead to a graph $G _ { s }$ which plausibly reflects the power-law element distributions that we empirically observe in $\\sigma$ . ",
        "bbox": [
            114,
            640,
            883,
            770
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "While the spirit of such an approximation is generally valid, we opt in the proof for a simpler, purely technical argument applicable to our specific setting, which gives a strict equality in the statement of Claim 9 subject to linear preparation of attention values with a function $A$ . In practice, this would mean that two successive layers of BDH with sparse state are sufficient to express a layer of BDH-Normfree under this reduction. ",
        "bbox": [
            114,
            775,
            882,
            832
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "To prove the claim, it is enough to embed the connection structure of the encoder matrix, treating it as a graph, into $G _ { s }$ . ",
        "bbox": [
            114,
            837,
            879,
            867
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Proof. (of Claim 9) Fix arbitrarily subset $D \\subseteq V$ of neurons, with $| D | = 2 d$ . For the given matrix $E \\in R ^ { d \\times n }$ from BDH-GPU, let $E ^ { \\prime } \\in ( R ^ { + } ) ^ { 2 d \\times n }$ be defined as in the proof of Claim 3 in Appendix C.3, and let $\\boldsymbol { D _ { y } ^ { \\mathrm { ~ \\mathfrak ~ { ~ e ~ } ~ } } } , \\boldsymbol { D _ { y } ^ { \\mathrm { ~ \\mathfrak ~ { i } ~ } } }$ also be applied as in that proof for considerations of decoder $D _ { y }$ . Define the value preparation function $A$ as the immersion of vectors over $V$ into $D$ using $E ^ { \\prime }$ . Define $G _ { s }$ to be the all-ones matrix on the $2 d$ columns corresponding to $D$ , and zeros elsewhere. Then, define $\\mathbf { \\bar { \\boldsymbol { E } } ^ { * } } \\in \\boldsymbol { R } ^ { 2 d \\times n }$ to be a diagonal matrix acting on its first $2 d$ elements (corresponding to $D$ ), and zeros elsewhere. Setting $G _ { y } { } ^ { \\mathfrak { e } } = D _ { y } { } ^ { \\mathfrak { e } } E ^ { * }$ and $G _ { y } ^ { \\mathrm { ~ i ~ } } = D _ { y } ^ { \\mathrm { ~ i ~ } } E ^ { * }$ , we obtain the claim. □ ",
        "bbox": [
            114,
            880,
            882,
            912
        ],
        "page_idx": 56
    },
    {
        "type": "image",
        "img_path": "images/c5b68610f9c811dd56dca2c9039db00c3393a1d33151c1c3988fa3a8dc787b35.jpg",
        "image_caption": [
            "Figure 17: Non-uniform graph attention: interpretation of $E ( \\pmb { \\sigma } _ { l , t } \\odot G _ { s } )$ after sparsification of graph $G _ { s }$ "
        ],
        "image_footnote": [],
        "bbox": [
            303,
            88,
            692,
            285
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            116,
            333,
            882,
            392
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "D Desirable properties of a local graph dynamics for language models ",
        "text_level": 1,
        "bbox": [
            116,
            411,
            712,
            430
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "We outline several general criteria of computational expressiveness and computational efficiency which a distributed computing system has to meet to effectively deal with language and reasoning. For this, we take a first-principles approach, relying only on very fundamental properties which an attention-based language model appears to need to capture, and which are applicable far beyond the specific case of BDH — plausibly, being equally applicable to human and human-like reasoning.20 ",
        "bbox": [
            116,
            444,
            882,
            513
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "Hypothesis 2. We expect any efficient graph-based distributed system dealing with language and reasoning using an attention-based approach to have the following characteristics: ",
        "bbox": [
            119,
            517,
            882,
            546
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• [No Easy Simulation] The system achieves computationally irreducible dynamics, i.e., it provides no systematic opportunity to predict the outcomes of its inference or approximate its dynamics in a numerically easier way than by running the system itself. ",
        "bbox": [
            158,
            558,
            883,
            601
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• [Particles Talk] The state-space dynamics of the distributed system is a non-linear interacting particle dynamics, i.e., the system does not admit an efficient representation as a non-interacting particle system, but relies on a form of non-linear evolution expressed through (at least) two-particle interactions. (Such interactions are necessary, in particular, to enable multi-point correlation analysis on language inputs, when assuming only a small number of inference steps of the system per output token.) ",
        "bbox": [
            160,
            609,
            883,
            679
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• [Attention Deforms Pairwise Connections] The system is capable of computing correlations between pairs of scalar variables localized at different nodes of the distributed system, and storing the state of such correlations so that the result is accessible from these two nodes. (This is plausibly needed to express attention in a state-space system.) ",
        "bbox": [
            160,
            688,
            883,
            744
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• [Time Dictates Structure] The communication graph of the distributed system does not, in itself, represent any specific task input to solve, but reflects a trained model (a program), whereas tasks are represented as inputs to this program, presented over time. The communication graphs used to solve language and reasoning problems are expected to display modular, scale-free structure. ",
        "bbox": [
            160,
            753,
            885,
            809
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "A detailed discussion of the four items of the Hypothesis is provided below. ",
        "bbox": [
            116,
            820,
            604,
            835
        ],
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "[No Easy Simulation] $\\diamond$ Computational models have irreducible dynamics. ",
        "text_level": 1,
        "bbox": [
            117,
            90,
            629,
            106
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "We start by recalling a general observation which is applicable to most learning systems $L$ (machine learning models, biological systems) that have learned how to do computations: they are likely to have chosen state-space dynamics that will allow them to resolve their computational problem with the least effort during inference. In other words, if there is a physical system $P$ that solves a given computational problem, and if there exists a simulation $S ( P )$ of this physical system that would approximate system $P$ with less effort, the learning system $L$ will be following the dynamics of $S ( P )$ , not those of $P$ . ",
        "bbox": [
            116,
            116,
            883,
            199
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "We provide a few hypothetical examples for intuition, anchored in different areas of particle dynamics. ",
        "bbox": [
            119,
            205,
            785,
            220
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "If $P$ were the particle dynamics of electrons in a resistor network, the simulation $S ( P )$ could be a calculation based on Ohm’s law with a Laplacian solver — and we would consequently expect the dynamics of our computational system $L$ to follow the Laplacian solver code, and not to simulate electron dynamics. ",
        "bbox": [
            116,
            226,
            885,
            268
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "If $P$ were the ensemble of billions of Internet users performing short walks clicking through links of the world wide web, the simulation $S ( P )$ would be a calculation of aggregate behavior, reminiscent of PageRank, and we would expect $L$ to encode the parallel dynamics of Map-Reduce matrix operations of PageRank, not the simulation of individual agents. ",
        "bbox": [
            114,
            275,
            882,
            330
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "If $P$ were a quantum system amenable to approximation by perturbation theory, we would expect $L$ to simulate the (classical) calculus of this perturbation theory, and not the quantum system $P$ directly. ",
        "bbox": [
            116,
            337,
            880,
            366
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "Most mechanical systems admit some form of more efficient simulation, which means the the dynamics of such systems are rarely a suitable choice for neuronal models. Anecdotally, in nature, only very simple systems like the Physarum slime mold (Jabr and Rothschild, 2012) rely on direct action (with hydrostatic pressure gradients) to perform their optimization process; and contemporary neuroscience research suggests that even the simplest neuronal brains do not perform their work in a similar “fluid-mechanical” manner. ",
        "bbox": [
            116,
            371,
            882,
            440
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "The irreducibility of $L$ means that this system is stretched to the limits of stability, just as a highly optimized numerical algorithm would be have been simplified and optimized to the limit of numerical stability. This relates to the limits of dimensionality reduction techniques that we have explored through a largely equivalent information-lens perspective of loss of precision and loss of information which it inflicts upon the model. ",
        "bbox": [
            116,
            446,
            882,
            502
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "[Particles Talk] $\\diamond$ Latent concept spaces arise from outcomes of particle-particle interactions. ",
        "text_level": 1,
        "bbox": [
            114,
            522,
            759,
            537
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "Dynamics of systems with multiple particles moving around in a (deformable) environment fall into two broad categories, depending on the strength of interaction between different parts of the dynamics. In the simpler setting, particles can be assumed at short time scales to be moving in an environment unchanged by other particles — the concurrent action of other particles, which would change the environment, does not need to be taken into account when representing individual particle motion, nor is it necessary to consider particle-particle interactions. By contrast, in the more general setting, the dynamics of multiple particles are tightly coupled, and their dynamics need to be modeled (simulated) together. ",
        "bbox": [
            114,
            547,
            882,
            645
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "An example of a dynamics with no coupling would be a dynamics of multiple independent random walkers, such as the previously mentioned dynamics of electricity in wires, or the dynamics of PageRank. Examples of dynamics including interactions between particles, which may either happen directly or be moderated through the environment, include cellular automata, particle method simulations and molecular simulations, or swarms of communicating agents. ",
        "bbox": [
            116,
            651,
            882,
            707
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "The natural representation of state-space models as moving particles comes from the following interpretation. A distributed system with depth- $L$ computations (not least BDH or the BDH-GPU model given by the state equations (4)) is amenable to interpretation as a system of walker particles performing an $L$ -step walk over layers, starting at some token $t _ { 0 }$ in the input layer 0 and, in each time step $t \\geq t _ { 0 }$ , either pausing (skipping a time step) or moving on to the next layer, until they reach the last layer $L$ in some time step $t _ { f }$ , at which point they leave the system, contributing to the distribution of the $t _ { f }$ -th output token. When attempting this approach with independent walkers, the distribution of tokens output by such a system could be described by correlation functions following or resembling the Dyson series, $\\begin{array} { r } { \\sum _ { \\tau _ { L } = 0 } ^ { t } \\sum _ { \\tau _ { L - 1 } = 0 } ^ { \\dot { \\tau _ { L } } - 1 } . . . \\sum _ { \\tau _ { 1 } = 0 } ^ { \\tau _ { 2 } - 1 } F ( \\mathrm { i n p u t } ( \\tau _ { 1 } ) , . . . , \\mathrm { i n p u t } ( \\tau _ { L } ) ) } \\end{array}$ . However, the output of attention (e.g., the linear attention output $a ^ { * }$ given by equation (4) for BDH-GPU, or defined similarly in other state space models based on linear attention), cannot be represented as a Dyson formula when unrolling the dynamics backwards through layers (even if it looks deceptively similar at first glance). Each entry retrieved from attention is an interplay between two moments of time: the moment at which the key-value pair was entered, and the moment at which the corresponding query arrived. In consequence, the considered dynamics can be represented, in each layer, as a linear sum of two-point correlations between current time $t$ and some point $\\tau$ in the past. Thus, in the $l$ -th layer, this recursion can (with some approximation) be unrolled into a linear combination of functions of sets of $2 ^ { l }$ input tokens (provided in the 0-th layer), but cannot be represented through correlation functions $F$ on smaller sets of tokens (e.g., of size linear in $l$ ). Otherwise put, a system like BDH can be described using particles performing $l$ -step walks when relying on intermediate elements of $K V .$ -state $\\sigma$ , which are produced during interactions with other walker particles in intermediate layers, but needs to be viewed through at least $2 ^ { l }$ -point correlation functions defined directly on input tokens in the input layer. ",
        "bbox": [
            114,
            713,
            882,
            911
        ],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            116,
            90,
            882,
            161
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "The considered point is relevant because it precludes many forms of modeling of attention-based language dynamics, in particular those using non-interacting particle theories. The precluded approaches include: ",
        "bbox": [
            116,
            166,
            879,
            195
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "• $L$ -grams, word2vec-like $L$ -skip-grams (Mikolov et al., 2013), as well as any other $L$ -point correlations of past input tokens.   \n• $L$ -step non-interacting random walk models (walks inside the network structure, which move from input layers towards output layers across time).   \n• systems known to be equivalent to the above, such as approximations of classical spin-chain systems by means of Feynman integral path lengths bounded by $L$ (Kalev and Hen, 2025), and many forms of graph/GNN kernels based on $L$ -th powers of the graph Laplacian.   \n• by extension, $L$ -layer state-space systems which perform excessive compression (size reduction) of their state, in a way which eliminates most long-term correlations. ",
        "bbox": [
            160,
            207,
            883,
            353
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "We can ask if this requirement for communication between particles is an artifact of the construction of BDH (and similarly, of the Transformer), or if it comes from a genuine need related to language and reasoning tasks. For language problems per se, the need for multi-point token correlation in $L$ -layer language modeling plausibly follows from the expectation that the model should have the ability to create a syntax tree of a sentence by means of a single quick parallel scan over words in this sentence. With this assumption, the depth $L$ of computation used to build a language syntax tree should be sufficient to represent the number of levels of the syntax tree that the model is able to process naturally, but can be (and in general, should plausibly be) much smaller than the number of leaves (words) of this syntax tree. This is consistent with the RASP-L-based understanding of the Transformer’s capabilities, which allows for expressing depth-L trees in a depth-L Transformer.21 ",
        "bbox": [
            114,
            366,
            882,
            491
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "Such a way of mapping the tree structure of problems into the model’s layers, from bottom to top, also essentially captures the “generative” nature of the considered models, which rely on concept spaces created and stored in state in intermediate layers, to guide both language comprehension and reasoning on language. Thus, the ability to handle language syntax trees efficiently, in itself, precludes the previously-mentioned types of modeling approaches. ",
        "bbox": [
            116,
            497,
            882,
            553
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "[Attention Deforms Pairwise Connections] $\\diamond$ The interaction process $X ( i ) , Y ( j )  \\sigma ( i , j )$ describes attention. ",
        "text_level": 1,
        "bbox": [
            120,
            569,
            872,
            585
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "The preceding discussion in paragraph [Particles Talk] grounds state-of-the-art state-space language models in the world of interacting particle systems. ",
        "bbox": [
            119,
            594,
            877,
            622
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "Whenever the global vector-based description of a state-space model calls for a three-point operation, such as the trilinear operation of key-value-query attention, this translates into the nature of pairwise (for polynomial interaction terms, degree-two) non-linear particle interactions in the transition equations of the same model when described at the level of particles. Notably, at scale, the state-space transition equations of an attention-based model plausibly involve altering or deforming correlation strength between pairs of particles, with such pairs being represented as interaction variables in the state of the system. This requirement on structure, repeated across layers, can be seen as sufficient: interactions of particle pairs are about the only requirement on non-linear rulesets that the system needs to be support, as demonstrated by the simple local transition rules of BDH. ",
        "bbox": [
            114,
            628,
            882,
            739
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "Overall, the statement “attention is all you need”, which describes a system-level global property, translates into $\\cdot X ( i ) , Y ( j ) \\to \\sigma ( i , j )$ is all you need” at the level of particle dynamics of a state-space language model. ",
        "bbox": [
            112,
            746,
            879,
            775
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "[Time Dictates Structure] $\\diamond$ Inputs to reasoning problems are sequential, not graph-based. ",
        "text_level": 1,
        "bbox": [
            114,
            791,
            741,
            806
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "Many real-world graphs are anchored in a spatial embedding of their nodes which is given by external constraints. For example, the structure of many social and transportation networks is impacted by the geographical placement of people and infrastructure on the globe. ",
        "bbox": [
            116,
            815,
            883,
            858
        ],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "In designing the dynamics for BDH, we are free from such spatial constraints. The graph topology corresponding to the model is free to take the shape needed to best resolve the problem. The problem itself is encoded as a sequence of tokens which arrive over time to the model (we take here a state-space view of the system). We can naturally presume that the structure of the model graph of BDH is shaped in a way which follows from two aspects: this temporal encoding of information, and from the abstract (Platonic) latent space of concepts needed to deal with language and reasoning. ",
        "bbox": [
            114,
            90,
            882,
            175
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "When looking for the right particle dynamics for language models, it seems reasonable to discard all unnecessary aspects of spatial constraints. ",
        "bbox": [
            116,
            181,
            880,
            209
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "One example of a particle interaction system which includes externally imposed constraints on the structure of the state space is that of cellular automata operating on a two-dimensional grid. While 2D cellular automata have appealed to public imagination, appearing in attempts to observe the emergence of intelligence at least since the 1970’s, they are, in fact, an extremely cumbersome choice for representing in-context reasoning or language for any attention-based model. State-of-the-art language models seem to have no structural need for a low-dimensional grid in their dynamics. Arguably, the connection structure which needs to emerge in a graph system, allowing it to work efficiently in a setting of efficient information search is precisely the opposite: it is a multi-scale, expander-like system of shortcuts, cf. e.g. (Fraigniaud and Giakkoupis, 2010). This scale-free graph structure is expected to correspond to the scale-free temporal behavior observed in natural systems (He et al., 2010). ",
        "bbox": [
            114,
            215,
            882,
            340
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "In the rest of this paragraph we briefly review other areas of computer science, and how they relate to the particle dynamics we are looking for in terms of their relationship to handling temporal inputs and the constraints they impose on the structure of the state-space. ",
        "bbox": [
            116,
            345,
            882,
            387
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "The freedom of choice of graph topology in solving problems around language and in-context reasoning, which we are dealing with here, can be contrasted with settings in which the graph is, at the same time, part of the system dynamics (encoding interactions in the system) and a part of the statement of the problem input. This is particularly true for models of distributed computing inspired by computer networking (LOCAL, CONGEST, etc.) and other forms of interaction networks (Approximate Message Passing, quantum LOCC, etc.), where the same graph $G$ represents the communication network for the dynamics, and encodes the problem input — with the required output being some function of $G$ (e.g., a clustering, coloring, spanning tree, etc.). Some distributed problems on graphs can be formulated so that the input and required output are independent of the graph structure, the notable ones being: majority consensus, leader election, information broadcasting, and computing aggregates. For such problems, the graph represents only a communication system, whose topology is more an obstacle to overcome, than an actual help in solving the problem. This applies also to architectures in Machine Learning which adhere to a known graph structure, such as Graph Neural Networks or Graph Transformers, when solving problems whose inputs are not naturally embedded in such a structure. ",
        "bbox": [
            114,
            395,
            882,
            560
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "A handful of approaches in distributed computing are intended to describe systems which compute a function of an input signal which, like language, is spread out sequentially over time, and where computations happen while this signal is still arriving. In particular, some forms of particle dynamics can be distilled from the theory of selfstabilizing systems (Dolev, 2000), giving rise to settings where the system is expected to adapt its state in response to a time-changing input (see e.g. (Boczkowski et al., 2019)). Among distributed streaming frameworks, one approach which, owing to its design, admits an elegant particle-based interpretation for time-changing inputs, is the incremental computing framework (McSherry et al., 2013). This framework emphasizes temporal commutativity, and is well suited to expressing dynamics of non-interacting particles, such as PageRank-like computation performed incrementally with Map-Reduce on time-changing graphs, or building nearest-neighbor indexes on sets of changing vectors. It does not naturally extend to the non-linear particle-particle interaction dynamics that appear in the context of attention (see paragraph [Particles Talk]). ",
        "bbox": [
            114,
            565,
            882,
            719
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "E BDH-GPU PyTorch code listing ",
        "text_level": 1,
        "bbox": [
            117,
            737,
            415,
            755
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "The code listing below implements BDH-GPU (Definition 4) for PyTorch version 2.7. It is self-contained, except for the implementation of RoPE which needs to be filled by the user. With respect to the state dynamics of Eq. (8), it provides an extension supporting heads. The placement of layer norms and residual connections is modified with respect to Eq. (8); in general, this aspect offers some flexibility. ",
        "bbox": [
            116,
            768,
            882,
            825
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "This implementation assumes the simplest case of a fixed context window of length $T$ . An unbounded context window is technically supported using a state-space kernel for Linear Attention, and works best following appropriate adaptation of the model for truncated backpropagation through time (see Appendix B.2). ",
        "bbox": [
            117,
            832,
            882,
            875
        ],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "i m p o r t t o r c h   \ni m p o r t t o r c h . nn . f u n c t i o n a l a s F   \nfrom t o r c h i m p o r t nn   \n$\\mathrm { ~ D ~ = ~ } 2 5 6$ # i n t e r n a l d i m e n s i o n   \n$\\mathrm { ~ H ~ } = \\mathrm { ~ 4 ~ }$ # he a ds   \n$\\vert \\mathrm { ~ N ~ } = ~ 3 2 7 6 8$ # n e u r o n s   \n$\\mathrm { ~ L ~ } = \\mathrm { ~ 6 ~ }$ # l a y e r s   \nd r o p o u t $\\it \\Delta \\phi _ { \\mathrm { ~ \\normalfont ~ 0 ~ . 0 ~ 5 ~ } }$   \nv o c a b _ s i z e $= ~ 2 5 6$   \nc l a s s BDH_GPU( nn . Module ) : d e f _ i n i t _ _ ( s e l f ) : s e l f . l n $=$ nn . L a y e r N o r m ( D , e l e m e n t w i s e _ a f f i n e $\\mathrm { = F }$ a l s e , b i a $\\mathsf { s } = \\mathsf { F }$ a l s e ) s e l f . w t e $=$ nn . E m b e d d i n g ( v o c a b _ s i z e , D) s e l f . d r o p $=$ nn . D r o p o u t ( d r o p o u t ) s e l f . e n c o d e r $=$ n n . P a r a m e t e r ( t o r c h . z e r o s ( ( N , D ) ) . n o r m a l _ $\\mathbf { s } _ { } { \\mathrm { t d } } = 0 . 0 2$ ) ) s e l f . d e c o d e r _ $\\mathrm { ~ \\bf ~ { ~ X ~ } ~ } =$ n n . P a r a m e t e r ( t o r c h . z e r o s ( ( H , D , N / / H ) ) . n o r m a l _ ( s t d = 0 . 0 2 ) ) s e l f . d e c o d e r _ y $=$ n n . P a r a m e t e r ( t o r c h . z e r o s ( ( H , D , N / / H ) ) . n o r m a l _ ( $\\mathbf { s } \\operatorname { t d } = 0 . 0 2$ ) ) s e l f . r e a d o u t $=$ n n . P a r a m e t e r ( t o r c h . z e r o s ( ( D , v o c a b _ s i z e ) ) . n o r m a l _ ( s t d = 0 . 0 2 ) ) s e l f . a t t n $=$ L i n e a r A t t e n t i o n ( ) d e f f o r w a r d ( s e l f , i d x ) : B , $\\mathrm { ~ T ~ } =$ i d x . s i z e ( ) # m i n i − b a t c h d i m e n s i o n s $\\mathrm { ~ v ~ \\_ ~ a ~ s ~ t ~ \\_ ~ } =$ s e l f . l n ( s e l f . w t e ( i d x ) . u n s q u e e z e ( 1 ) ) # B , 1 , T , D f o r _ i n r a n g e ( L ) : $\\mathrm { ~ \\bf ~ { ~ x ~ } ~ } = \\mathrm { ~ \\bf ~ F ~ }$ . r e l u ( v _ a s t $@$ s e l f . d e c o d e r _ x ) # B , H , T , N / / H a _ a s t $=$ s e l f . a t t n ( $\\scriptstyle { \\mathrm { Q } } = \\mathbf { X }$ , $\\operatorname { K } { = } \\mathbf { X }$ , $\\mathbf { V } { = } \\mathbf { V }$ _ a s t , ) $\\mathrm { ~ y ~ } = \\mathrm { ~ F ~ }$ . r e l u ( s e l f . l n ( a _ a s t ) $@$ s e l f . d e c o d e r _ y ) \\* x # B , H , T , N / / H $\\textrm { y } =$ y . t r a n s p o s e ( 1 , 2 ) . r e s h a p e ( B , 1 , T , N ) $\\textrm { y } =$ s e l f . d r o p ( y ) # S t a r t o f l a y e r w i t h v e c t o r s x , y v _ a s t $=$ v _ a s t $^ +$ s e l f . l n ( y $@$ s e l f . e n c o d e r ) # B , 1 , T , D v _ a s t $=$ s e l f . l n ( v _ a s t ) r e t u r n v _ a s t . s q u e e z e ( 1 ) $@$ s e l f . r e a d o u t # B , T , v o c a b _ s i z e   \nc l a s s L i n e a r A t t e n t i o n ( nn . M o d u l e ) : d e f f o r w a r d ( Q , K , V ) : $\\mathrm { Q r } \\ = \\ \\mathrm { R o P E } \\left( \\mathrm { Q } \\right)$ $\\mathrm { K r } \\ = \\ \\mathrm { R o P E } \\left( \\mathrm { K } \\right)$ r e t u r n ( Qr @ Kr . mT ) . t r i l ( d i a g o n a l = − 1 ) @ V ",
        "bbox": [
            112,
            99,
            872,
            892
        ],
        "page_idx": 61
    }
]